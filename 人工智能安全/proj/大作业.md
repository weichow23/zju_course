<center>
  <font face="黑体" size = 9>
    《人工智能安全》
  </font>
   <center><font face="黑体" size = 4>
     Privacy Risks of Securing Machine Learning Models against Adversarial Examples 
  </font>
   <center><font face="黑体" size = 6>
     together with
  </font>
  <center><font face="黑体" size = 4>
    Privacy Leakage of Adversarial Training Models in Federated Learning Systems
  </font>
  <center><font face="黑体" size = 6>
      论文复现 
  </font>
  <center><font face="黑体" size = 4>
    姓名： 周炜  黄鸿宇
  </font>
  <center><font face="黑体" size = 4>
    学号： 32010103790  3210105703
  </font>
</center> 



# 实验总览

### 实验简介

##### Privacy Risks of Securing Machine Learning Models against Adversarial Examples 

在本次课程的学习中，我们了解了许多针对人工智能的**鲁棒性攻击**方式，比如对抗样本攻击(**Evasion attacks**)和成员推断攻击(**Poisoning attacks**)等，同时也了解了其防御方法。我们还学习了人工智能的**隐私性攻击**和保护：一类是针对数据的攻击(**data privacy**) ，包括包括：membership inference attacks，property inference attacks，covert channel model training attacks等，推断某些样本是否属于训练集合的成员推理攻击；另一类是针对模型的攻击（**Model attacks**），包括模型窃取攻击或者参数窃取攻击。

<img src="pic/10.png" style="zoom:60%;" />

在设计针对不同攻击的防御方法时，我们根据该攻击依赖的特性（如决策边界，置信度）针对性进行防御，然而尴尬的是，现实中，一个模型常常要面对不同目的的攻击，而我们的防御手段却只针对其中的一种，甚至会成为另一种攻击方法的“帮凶”，典型的例子就是针对对抗样本攻击的防御，我们会通过数据增强的方法提高模型对于正确样本的敏感性，而这种敏感性的变化恰恰成了成员推断攻击的“武器”，成员数据能被更好地区分。

这篇论文针旨在联系安全与隐私这两个维度，着力探讨安全方面的对抗样本攻击与隐私方面的成员推断攻击之间的关系。

本文搜集了**六种**可以降低**对抗样本攻击**的防御方法，对它们进行**成员推理攻击**（模型预测的现有推理方法，以及本文提出的两种新推理方法，这些新方法利用了对抗性扰动数据下鲁棒模型的结构特性），测量其成功率。

文章发现：隐私保护与安全防御，二者难以两全。**对于那些对对抗样本攻击采取了防御机制的机器学习系统来说，成员推断攻击的成功率会大大提高。** 要防御对抗样本攻击，机器学习系统就要对施加在样本之上的微小扰动更加鲁棒，使其不会轻易被骗。而这项性能的优化严重依赖于训练数据。因此，反过来讲，严重依赖于训练数据的抗扰动系统，就会更容易被推断出其训练样本是什么。

##### Privacy Leakage of Adversarial Training Models in Federated Learning Systems

针对前一篇论文提到的结论：**对于那些对对抗样本攻击采取了防御机制的机器学习系统来说，成员推断攻击的成功率会大大提高**， 这篇论文则提出了一种正对隐私敏感的**联邦学习系统**的攻击， 发现，**经过对抗训练的鲁棒模型比未经过对抗训练的模型更容易被重建用户的训练数据**(在这篇文章中主要是`ImageNet`数据集被重建），即使在联邦训练用的`batch`数量很大，这些图像的还原率也是非常高。进一步验证前一篇论文的结论

### 相关资源

##### Privacy Risks of Securing Machine Learning Models against Adversarial Examples 

[论文链接 :hugs:](https://dl.acm.org/doi/10.1145/3319535.3354211)

[Github 仓库:cat:](https://github.com/inspire-group/privacy-vs-robustness)

##### Privacy Leakage of Adversarial Training Models in Federated Learning Systems

[论文链接 :hugs:](https://ieeexplore.ieee.org/document/9857077)

[Github 仓库:cat:](https://github.com/zjysteven/PrivacyAttack_AT_FL)

### 环境要求

**所有的环境要求我都写了一份**`requirement.txt`(已经定义换源)放在各自的工程文件中, 以下我列出了一些重要的配置，并且给出了`conda`配置环境的`shell`命令

##### Privacy Risks of Securing Machine Learning Models against Adversarial Examples 

上古环境（:sweat_smile:, `pip insatll`可能会失败，需要使用`conda install`

```shell
tensorflow==1.12
torch==0.4
```

配置`conda`环境的命令：

```shell
conda create -n PR python=3.6
conda activate PR
python -m pip install -r requirements.txt
```

##### Privacy Leakage of Adversarial Training Models in Federated Learning Systems

```shell
torch==1.9.0
torchvision==0.10.0
```

配置`conda`环境的命令：

```shell
conda create -n FL python=3.8
conda activate FL
python -m pip install -r requirements.txt
```

### 文件组织

```python
└── Privacy Risks  # Privacy Risks of Securing Machine Learning Models against Adversarial Examples 
    ├── Abstract verification # abstract interpretation-based verification
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py
    ├── Duality verification# duality-based verification
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py  
    ├── Distributional # Distributional Adversarial
        │   ├── CIFAR10
    	│   	└── ...
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py    
    ├── Difference # difference-based adversarial training
        │   ├── CIFAR10
    	│   	└── ...
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py    
    ├── IBP verification# interval bound propagation-based verification
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py
    ├── PGD # PGD-based adversarial training      
        │   ├── CIFAR10
    	│   	└── ...
    	│   ├── FMNIST
    	│   	└── ...
        │   ├── Yale
    	│   	└── ...
        │   └── output_utils.py    
    ├── inference_utils.py	# 客户端，用于本地训练
	├── membership_inference_results.ipynb  # 用于打印输出所有的结果
    ├── utils.py
    └── requirements.txt # 环境配置文件
└── Privacy Leakage # Privacy Leakage of Adversarial Training Models in Federated Learning Systems
    ├── ImageNet   # 数据集       
    │   ├── train 
    │   	└── ...
    │   └── val
    │   	└── ...
    ├── models   # 训练好的鲁棒模型
    │	├── resnet50_l2_eps3.ckpt
    │	├── 
    │	├──...
    ├── inversion.py
    ├── main.py
    ├── test.sh #用于直接运行程序
    ├── utils.py
    └── requirements.txt # 环境配置文件  
```

### 数据集

##### [CIFAR-10](https://huggingface.co/datasets/cifar10)

`CIFAR-10`数据集包含10个类别的60000张32x32彩色图像，每个类别有6000张图像。其中有50000张训练图像和10000张测试图像。数据集被分为五个训练批次和一个测试批次，每个批次包含10000张图像。测试批次中每个类别恰好包含1000张随机选择的图像。训练批次中的其余图像以随机顺序排列，但某些训练批次可能包含比另一个类别更多的图像。在这些批次中，每个类别恰好包含5000张图像

<img src="pic/01.png" style="zoom: 67%;" />

##### [fashion_mnist](https://huggingface.co/datasets/fashion_mnist)

`Fashion-MNIST`是 Zalando 文章图像的数据集，由 60,000 个示例的训练集和 10,000 个示例的测试集组成。每个示例都是一个 28x28 灰度图像，与 10 个类别的标签相关联

<img src="pic/02.png" style="zoom:60%;" />

##### [Yale Face](http://cvc.cs.yale.edu/cvc/projects/yalefaces/yalefaces.html)

耶鲁人脸数据库是一个人脸数据集，主要用于身份鉴定，包含 15 个主题，其中每个主题有 11 张图像共计 165 个 GIF 格式的灰度图像，每个主题包含不同的面部表情：中心光、带眼镜、快乐、左光、没有眼镜、正常、右光、悲伤、困、惊讶和眨眼

<img src="pic/03.png" style="zoom: 50%;" />

##### [ImageNet](https://huggingface.co/datasets/imagenet-1k)

是一个根据`WordNet`层次结构组织的图像数据集。`WordNet`中的每个有意义的概念，可能由多个词或词组描述，被称为 "同义词集"。`WordNet`中有超过100,000个同义词集，其中大部分是名词（80,000+）。`ImageNet`的目标是为每个同义词集提供平均1000张图片来说明。每个概念的图像都是经过质量控制和人工注释的

:no_entry:由于完整的数据集大小为$167.62G$，由于该课程没有提供计算资源，我train部分只sample了部分数据量，因此精度上复现的时候存在损失(我选择的是[kaggle](https://www.kaggle.com/datasets/ambityga/imagenet100)上的sample的版本并且对每个class又重新进行了sample)

<img src="pic/09.png" style="zoom:60%;" />

### 训练模型

**经过对抗训练的鲁棒模型模型**可以从[Github](https://github.com/microsoft/robust-models-transfer)上获得，只需要运行下列这些`shell`语句(只展示一种，其余可以具体见于`Github`)

```shell
mkdir models
wget -O models/resnet50_l2_eps3.ckpt "https://robustnessws4285631339.blob.core.windows.net/public-models/robust_imagenet/resnet50_l2_eps3.ckpt?sv=2020-08-04&ss=bfqt&srt=sco&sp=rwdlacupitfx&se=2051-10-06T07:09:59Z&st=2021-10-05T23:09:59Z&spr=https,http&sig=U69sEOSMlliobiw8OgiZpLTaYyOA5yt5pHHH5%2FKUYgI%3D"
```

主要使用的模型有`VGG16`和`ResNet18`,`ResNet50`等等

##### [ResNet(残差神经网络)](https://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)

`ResNet`模型的架构图，仿照`AlexNet`的8层网络结构，我们也将`ResNet`划分成8个构建层（BuildingLayer）。一个构建层可以包含一个或多个网络层、以及一个或多个构建块（如`ResNet`构建块）

![](pic/15.png)

- 第一个构建层，由1个普通卷积层和最大池化层构建
- 第二个构建层，由3个残差模块构成
- 第三、第四、第五构建层，都是由降采样残差模块开始，紧接着3个、5个、2个残差模块

##### [VGG（深卷积残差网络)](https://arxiv.org/abs/1409.1556)

`VGG` 的出现时间早于`ResNet`，`VGG` 可以看成是加深版的`AlexNet` ，整个网络由卷积层和全连接层叠加而成，和`AlexNet`不同的是，
`VGG`中使用的都是小尺寸的卷积核(3×3)，其网络架构如下图所示：

<img src="pic/16.png" style="zoom:60%;" />

- 每经过一个pooling层, 通道数目翻倍, 因为pooling会丢失某些信息, 所以通过增加通道数来弥补, 但是增加到512后, 便不再增加了, 可能是考虑到算力, 内存的问题
- FC全连接层, 最后输出的是1000个类的分类结果看D, E, 加卷积层都是在后面加, 因为前面的图片太大, 运算起来太耗时, maxpooling后, 神经图变小
- 纵向地看, 可以看到神经网络是由浅到深的. VGGNet采用递进式训练, 可以先训练浅层网络A, 然后用A初始化好的参数去训练后面深层的网络

# 方案设计

### Adversarial attack简介 

传统的adversarial attack 通过在一定范围内寻找扰动，找到能使样本扰动过决策边界的样本，因此，应对adversarial  attack的攻击尝试在目标函数加入robust loss来防御
$$
\min _{\theta} \frac{1}{\left|D_{\text {train }}\right|} \sum_{\mathbf{z} \in D_{\text {train }}} \alpha \cdot \ell\left(F_{\theta}, \mathbf{z}\right)+(1-\alpha) \cdot \ell_{R}\left(F_{\theta}, \mathbf{z}, \mathcal{B}_{\epsilon}\right)\\

\ell_{R}\left(F_{\theta}, \mathbf{z}, \mathcal{B}_{\epsilon}\right)=\max _{\tilde{\mathbf{x}} \in \mathcal{B}_{\epsilon}(x)} \ell^{\prime}\left(F_{\theta},(\tilde{\mathbf{x}}, y)\right)
$$
由于$\ell_{R}\left(F_{\theta}, \mathbf{z}, \mathcal{B}_{\epsilon}\right)$指的是在扰动范围内可以实现的最大偏移，因此这十分难以计算，为了将该公式更好地运用于实践当中，研究者们提出了基于经验的防护和基于验证的防护

在第一篇论文**Privacy Risks of Securing Machine Learning Models against Adversarial Examples**中评估了六种鲁棒训练算法引入的隐私泄露，分为3种**empirical defenses**和3种**Verifiable defenses**，证明该文对于鲁棒性模型的攻击具有较强的普适性。

#### 经验性防御

经验防御方法通过使用最先进的攻击方法，在每个训练步骤生成对抗性示例 Xadv 并计算其预测损失，从而近似鲁棒损失值。其鲁棒训练算法表示如下：
$$
\min _{\theta} \frac{1}{\left|D_{\text {train }}\right|} \sum_{\mathbf{z} \in D_{\text {train }}} \alpha \cdot \ell\left(F_{\theta}, \mathbf{z}\right)+(1-\alpha) \cdot l'\left(F_{\theta}, (x_{adv},y)\right)
$$
<img src="pic/23.png" style="zoom: 80%;" />

第一篇论文中测试的三种经验对抗性防御方法如下

#####  [PGD对抗训练](https://arxiv.org/abs/1706.06083)

使用投影梯度下降（`PGD`）方法生成对抗性示例，以最大化交叉熵损失($ℓ′= ℓ$) ，并且只在对抗性示例($α=0$）进行训练。`PGD `攻击包含 T 个梯度下降步骤

由于神经网络是非线性的，用`FGSM`效果一般比较差; `PGD`算法是`FGSM`算法的优化版本, 也属于白盒攻击的范畴。`PGD`采用小步多走的策略进行对抗。具体来说，就是一次次地进行前后向传播，一次次地根据`grad`计算扰动，最终最大化与原分类的差别（该差别由损失函数度量），如果是有目标攻击，则在最大化与原分类的差别同时，最小化与目标分类的差别，实现对抗样本生成的效果

<img src="pic/20.png" style="zoom:60%;" />

在扰动范围的球体内，最佳对抗样本出现的点不一定在球面上，也可能出现在球体的内部。如上图中右侧的1 和 2 两条路径 PGD 算法可以沿着1 和 2 的路径找到使损失函数最大的对抗样本点。但是 FGSM算法只能找到球面上的点，但显然这不是最好的对抗样本点。一般认为PGD 算法比 FGSM 算法更有效且更难防御。本实验中，我们实现的是$l_{\infin}$ 范数度量距离的PGD算法生成对抗样本

#####  [分布式对抗训练](https://arxiv.org/abs/1710.10571)

与 PGD 攻击中的投影步长 $∏_{B_ϵ}(x)$不同，分布式对抗训练通过解决交叉熵损失的拉格朗日松弛来生成对抗示例:
$$
\mathop {\max }\limits_{\widetilde x} l({F_\theta },(\widetilde x,y)) - \gamma ||\widetilde x - x|{|_p}
$$
其中$ γ $是$ lp$ 距离的惩罚参数。采用多步梯度下降法求解上述方程。然后仅在目标对抗性示例($α=0$)下，根据交叉损失熵对模型进行训练($ℓ′= ℓ$)

#####  [差分对抗训练](https://arxiv.org/abs/1901.08573)

差分对抗训练使用良性输出 $F_θ(x)$和对抗性输出 $F_θ(x_{adv})$之间的差分（例如 **Kullback-Leibler（KL）散度**）
$$
ℓ' = ({F_\theta },({x_{adv}},y)) = {d_{kl}}({F_\theta }({x_{adv}}),{F_\theta }(x))
$$
其中 $d_kl$ 计算 $KL$ 散度。基于 `PGD` 的攻击也会产生对抗性示例，但当前攻击目标是最大化输出差异:
$$
{\widetilde x^{t + 1}} = \prod\nolimits_{{\mathcal{B}_\varepsilon }(x)} {[{{\widetilde x}^t} + \eta *sign(\nabla  {{\widetilde x}^t}{d_{kl}}({F_\theta }({{\widetilde x}^t}),{F_\theta }(x)))]}
$$

#### 可验证防御

尽管经验防御方法对最先进的对抗性示例有效，但无法保证鲁棒性。为此提出了可验证防御方法：计算在对抗性扰动约束$ B_ϵ $下预测损失的上界 $ℓ′$。如果在已验证的最坏情况下仍能正确预测输入，则可以确定 $B_ϵ$ 下不存在错误分类。

可验证防御方法在训练过程中，使用已验证最坏情况的预测损失作为鲁棒损失值 $ℓ_R$，鲁棒训练算法为：
$$
\min _{\theta} \frac{1}{\left|D_{\text {train }}\right|} \sum_{\mathbf{z} \in D_{\text {train }}} \alpha \cdot \ell\left(F_{\theta}, \mathbf{z}\right)+(1-\alpha) \cdot \mathcal{V} (ℓ'\left(F_{\theta}, ({\widetilde x}),y), \mathcal{B}_{\epsilon}\right)
$$
上式中，$\mathcal{V} $表示在对抗扰动约束 $B_ϵ$ 下预测损失 $ℓ′$的已经验证的计算上界

第一篇论文中用到的三种可验证对抗防御如下：

#####  [基于对偶的验证](https://arxiv.org/abs/1805.12514)

基于对偶的验证对抗训练在非凸的 `ReLU函数` 操作上使用凸松弛，来解决其对偶问题，并计算经过验证的最坏情况损失，仅最小化过近似的鲁棒损失值（$α=0，ℓ′=ℓ$).与随机投影技术相结合，以扩展到更复杂的神经网络结构

##### [基于抽象解释的验证](http://proceedings.mlr.press/v80/mirman18b.html)

基于抽象解释的验证对抗训练利用抽象解释技术来计算最坏情况下的损失：使用抽象域（如区间域、zonotope 域）来表示输入层的对抗性扰动约束 $B_ϵ$，通过在其上应用抽象变压器，得到模型输出的最大验证范围。最后，在 $logit g_θ(\widetilde x)$ 上采用 softplus 函数来计算鲁棒损失值，然后将其与自然训练损失（$α≠0$)相结合:
$$
l' = ({F_\theta },(\widetilde x,y)) = \log ({e^{\mathop {\max }\limits_{y' \ne y} {g_\theta }{{(\widetilde x)}_{y'}} - {g_\theta }{{(\widetilde x)}_y} + 1}})
$$
<img src="pic/21.png" style="zoom:67%;" />

##### [基于间隔绑定传播的验证](https://arxiv.org/abs/1810.12715)

基于间隔绑定传播的验证训练将约束 Bϵ 表示为一个有界区间域（Mirman 等人考虑为一个指定域），并将该绑定传播到输出层。鲁棒损失计算为经过验证的、最坏情况输出的交叉熵损失($ℓ′= ℓ$) ，然后结合自然预测损失（$α≠0$）作为训练过程中的最终损失值

<img src="pic/22.png" style="zoom:50%;" />

上图是区间约束传播的验证对抗训练的一个例子。从左边开始、 对抗性多角形（为清晰起见，以二维形式展示）的名义 九"（红色）的名义图像通过卷积网络传播。网络传播。在每一层，多角形都会自我变形，直到最后一层。在最后一层，它在对数空间中呈现出复杂的非凸形。空间的复杂非凸形。区间界线（灰色）可以通过类似的方式传播：在每一层之后，界线都会被重塑。每一层的界线都被重塑为轴对齐的界线 框，始终包括对抗性多角形。在对数空间中，很容易计算出最坏情况下的违反规范的上限。 在对数空间中，很容易计算出要验证的规范的最差违反情况的上限

### Membership Inference Attacks简介

Membership Inference Attacks主要是判断输入是否属于训练集的一部分，以达到泄露模型隐私的效果，区分样本是否属于训练集的依据主要是模型预测的输出向量。

本文中，作者通过训练“`shadow model`”模拟原模型行为，最后再用`shadow model`训练推理模型，用以区分样本是否属于原模型的训练集，进一步发现，通过将预测置信值与一定的阈值相比较，即使是简单的线性分类器也能够得出较好的结果。

本文综合了以上两种方法进行攻击，在置信度-阈值法适用时使用该方法，在不适用时使用影子模型模拟目标模型，并构建训练数据集来训练二分类攻击模型，从而实现成员推断, 具体如下图所示:

<img src="pic/11.png" style="zoom:60%;" />

该方案主要分为三步：数据合成、影子模型模拟和攻击模型构建

a) **数据合成**：黑盒访问场景下，攻击者并不知道成员数据信息。故需要通过不同的统计算法去合成近似数据，比如基于模型、基于统计分布、基于噪声的真实数据。

b) **影子模型模拟**：利用合成的相关数据去训练一个或者多个影子模型，该影子模型结构和目标模型结构一致，即在未知目标模型任何信息的情况下，影子技术模拟目标模型，通过分析模拟的影子模型来替代原始的目标模型。

c) **攻击模型构建**：基于影子模型的数据集和目标模型的输出置信向量，并结合设定标签(如果数据$x$输于影子模型的训练集，则 $label=1$，否则 $label=0$ 训练一个二分类攻击模型，该模型能够判断给定目标数据点是否属于目标模型的训练数据集

对于使用对抗性约束$ B_ϵ$ 进行鲁棒训练的神经网络模型 $F$（为简单起见，我们跳过其参数 $θ$），成员推理攻击旨在确定给定输入示例 $z=(x,y)$是否在其训练集中。我们将敌手采用的推理策略表示为$ I(F,B_ϵ,z)$, 并且显然$ I(F,B_ϵ,z)$是$0$或者是$1$

第一篇论文中使用正确的成员预测分数作为评估成员推断准确性的指标，并且测试集 $D_{test}$ 来表示非成员，以 50%的概率从$ D_{train}$或 $D_{test}$ 中随机抽取一个数据点$(x，y)$来测试成员推断攻击，准确性如下：
$$
{A_{\inf }}(F,{\mathcal{B}_\varepsilon },I) = \frac{{\sum\nolimits_{z \in {D_{train}}} {I(F,{\mathcal{B}_\varepsilon },z)} }}{{2|{D_{train}}|}} + \frac{{\sum\nolimits_{z \in {D_{test}}} {1 - I(F,{\mathcal{B}_\varepsilon },z)} }}{{2|{D_{test}}|}}
$$
为了进一步衡量本文成员推理策略的有效性，本文还使用了**成员推理优势的概念**（推理精度对随机猜测的提高量）:
$$
ADV{T_{\inf }} = 2 \times ({A_{\inf }} - 0.5)
$$

### 对鲁棒模型进行推断攻击

​	要想了解为什么鲁邦模型面对推断攻击会变得更加脆弱，就得了解成员推断攻击产生的原理：

* 成员推断攻击的误差与模型的泛化误差高度相关，一个模型越泛化说明其不仅在训练集上表现良好，在测试集上也能有十分良好的表现，但是这是很困难的，一般情况下，训练的模型由于样本覆盖面不够，训练参数设置等问题都会出现过拟合等现象，这导致了在测试集上的精确度降低，也间接导致了模型对于训练集和测试集输入的判断会有区别，因此产生了成员推断共计的可能。进一步，由于鲁棒模型基于训练集的样本产生的对抗性样本进行进一步训练，这会使得模型在训练集和测试集之间产生更大的"generation gap"，这会导致更大的训练误差。

* 成员推断攻击的性能与目标模型对训练数据的敏感性有关。灵敏度度量是通过计算一个数据点对目标模型性能的影，当一个训练点对目标模型的影响较大时，其模型预测可能与模型对测试点的预测不同，因此对手更容易区分其隶属度。鲁棒训练算法的目的是确保模型预测在任何数据点周围的一个小区域（在对抗样本攻击中，我们采用$l_\infin$范数度量距离产生的一个球）上保持不变。但是在实际实践中着放大了训练数据对模型的影响。因此，与自然训练相比，鲁棒训练算法可以通过提高模型对训练数据的敏感性，使模型更容易受到成员推断攻击。

### 对推断攻击效果的衡量

在整篇论文中，我们关注的是 $l_∞$ 微扰约束：
$$
\mathcal{B}_ϵ =\{x'|||x'-x||_{\infin}\le \epsilon\}
$$
在第一篇论文中，使用了 `Yale Face`、`Fashion MNIST` 和 `CIFAR10 `数据集，并分别设计使用合适的卷积神经网络，设置 $l_∞ $扰动预算(ϵ)为 $8/255$

##### 对普通样本推断攻击效果的衡量

采用了置信阈值推理策略 $\mathcal{I}_B$：如果输入$(x,y)$的预测置信度 $F(x)$大于预设阈值，则推断输入$(x,y)$为成员。表达式为：
$$
\begin{aligned}
\mathcal{I}_{\mathbf{B}}\left(F, \mathcal{B}_{\epsilon},(\mathbf{x}, y)\right)= & \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathbf{A}}\right\} \\
A_{\text {inf }}\left(F, \mathcal{B}_{\epsilon}, \mathcal{I}_{\mathbf{B}}\right)= & \frac{1}{2}+\frac{1}{2} \cdot\left(\frac{\sum_{\mathbf{z} \in D_{\text {train }}} \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathbf{B}}\right\}}{\left|D_{\text {train }}\right|}\right.  \left.-\frac{\sum_{\mathbf{z} \in D_{\text {test }}} \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathbf{B}}\right\}}{\left|D_{\text {test }}\right|}\right)
\end{aligned}
$$
##### 对经验对抗样本推断攻击效果的衡量

在约束 $\mathcal{B}_ϵ$ 下为输入$(x,y)$生成一个（非目标）对抗性示例 $x_{adv}$，并对模型于 $x_{adv}$ 的预测置信度使用一个阈值。该策略的表达式：
$$
\begin{aligned}
\mathcal{I}_{\mathbf{A}}\left(F, \mathcal{B}_{\epsilon},(\mathbf{x}, y)\right)= & \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathrm{A}}\right\} \\
A_{\text {inf }}\left(F, \mathcal{B}_{\epsilon}, \mathcal{I}_{\mathrm{A}}\right)= & \frac{1}{2}+\frac{1}{2} \cdot\left(\frac{\sum_{\mathbf{z} \in D_{\text {train }}} \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathrm{A}}\right\}}{\left|D_{\text {train }}\right|}\right.  \left.-\frac{\sum_{\mathbf{z} \in D_{\text {test }}} \mathbb{1}\left\{F\left(\mathbf{x}_{a d v}\right)_{y} \geq \tau_{\mathrm{A}}\right\}}{\left|D_{\text {test }}\right|}\right)
\end{aligned}
$$
我们使用前文等式的 PGD 攻击方法来获得$x_{adv}$。同样，我们选择预设阈值$\mathcal{I}_A$来实现最高的推理精度，即最大化对抗性训练与测试示例上预测置信度的、两个互补累积分布函数之间的差距。表达式中的扰动约束$\mathcal{B}_ϵ$  需要指定

##### 对验证对抗样本推断攻击效果的衡量

我们利用可验证防御模型使用的验证技术 $V$，在对抗性约束 $\mathcal{B}_ϵ$ 下获得输入的最坏情况预测。我们使用输入的最坏情况预测置信度来预测其成员身份。该策略$\mathcal{I}_V$的表达式及其推理精度如下所示：
$$
\begin{aligned}
\mathcal{I}_{\mathbf{V}}\left(F, \mathcal{B}_{\epsilon},(\mathbf{x}, y)\right)= & \mathbb{1}\left\{\mathcal{V}\left(F(\tilde{\mathbf{x}})_{y}, \mathcal{B}_{\epsilon}\right) \geq \tau_{\mathbf{V}}\right\} \\
A_{\text {inf }}\left(F, \mathcal{B}_{\epsilon}, \mathcal{I}_{\mathbf{V}}\right)= & \frac{1}{2}+\frac{1}{2} \cdot\left(\frac{\sum_{\mathbf{z} \in D_{\text {train }}} \mathbb{1}\left\{\mathcal{V}\left(F(\tilde{\mathbf{x}})_{y}, \mathcal{B}_{\epsilon}\right) \geq \tau_{\mathbf{V}}\right\}}{\left|D_{\text {train }}\right|}\right.  \left.-\frac{\sum_{\mathbf{z} \in D_{\text {test }}} \mathbb{1}\left\{\mathcal{V}\left(F(\tilde{\mathbf{x}})_{y}, \mathcal{B}_{\epsilon}\right) \geq \tau_{\mathbf{V}}\right\}}{\left|D_{\text {test }}\right|}\right)
\end{aligned}
$$
我们可以看出三者基本相同（其实就是输入不同而已），下面结合代码对其进行阐述

### 联邦学习成员推断攻击

第二篇论文提出的联邦成员推断攻击主要根据客户端和中央服务器之间传递的权重梯度重建自己的训练图像（可能包含私人信息），从而损害联邦学习客户端的隐私。首先恢复特征（即倒数第二层的输出）从梯度中，然后使用恢复的特征作为监督来重建输入。虽然批次内样本的梯度融合在一起并禁止每个单独输入的精确重建，但特征耦合较少，并且（一旦恢复）可以为重建提供比批次更精确的信息- 平均梯度，因此这种方法理论上是有效的。流程图如下：

<img src="pic/12.png" style="zoom:60%;" />

用数学来具体表示：

首先我们定义图像分类任务中，在局部训练期间，联邦学习模型参数的损失梯度计算如下
$$
\begin{equation*}{\Delta _\theta }: = \frac{1}{N}\sum\limits_{i = 1}^N {{\nabla _\theta }} {\mathcal{L}}\left({{f_\theta }\left({{x_i}}\right),{y_i}}\right)\end{equation*}
$$
其中$f_θ(-)$是以$θ$为参数的DNN模型的映射函数，$N$是批次大小，$x_i$是批次的第$i$个输入图像，$y_i$是相应的groundtruth标签，$\mathcal{L}(-,-)$计算交叉熵损失，$Δθ$是批次平均梯度的简化概念。注意，如果客户正在执行对抗样本训练，则输入$x_i$将是对抗性的例子，而不是原始的干净样本

为了**从梯度中恢复特征**：首先令$f()=h(r())$，其中$r(-)$通常包括DNN卷积层并提取输入的高级表征，$h(-)$是线性层，在特征之上进行分类并为每个类别输出激活函数的值。为了简单起见，我们把输入$\textbf{x}$的特征表示为$\textbf{r}$，即$\textbf{r} := r(x)$，$\textbf{r}∈ℝ^D$

由于特征可以从线性层权重的梯度中恢复，我们可以将线性层的权重矩阵表示为$\mathcal{W}∈ℝ^{D\times K}$，其中第$k$列$\mathcal{W}_k∈ℝ^D$是对应于第$k$类的权重向量，$K$是类的总数。然后，类的激活$/logits$为$a = \mathcal{W}^Tr$，类的概率为$p = softmax(a)$。 因此，我们可以推导出线性层中每个权重向量W:,k的损失梯度：
$$
\begin{align*}& \frac{{\partial l}}{{\partial {{\boldsymbol{W}}_{:,k}}}} = \frac{{\partial l}}{{\partial {{\boldsymbol{p}}_y}}} \cdot \sum\limits_{j = 1}^K {\frac{{\partial {{\boldsymbol{p}}_y}}}{{\partial {{\boldsymbol{a}}_j}}}} \cdot \frac{{\partial {{\boldsymbol{a}}_j}}}{{{{\boldsymbol{W}}_{:,k}}}} = \begin{cases} { - \left({1 - {{\boldsymbol{p}}_y}}\right){\boldsymbol{r}},}&{k = y} \\ {{{\boldsymbol{p}}_k}{\boldsymbol{r}},}&{k \ne y} \end{cases}\end{align*}
$$
考虑一批$N$个输入$\textbf{x}_i$，每个都有一个相应的特征向量$\textbf{r}_i$，假设在该批中没有共享相同标签的样本,当规模$N$比类的数量K小很多时，这个假设对随机构建的批次通常是成立的。然后，假设我们想恢复对应标签为$y_i$的特征向量$\textbf{r}_i$，我们可以直接取梯度，即权重向量$\mathcal{W}_{:y_i}$
$$
\begin{align*}
& {\widehat {\boldsymbol{r}}_i}: = {\Delta _{{{\boldsymbol{W}}_{:,{y_i}}}}} = \frac{1}{N}\sum\limits_{j = 1}^N {\frac{{\partial {l_j}}}{{\partial {{\boldsymbol{W}}_{:,{y_i}}}}}} 
 \propto - \left({1 - {{\boldsymbol{p}}_{i,{y_i}}}}\right){{\boldsymbol{r}}_i} + \sum\limits_{j = 1,j \ne i}^N {{{\boldsymbol{p}}_{j,{y_i}}}} {{\boldsymbol{r}}_j}  \approx - \left({1 - {{\boldsymbol{p}}_{i,{y_i}}}}\right){{\boldsymbol{r}}_i}\end{align*}
$$
接下来，我们恢复批量标签$y_i$，这对于知道$\mathcal{W}$的梯度矩阵中哪些列应该被用于特征恢复是必要的.在不丧失一般性的情况下，假设$N$个样本的批次标签为$1、2、...、N$，即第k个样本来自第k类。那么根据上面的公式，与$\mathcal{W}$的第$k$列有关的梯度为：
$$
\begin{align*}& \frac{1}{N}\sum\limits_{j = 1}^N {\frac{{\partial {l_j}}}{{\partial {W_{:,k}}}}} \propto  \left\{ {\begin{array}{ll} { - \left({1 - {p_{k,k}}}\right){r_k} + \sum\nolimits_{j = 1,j \ne k}^N {{p_{j,k}}} {r_j},}&{1 \leq k \leq N} \\ {\sum\nolimits_{j = 1}^N {{p_{j,k}}} {r_j},}&{N < k \leq K} \end{array}.} \right.\end{align*}
$$
一旦我们得到了恢复的特征$\widehat{\textbf{r}}_i$，就可以通过解决以下优化目标来重建每个输入图像:
$$
\begin{equation*}\mathop {\arg \min }\limits_{{{\boldsymbol{z}}_i}} {{\mathcal{L}}_{{\text{feat }}}}\left({{r_\theta }\left({{{\boldsymbol{z}}_i}}\right),{{\boldsymbol{r}}_i}}\right) + {\mathcal{R}}\left({{{\boldsymbol{z}}_i}}\right)\end{equation*}
$$

# 方案创新

1. 复现了**2**篇论文（**Privacy Risks of Securing Machine Learning Models against Adversarial Examples** 及**Privacy Leakage of Adversarial Training Models in Federated Learning Systems**），并且对两篇论文进行了比较和分析
2. 另外**3**篇相关的论文进行了探讨：一篇为**Membership Inference Attacks on Machine Learning: A Survey**，借用这篇综述探讨了成员推断攻击的常见方式及防御；另一篇为 **Membership Inference Attacks Against Robust Graph Neural Network**，我们将复现论文的隐私性和鲁棒性联系的思想推广到**非欧式空间**，得到了同样的结论。最后，基于我们对鲁棒性和隐私性的探讨，更进一步得对**Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling**中提到的**可信人工智能系统**进行了探讨
3. 对复现的两篇论文的某些代码细节进行了优化（比如配置了一些加速训练的方法）, 并且解决了原代码中的一些错误

# 代码分析

### Privacy Risks of Securing Machine Learning Models against Adversarial Examples

`membership_inference_results.ipynb`主要用于输出整个程序的训练结果，调用各个训练好的`.npz`文件中的模型使用`inference_utils.py`中的`inference_via_confidence`函数来计算输出4项准确率, $\mathcal{I}_A$ 和$\mathcal{I}_B$,

首先直接拿这些训练好的结果进行测试：

```python
for num in range(confidence_mtx1.shape[0]):
    confidence1.append(confidence_mtx1[num,label_vec1[num]])
    if np.argmax(confidence_mtx1[num,:]) == label_vec1[num]:
        acc1 += 1
```

该部分是统计分类是否正确的代码，通过`argmax`函数从置信度向量中统计模型推断结果，计算正确样本数，最终统计正确率。

```python
print('model accuracy for training and test-', (acc1/confidence_mtx1.shape[0], acc2/confidence_mtx2.shape[0]) )
```

在推断攻击方面，本论文使用置信度阈值法进行检测，在如何选取阈值这一问题上，本论文使用了十分简单的方法，遍历选取，选取推断攻击效果最高的阈值：

```python
    sort_confidence = np.sort(np.concatenate((confidence1, confidence2)))
    max_accuracy = 0.5
    best_precision = 0.5
    best_recall = 0.5
    for num in range(len(sort_confidence)):
        delta = sort_confidence[num]
        ratio1 = np.sum(confidence1>=delta)/confidence_mtx1.shape[0]
        ratio2 = np.sum(confidence2>=delta)/confidence_mtx2.shape[0]
        accuracy_now = 0.5*(ratio1+1-ratio2)
        if accuracy_now > max_accuracy:
            max_accuracy = accuracy_now
            best_precision = ratio1/(ratio1+ratio2)
            best_recall = ratio1
```

如上代码，从高置信度到低置信度遍历，根据该阈值获得训练集和测试集中高于该阈值的样本所占比例，由于样本推断攻击要尽可能推断出训练集中的样本，尽可能少推断出测试集的样本，所以最终推断结果的公式如下：（也就是刚刚上面展示的公式）
$$
ADV{T_{\inf }} = 2 \times ({A_{\inf }} - 0.5)
$$

```python
accuracy_now = 0.5*(ratio1+1-ratio2)
```

遍历选择能使这个准确率最大的阈值，作为推断阈值，然后输出结果

而`utils.py`主要就是对`Yale Face`做一些处理划分为`train`和`test`两部分，因为原数据集只有一个文件

```python
def YALE_split(Yale_file, train_points_per_label=50):
    # 加载Yale文件
    YALE = io.loadmat(Yale_file) 
    X = YALE['X']  # 获取特征数据
    Y = YALE['Y']  # 获取标签数据
    X = X.T/255.0  # 特征数据归一化
    X = X.reshape((2414,  168, 192)).swapaxes(1,2)  # 重新调整特征数据的形状
    Y = Y.flatten()  # 将标签数据展平
    train_data, train_label, test_data, test_label = [], [], [], []  # 初始化训练和测试数据集
    np.random.seed(0)  # 设置随机种子
    label_count = 0  # 标签计数器
    for label in set(Y):
        label_idx = np.argwhere(Y==label).flatten()  # 获取具有特定标签的索引
        tot_num = len(label_idx)  # 计算具有特定标签的总样本数量
        idx_permute = np.random.permutation(label_idx)  # 随机打乱索引
        # 将数据分为训练集和测试集
        train_data.append(X[idx_permute[:train_points_per_label]])  # 将一部分数据添加到训练集
        train_label.append(np.repeat(label_count, train_points_per_label))  # 将相应标签添加到训练标签集
        test_data.append(X[idx_permute[train_points_per_label:]])  # 将剩余的数据添加到测试集
        test_label.append(np.repeat(label_count, tot_num - train_points_per_label))  # 将相应标签添加到测试标签集
        label_count += 1  # 标签计数器递增
    train_data = np.concatenate(train_data)  # 合并训练数据集
    test_data = np.concatenate(test_data)  # 合并测试数据集
    train_label = np.concatenate(train_label)  # 合并训练标签集
    test_label = np.concatenate(test_label)  # 合并测试标签集
    train_idx_permute = np.random.permutation(len(train_label))  # 随机打乱训练数据的索引
    train_data = np.expand_dims(train_data[train_idx_permute], 3)  # 对训练数据进行维度扩展
    train_label = train_label[train_idx_permute]  # 对训练标签进行相应调整
    test_idx_permute = np.random.permutation(len(test_label))  # 随机打乱测试数据的索引
    test_data = np.expand_dims(test_data[test_idx_permute], 3)  # 对测试数据进行维度扩展
    test_label = test_label[test_idx_permute]  # 对测试标签进行相应调整
    return train_data, train_label, test_data, test_label
```

6中对抗训练的文件中都还有一个`output_utils.py`, 主要功能是评估分类器的性能，通过投影梯度下降法生成对抗样本，计算分类器在原始样本和对抗样本上的预测结果，并输出相应的准确率。以`PGD/`文件夹下的为例：

它包含了以下几个函数：

`PGD_perturb(sess, model, gradient, x, y, num_step, step_size, max_perturb)`: 使用投影梯度下降法生成对抗样本。对给定的输入样本x和标签y，根据模型model和梯度gradient，迭代地更新扰动perturb，生成对抗样本

```python
def PGD_perturb(sess, model, gradient, x, y, num_step, step_size, max_perturb):
    # 使用投影梯度下降法生成对抗样本
    perturb = np.zeros(x.shape)  # 初始化扰动
    for num in range(num_step):
        # 计算梯度并更新扰动
        perturb += step_size * np.sign(sess.run(gradient, feed_dict={model.x_input: x + perturb, model.y_input: y}))
        perturb = np.clip(perturb, -max_perturb, max_perturb)  # 限制扰动的大小
        perturb = np.clip(x + perturb, 0, 1.0) - x  # 限制样本的像素值范围在0到1之间
    return np.clip(x + perturb, 0, 1.0)  # 返回生成的对抗样本
```

`softmax_by_row(logits, T=1.0)`: 对logits进行softmax处理。对给定的logits，使用温度参数T计算每一行的softmax概率，并返回结果

```python
def softmax_by_row(logits, T=1.0):
    # 对logits进行softmax处理，T为温度参数
    mx = np.max(logits, axis=-1, keepdims=True)
    exp = np.exp((logits - mx) / T)
    denominator = np.sum(exp, axis=-1, keepdims=True)
    return exp / denominator  # 返回softmax后的结果
```

`classifier_performance(sess, model, train_data, train_label, test_data, test_label, batch_size, num_step, max_perturb, step_size)`: 计算分类器的性能指标。使用给定的参数，计算分类器在原始样本和对抗样本上的准确率，并打印输出。该函数还利用上述的两个函数生成对抗样本，并将分类器输出结果保存在相应的变量中

```mermaid
graph LR
A[开始] --> B[计算性能指标]
B --> C[计算损失和梯度]
C --> D[处理训练集]
C --> E[处理测试集]
D --> F[生成对抗样本]
D --> G[计算训练集输出原始样本]
F --> H[计算训练集输出对抗样本]
E --> I[生成对抗样本]
E --> J[计算测试集输出原始样本]
I --> K[计算测试集输出对抗样本]
B --> L[计算准确率]
G --> L
H --> L
J --> L
K --> L
L --> M[输出准确率]
M --> N[返回输出数据]
```

具体代码为：

```python
def classifier_performance(sess, model, train_data, train_label, test_data, test_label, batch_size, num_step,
                           max_perturb, step_size):
    # 计算分类器的性能指标
    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=model.pre_softmax, labels=model.y_input)
    gradient = tf.gradients(loss, model.x_input)[0]  # 计算输入对应的梯度
    output_train_benign = []
    output_train_adversarial = []
    # 对训练集进行批量处理
    for num in range(math.ceil(train_data.shape[0] / batch_size)):
        end_idx = num * batch_size + batch_size
        if end_idx > train_data.shape[0]:
            end_idx = train_data.shape[0]
        input_data = train_data[num * batch_size: end_idx]
        input_label = train_label[num * batch_size: end_idx]
        input_PGD = PGD_perturb(sess, model, gradient, input_data, input_label, num_step, step_size, max_perturb)
        output_train_benign.append(softmax_by_row(sess.run(model.pre_softmax, feed_dict={model.x_input: input_data})))
        output_train_adversarial.append(
            softmax_by_row(sess.run(model.pre_softmax, feed_dict={model.x_input: input_PGD})))
    output_train_benign = np.concatenate(output_train_benign)
    output_train_adversarial = np.concatenate(output_train_adversarial)
    output_test_benign = []
    output_test_adversarial = []
    # 对测试集进行批量处理
    for num in range(math.ceil(test_data.shape[0] / batch_size)):
        end_idx = num * batch_size + batch_size
        if end_idx > test_data.shape[0]:
            end_idx = test_data.shape[0]
        input_data = test_data[num * batch_size: end_idx]
        input_label = test_label[num * batch_size: end_idx]
        input_PGD = PGD_perturb(sess, model, gradient, input_data, input_label, num_step, step_size, max_perturb)
        output_test_benign.append(softmax_by_row(sess.run(model.pre_softmax, feed_dict={model.x_input: input_data})))
        output_test_adversarial.append(
            softmax_by_row(sess.run(model.pre_softmax, feed_dict={model.x_input: input_PGD})))
    output_test_benign = np.concatenate(output_test_benign)
    output_test_adversarial = np.concatenate(output_test_adversarial)
    # 计算分类器的准确率
    train_acc1 = np.sum(np.argmax(output_train_benign, axis=1) == train_label.flatten()) / len(train_label)
    train_acc2 = np.sum(np.argmax(output_train_adversarial, axis=1) == train_label.flatten()) / len(train_label)
    test_acc1 = np.sum(np.argmax(output_test_benign, axis=1) == test_label.flatten()) / len(test_label)
    test_acc2 = np.sum(np.argmax(output_test_adversarial, axis=1) == test_label.flatten()) / len(test_label)
    print('Benign accuracy: ', (train_acc1, test_acc1), ' Adversarial accuracy: ', (train_acc2, test_acc2))
    return output_train_benign, output_train_adversarial, output_test_benign, output_test_adversarial
```

由于本课程作业的要求为成员推断攻击相关论文的复现，因此代码中的六种对抗训练方法的代码这里不再赘述

### Privacy Leakage of Adversarial Training Models in Federated Learning Systems

`utils.py`中是一些用于辅助训练过程的函数，总体来说也不太重要，报告中直接略去该部分

`main.cpp`主要是统筹了整个训练过程，用了预训练的神经网络模型和ImageNet数据集进行实验,需要输入一些参数，比如：

```shell
python main.py \
    --inet-dir ImageNet \
    --gpu 6 7 \
    --expr rep_recover_acc \
    --batch-size 16 \
    --num-samples-per-class 1 \
    --num-batches 1 \
    --arch resnet50 \
    --eps 3 
```

使用的正常模型是`torchvision`封装好的，只需要根据命令行的参数进行选择即可

```python
parser.add_argument('--arch', type=str, default='resnet18',
                    choices=['resnet18', 'resnet50', 'wide_resnet50_4', 'vgg16_bn', 'densenet161', 'resnext50_32x4d', 'mobilenet'], 
                    help='Choose architecture.')
```

鲁棒模型的加载路径则是通过`checkpoint_dir`参数指定的，而数据集`ImageNet`的路径是通过`inet_dir`参数指定的

`main.py`的运行流程图为：

```mermaid
graph LR

A[设置实验选项] --> B[加载模型和数据集]
B --> C{实验类型是rep_recover_acc?}
C -- 是 --> D[随机选择样本并计算表示恢复准确度]
C -- 否 --> E{实验类型是softmax_prob_distribution?}
E -- 是 --> F[计算softmax概率分布]
E -- 否 --> G{实验类型是img_recover?}
G -- 是 --> H[加载表示恢复结果并生成逆推图像]
G -- 否 --> I[引发异常: ValueError]

D --> J[保存恢复后的图像和结果]
F --> K[保存softmax概率分布结果]
H --> L[保存生成的逆推图像]

J --> M[结束]
K --> M
L --> M
I --> M

```

根据实验类型（由命令行输入的参数`args.expr`决定）执行相应的操作：

- 如果实验类型是`rep_recover_acc`，则随机选择每个类别的样本，计算每个样本的表示恢复准确度，并保存恢复后的图像

1. 随机选择每个类别的样本，并存储为锚点索引列表。
2. . 初始化存储所有结果的列表：`all_results`、`all_true_rep`和`all_recov_rep`
3. 对于每个类别和对应的锚点索引列表，进行以下操作：   
   - 遍历锚点索引列表中的每个样本
   - 获取批次索引列表和锚点位置列表
   - 根据批次索引列表和锚点位置列表，创建数据加载器
   - 遍历数据加载器中的每个批次，执行以下操作：  检查批次数据中的锚点位置是否与类别标签匹配, 获取批次数据中锚点位置对应的特征梯度、真实表示和真实图像, 使用特征梯度恢复表示，并计算真实表示和恢复表示之间的余弦相似度,存储余弦相似度最大的图像作为基准图像, 创建保存结果的文件夹，并将基准图像保存

```python
if args.expr == 'rep_recover_acc':
    # 从每个类别随机选择样本
    label_2_index = get_inet_label_index_pair(args.inet_dir, args.split)

    rng = np.random.RandomState(args.seed)
    anchor_labels = list(range(10))  # 如果训练样本比较小，那就要改小, 原来为1000，但是我的样本要小很多，所以改成了10
    anchor_indices_list = [list(rng.choice(label_2_index[i], args.num_samples_per_class, replace=False)) for i in anchor_labels]
    all_results = []  # 存储所有结果
    all_true_rep = []  # 存储真实的表示
    all_recov_rep = []  # 存储恢复的表示
    for anchor_label, anchor_indices in tqdm(enumerate(anchor_indices_list), total=len(anchor_indices_list), leave=True,
                                             position=2, desc='Classes'):
        level_1 = []  # 第一层级的结果
        tr_level_1 = []  # 第一层级的真实表示
        rr_level_1 = []  # 第一层级的恢复表示
        for anchor_index in tqdm(anchor_indices, total=len(anchor_indices), leave=False, position=1, desc='Samples'):
            # 获取批次索引和锚点位置
            batch_index_list, anchor_position_list = get_batch_index(anchor_label, anchor_index, label_2_index,
                                                                     batch_size=args.batch_size,
                                                                     num_batches=args.num_batches)
            all_batches = [i for b in batch_index_list for i in b]
            dataloader = DataLoader(
                Subset(dset, all_batches),
                batch_size=args.batch_size,
                shuffle=False,
                num_workers=4,
            )
            level_2 = []  # 第二层级的结果
            tr_level_2 = []  # 第二层级的真实表示
            rr_level_2 = []  # 第二层级的恢复表示
            gt_list = []  # 存储锚点位置对应的真实图像
            for batch_data, anchor_position in tqdm(zip(dataloader, anchor_position_list), total=len(dataloader),
                                                    leave=False, position=0, desc='Batches'):
                # 检查
                assert batch_data[1][anchor_position] == anchor_label
                # 获取批次中锚点位置对应的特征梯度、真实表示和真实图像
                batch_fc_grads, true_reps, gt_imgs = get_batch_fc_gradients_and_reps(batch_data, net, eps=args.eps)
                # 从特征梯度中恢复表示
                recovered_reps = recov_reps_from_gradients(batch_fc_grads, args.batch_size)
                # 计算真实表示和恢复表示之间的余弦相似度
                level_2.append(
                    F.cosine_similarity(true_reps[anchor_position], -recovered_reps[anchor_position], dim=0).item())
                tr_level_2.append(true_reps[anchor_position].detach().cpu())
                rr_level_2.append(-recovered_reps[anchor_position].detach().cpu())
                gt_list.append(gt_imgs[anchor_position].cpu())
                # 选择余弦相似度最大的图像作为基准图像
                gt_img = gt_list[np.argmax(level_2)]
                # 创建保存结果的文件夹路径
                save_folder = os.path.join(
                    'results',
                    f"recov_img_arch={args.arch}_eps={args.eps:g}_bs={args.batch_size}_samplesperclass={args.num_samples_per_class}_numbatches={args.num_batches}",
                    f"{anchor_label}_{anchor_index}_{np.max(level_2):.16f}"
                )
                if not os.path.exists(save_folder):
                    os.makedirs(save_folder)
                # 存储结果到相应的列表中
                level_1.append(level_2)
                tr_level_1.append(tr_level_2[np.argmax(level_2)])
                rr_level_1.append(rr_level_2[np.argmax(level_2)])
            all_results.append(level_1)
            all_true_rep.append(tr_level_1)
            all_recov_rep.append(rr_level_1)
        if not os.path.exists('results'):
            os.makedirs('results')
        # 保存表示恢复准确度的结果
        np.save(
            os.path.join('results',
                         f"rep_acc_arch={args.arch}_eps={args.eps:g}_bs={args.batch_size}_samplesperclass={args.num_samples_per_class}_numbatches={args.num_batches}_{args.split}.npy"),
            np.array(all_results)
        )
        # 保存真实表示和恢复表示的结果
        torch.save({
                'true_rep': all_true_rep,
                'recov_rep': all_recov_rep
            },
            os.path.join('results',
                         f"rep_arch={args.arch}_eps={args.eps:g}_bs={args.batch_size}_samplesperclass={args.num_samples_per_class}_numbatches={args.num_batches}_{args.split}.pt")
        )
```

- 如果实验类型是`softmax_prob_distribution`，则计算每个样本的softmax概率分布，并保存结果

主要步骤如下：

1. 获取类别标签与索引的对应关系。
2. 创建随机数生成器并生成每个类别的样本索引列表。
3. 将锚点索引列表展平为一维数组，并创建数据加载器，加载锚点样本的子集。
4. 设置攻击参数，

```python
elif args.expr == 'softmax_prob_distribution':
    # 获取类别标签与索引的对应关系
    label_2_index = get_inet_label_index_pair(args.inet_dir, args.split)

    rng = np.random.RandomState(args.seed)
    anchor_labels = list(range(1000))  # 所有类别的标签列表
    anchor_indices_list = [list(rng.choice(label_2_index[i], args.num_samples_per_class, replace=False)) for i in
                           anchor_labels]
    anchor_indices_list = np.asarray(anchor_indices_list).flatten()  # 将锚点索引列表展平为一维数组
    # 创建数据加载器，加载锚点样本的子集
    dataloader = DataLoader(Subset(dset, anchor_indices_list), shuffle=False, batch_size=200, num_workers=4)
    save_path = os.path.join('results', f"softmax_prob_arch={args.arch}_eps={args.eps:.1f}_{args.split}.pt")
    # 攻击参数设置
    attack_kwargs = {
        'constraint': '2',  # L-2 PGD
        'eps': args.eps,  # 约束的 epsilon 值（L-2 范数）
        'step_size': args.eps * 2 / 3,  # PGD 的学习率
        'iterations': 3,  # PGD 的迭代次数
        'targeted': False,  # 非目标攻击
        'custom_loss': None  # 使用默认的交叉熵损失函数
    }
    make_adv = args.eps > 0  # 是否生成对抗样本
    results = []  # 存储结果的列表
    for x, y in tqdm(dataloader, total=len(dataloader), desc='Batches', leave=True, position=0):
        x, y = x.cuda(), y.cuda()
        # 在神经网络上运行输入样本，得到 logits
        logits = net(x, y, make_adv, with_image=False, with_latent=False, **attack_kwargs)
        # 计算 softmax 概率，并将结果与真实标签的 one-hot 编码相乘再求和
        results.append(
            (F.softmax(logits, dim=-1) * F.one_hot(y, num_classes=1000)).sum(dim=-1).detach()
        )
    results = torch.cat(results).cpu()  # 将结果拼接为一个 tensor，并转移到 CPU 上
    assert len(results) == len(anchor_indices_list)  # 检查结果数量是否与锚点索引列表的长度相同
    torch.save(results, save_path)  # 保存结果到指定路径
```

- 如果实验类型是`img_recover`，则加载之前保存的表示恢复结果，并使用图像逆推算法生成逆推图像，并保存结果

具体来说：首先，加载之前保存的真实表示和恢复表示的结果，以及表示恢复准确度的结果。然后，根据特定的类别和索引选择目标表示，并使用配置参数初始化图像恢复器。接下来，对每个目标表示进行图像恢复，得到最优恢复图像和所有恢复图像，并将它们保存到指定的文件夹中。最后的结果将按照类别、恢复准确度和恢复方式进行组织和保存

由于这段代码并不是很重要，因此报告中不再给出，具体可以见代码

`inversion.py`主要实现了一个用于图像反演的算法(即论文中提出的**联邦学习成员推断攻击算法**),流程图如下：

```mermaid
graph LR
B[初始化配置参数]
C[初始化统计信息]
D[生成随机初始图像]
E[迭代优化图像]
F[计算前向输出和目标表示]
G[计算反演损失]
H[计算正则化项]
I[计算总损失函数]
J[更新图像]
K[记录统计信息]
L[检查是否达到最大迭代次数]
M[选择最优图像]
N[结束]

B --> C
C --> D
D --> E
E --> F
F --> G
G --> I
H --> I
I --> J
J --> K
E --> L
L --> M
M --> E
E --> N

```

主要的函数是`RepInversion`类中的`invert`方法，该方法通过迭代优化图像，将输入图像转化为与给定目标表示最相似的图像: 使用了配置参数进行初始化，并通过多次重启来得到最优的反演结果。在每次迭代中，算法根据模型的前向输出和目标表示计算反演损失，并结合正则化项来构建总损失函数。然后，通过优化器更新图像以最小化损失，并在每次迭代结束后记录相关的统计信息。最终，根据最佳损失选择最优的反演结果，并返回最优图像、所有图像和统计信息

其他辅助函数包括计算总变差、填充噪声、转换图像数据类型等不再赘述

```python
    # 反转函数，用于反演优化图像
    def invert(self, model, targets, bs=None, img_shape=(224, 224)):
        # 假设model是AttackerModel的实例
        assert isinstance(model, AttackerModel) or isinstance(model.module, AttackerModel)
        assert isinstance(targets, torch.Tensor) and len(targets.shape) == 2
        model.eval()
        if bs is None:
            bs = targets.size(0)
        # 初始化
        stats = defaultdict(list)  # 统计信息，默认字典列表
        torch.manual_seed(self.config['seed'])  # 设置随机种子
        random.seed(self.config['seed'])
        x = torch.randn((self.config['restarts'], bs, 3, *img_shape)).cuda() / 2 + 0.5  # 随机生成初始图像
        x = torch.clamp(x, 0, 1)  # 对图像进行范围限制[0, 1]
        # 多次试验并根据损失选择最佳结果
        scores = torch.zeros((self.config['restarts'], bs))  # 损失值
        all_stats = []  # 所有统计信息
        try:
            for trial in tqdm(range(self.config['restarts']), total=self.config['restarts'], leave=False, position=0,
                              desc='Trial'):
                x_trial, score, stats = self._run_default(model, x[trial], targets)  # 运行默认的优化方法得到反演图像

                x[trial] = x_trial
                scores[trial] = score
                all_stats.append(stats)
        except KeyboardInterrupt:
            print('Trial procedure manually interrupted.')
            pass
        x_optimal = torch.zeros((bs, 3, *img_shape)).cuda()  # 最优图像
        final_stats = []  # 最终统计信息
        for bi in range(bs):
            sample_scores = scores[:, bi]
            sample_scores = sample_scores[torch.isfinite(sample_scores)]  # 处理NaN和无穷大的得分值
            optimal_index = torch.argmin(sample_scores)  # 找到最小得分对应的索引
            x_optimal[bi] = x[optimal_index][bi]  # 选择最优图像
            final_stats.append(
                all_stats[optimal_index][bi]  # 选择最优统计信息
            )
        return x_optimal, x, final_stats  # 返回最优图像、所有图像和最终统计信息
```

# 实验结果分析

### Privacy Risks of Securing Machine Learning Models against Adversarial Examples

重新训练模型后再进行测试，发现运行结果与论文中给出的结果误差几乎很小,具体来说，数据为：

`Yale Face`中：

<img src="pic/1.jpg" style="zoom:40%;" />

`FMNIST`中：

<img src="pic/2.jpg" style="zoom:40%;" />

`CIFAR10`中：

<img src="pic/3.jpg" style="zoom:40%;" />

实验表明，与自然训练（不设防）方法相比，对抗性防御方法确实会增加目标模型抵御成员推理攻击的风险——与自然模型相比，鲁棒模型使成员推理优势对于 `Yale Face`、`Fashion MNIST` 和 `CIFAR10`，分别增加了 $3.2 $倍、$2 $倍、$3.5 $倍。当使用对抗性防御训练鲁棒模型时，成员推理的优势与自然不设防模型相比高达 $4.5 $倍

另外，还可以发现，基于对抗性实例预测置信度（$\mathcal{I}_A$）的隶属度推理攻击比基于良性实例预测置信度（$\mathcal{I}_B$）的推理攻击具有更高的推理精度。另一方面，对于自然模型，基于良性实例的预测置信度的推理攻击会导致更高的推断精度。这是因为成员推断攻击依赖于训练点和测试点的置信度分布之间的差异。对于鲁棒模型，大多数训练点（在经验上）对对抗性的例子是安全的，并且对抗性的扰动不会显著降低对它们的置信度。然而，测试集包含更多的不安全点，因此对抗性扰动将扩大训练例和测试例的置信分布之间的差距，从而导致更高的信息泄露

通过复现本篇实验，让我们意识到了安全性和隐私性结合起来考虑的重要性，在设计对抗性示例的方法时，需要考虑成员推断风险

### Privacy Leakage of Adversarial Training Models in Federated Learning Systems

首先是是验证该篇论文的作者提出的联邦学习成员推断攻击方法的效果

##### 训练图片和联邦成员推断攻击效果（使用对抗训练模型）

我使用的模型是`ResNet50`, 第一行是对使用对抗训练模型进行联邦成员推断攻击恢复的图片，第二行是真实用于训练的图片，可以发现，上下图片人眼观察的差别非常小(我值选取了随机挑的5张展示)

<img src="pic/19.png" style="zoom:60%;" />

下图为原论文中提供的效果图，数字表示两幅图片的相似度（**LPIPS score**），可以发现，不仅是人眼的差异很小，其恢复率大概在45%左右，这说明了隐私泄露的风险非常大

<img src="pic/14.png" style="zoom:67%;" />



##### 自然模型和对抗模型恢复效果对比

我使用的模型是`ResNet50`, 第一行是对使用正常模型进行联邦成员推断攻击恢复的图片，第二行是对使用对抗训练模型进行联邦成员推断攻击恢复的图片，第三行是真实用于训练的图片，可以发现，鲁棒模型使得隐私泄露的风险大为增加，而正常模型几乎不怎么泄露信息。可以发现，第二篇论文作者提出的成员推断攻击基本能让联邦学习的隐私保护失效，而第一篇论文中，虽然成员推断攻击在鲁棒模型上的成功率肉眼可见的提高了，但是并没有这么明显，有非常大的直观震撼

<img src="pic/18.png" style="zoom:67%;" />

# 拓展延申

上面复现的论文的数据都是属于计算机视觉(**CV**)范畴，讨论的范围也处于**欧式空间数据集**范畴内，但是实际上可以扩展到**非欧式空间数据集**上（在更高维的数据表达能力，主要是**图数据**）事实上也存在着这样的现在，即对对抗样本攻击防御能力更好的鲁棒模型更容易被成员推断攻击

<img src="pic/06.png" style="zoom:60%;" />

在`GCN`上应用同样的思路，应用`Cora`、`Cora-ml`、`Citeseer`、`Polblogs`和`Pubmed`这些数据集，采用随机对抗训练(**Random Remove Adversarial Training**)  LATGCN对抗训练  (**LATGCN Adversarial Training**)  的防御的鲁棒模型和正常的模型对比

<img src="pic/07.png" style="zoom:50%;" />

结果表明，**经过对抗训练后的鲁棒模型可以提高成员推理攻击的成功率**。并且通过分析还可以发现，输入鲁棒模型的训练集和测试集之间的**损失函数分布差异**是鲁棒模型可以提高成员推理攻击成功率的主要原因(左侧为**正常模型**，右侧为**鲁棒模型**)[[3]](https://dl.acm.org/doi/abs/10.1007/978-3-031-18067-5_19)。

<img src="pic/08.png" style="zoom:60%;" />

此外，另一个方面，我们验证的都是用于防御对抗样本攻击的对抗训练和隐私性的成员推断以及模型隐私泄露问题，但是实际上，对抗训练也会使得**模型反转攻击**( **model inversion attacks**)也变得更容易了 [[7]](https://arxiv.org/abs/1906.06449)。 并且，证实传统训练模型的输入空间主要由对抗性示例（强烈激活某个类但缺乏语义的数据点）主导，这使得进行模型反转攻击非常困难，而对抗训练则改变了这一特性，反而使得模型反转攻击变得容易。从下图可以看到，经过对抗训练的图片（来自`CIFAR100`）非常容易被模型反转攻击。

![](pic/13.png)

# 总结与思考

本文首次将安全方面的对抗样本攻击与隐私方面的成员推断相联系，为**安全**和**隐私**的联系之间的关系建立了联系，其实不仅如此，我们在人工智能安全这门课程种学习的准确性、**公平性**、**鲁棒性**和**隐私性**、还有**可解释性**，其实是一个紧密联系的整体并且相互之间存在着制约和权衡关系（如下图所示），而不是孤立的。

上完这门课程之后，我们意识到了在构建人工智能系统或者机器学习模型的时候要充分考虑各种安全因素，可信机器学习[[4]](https://dl.acm.org/doi/10.1145/3546872)的目标是在准确性、公平性、鲁棒性和隐私性之间实现可控的四向权衡。而通过复现论文，我进一步认识到了考虑各种安全因素的时候需要做好权衡（有研究表明，共同实现多个愿望是非常困难的[2](https://ieeexplore.ieee.org/document/9933776)）

<img src="pic/05.png" style="zoom: 40%;" />

# 参考文献

[1] [Membership Inference Attacks on Machine Learning: A Survey](https://dl.acm.org/doi/10.1145/3523273)

[2] [An Adversarial Perspective on Accuracy, Robustness, Fairness, and Privacy: Multilateral-Tradeoffs in Trustworthy ML](https://ieeexplore.ieee.org/document/9933776)

[3] [Membership Inference Attacks Against Robust Graph Neural Network](https://dl.acm.org/doi/abs/10.1007/978-3-031-18067-5_19)

[4] [Trustworthy AI: A Computational Perspective](https://dl.acm.org/doi/10.1145/3546872)

[5] [Automatic Fairness Testing of Neural Classifiers Through Adversarial Sampling](https://ieeexplore.ieee.org/document/9506918)

[6] [Analyzing and Defending against Membership Inference Attacks in Natural Language Processing Classification](https://ieeexplore.ieee.org/document/10020711)

[7] [Robust or Private? Adversarial Training Makes Models More Vulnerable to Privacy Attacks](https://arxiv.org/abs/1906.06449)

[8] [Privacy Leakage of Adversarial Training Models in Federated Learning Systems](https://ieeexplore.ieee.org/document/9857077)

[9] [Feature Purification: How Adversarial Training Performs Robust Deep Learning](https://arxiv.org/abs/2005.10190)
