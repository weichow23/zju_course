{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python",
   "version": "3.7.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "toc-showcode": true
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## 1. 数据同步"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. 导入依赖库"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import codecs\n",
    "from pathlib import Path\n",
    "\n",
    "import mindspore\n",
    "import mindspore.dataset as ds\n",
    "import mindspore.nn as nn\n",
    "from mindspore import Tensor\n",
    "from mindspore import context\n",
    "from mindspore.train.model import Model\n",
    "from mindspore.nn.metrics import Accuracy\n",
    "from mindspore.train.serialization import load_checkpoint, load_param_into_net\n",
    "from mindspore.train.callback import ModelCheckpoint, CheckpointConfig, LossMonitor, TimeMonitor\n",
    "from mindspore.ops import operations as ops"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:40:39.371386500Z",
     "start_time": "2024-03-18T06:40:37.978349800Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. 超参数设置"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from easydict import EasyDict as edict\n",
    "\n",
    "cfg = edict({\n",
    "    'name': 'movie review',  # 模型名称为电影评论\n",
    "    'pre_trained': False,  # 是否使用预训练模型，这里设置为 False\n",
    "    'num_classes': 2,  # 类别数量为 2\n",
    "    'batch_size': 64,  # 批处理大小为 64\n",
    "    'epoch_size': 4,  # 训练周期为 4\n",
    "    'weight_decay': 3e-5,  # 权重衰减系数为 3e-5\n",
    "    'data_path': './data/',  # 数据路径为当前目录下的 data 文件夹\n",
    "    'device_target': 'Ascend',  # 设备类型为 Ascend\n",
    "    'device_id': 0,  # 设备 ID 为 0\n",
    "    'keep_checkpoint_max': 1,  # 最大保留的检查点数量为 1\n",
    "    'checkpoint_path': './ckpt/train_textcnn-4_149.ckpt',  # 检查点保存路径为当前目录下的 ckpt 文件夹\n",
    "    'word_len': 51,  # 单词长度为 51\n",
    "    'vec_length': 40  # 词向量长度为 40\n",
    "})"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:02.541147100Z",
     "start_time": "2024-03-18T06:41:02.525141700Z"
    }
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "context.set_context(mode=context.GRAPH_MODE, device_target=cfg.device_target, device_id=cfg.device_id)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:03.645399500Z",
     "start_time": "2024-03-18T06:41:03.292853Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:03.207.2 [mindspore/run_check/_check_version.py:348] Using custom Ascend AI software package (Ascend Data Center Solution) path, package version checking is skipped. Please make sure Ascend AI software package (Ascend Data Center Solution) version is supported. For details, refer to the installation guidelines https://www.mindspore.cn/install\n",
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:03.387.1 [mindspore/run_check/_check_version.py:456] Can not find ccec_compiler(need by mindspore-ascend). Please check whether the Environment Variable PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:03.501.1 [mindspore/run_check/_check_version.py:461] Can not find the tbe operator implementation(need by mindspore-ascend). Please check whether the Environment Variable PYTHONPATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:03.601.4 [mindspore/run_check/_check_version.py:468] Can not find driver so(need by mindspore-ascend). Please check whether the Environment Variable LD_LIBRARY_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n",
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:03.692.4 [mindspore/run_check/_check_version.py:473] Can not find opp path (need by mindspore-ascend). Please check whether the Environment Variable ASCEND_OPP_PATH is set. For details, refer to the installation guidelines: https://www.mindspore.cn/install\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly, and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. libge_runner.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_ascend.so.1 failed. libge_runner.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.11.6 failed. libcublas.so.11: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.11.1 failed. libcublas.so.11: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.10.1 failed. libcublas.so.10: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/core/utils/ms_context.cc:361 SetDeviceTargetFromInner\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_context\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mGRAPH_MODE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_target\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcfg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_id\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/mindspore/_checkparam.py:1313\u001B[0m, in \u001B[0;36margs_type_check.<locals>.type_check.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1311\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(value, bound_types[name]):\n\u001B[1;32m   1312\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe parameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbound_types[name]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m, but got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(value)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m-> 1313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/mindspore/context.py:1463\u001B[0m, in \u001B[0;36mset_context\u001B[0;34m(**kwargs)\u001B[0m\n\u001B[1;32m   1461\u001B[0m \u001B[38;5;66;03m# set device target first\u001B[39;00m\n\u001B[1;32m   1462\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdevice_target\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;129;01min\u001B[39;00m kwargs:\n\u001B[0;32m-> 1463\u001B[0m     \u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_device_target\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mdevice_target\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1464\u001B[0m device \u001B[38;5;241m=\u001B[39m ctx\u001B[38;5;241m.\u001B[39mget_param(ms_ctx_param\u001B[38;5;241m.\u001B[39mdevice_target)\n\u001B[1;32m   1465\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems():\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/mindspore/context.py:385\u001B[0m, in \u001B[0;36m_Context.set_device_target\u001B[0;34m(self, target)\u001B[0m\n\u001B[1;32m    383\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReset device target to CPU when set_device_target.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    384\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPU\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 385\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_param\u001B[49m\u001B[43m(\u001B[49m\u001B[43mms_ctx_param\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice_target\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menable_debug_runtime \u001B[38;5;129;01mand\u001B[39;00m target \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCPU\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    387\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mset_backend_policy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvm\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/nlp/lib/python3.8/site-packages/mindspore/context.py:175\u001B[0m, in \u001B[0;36m_Context.set_param\u001B[0;34m(self, param, value)\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mset_param\u001B[39m(\u001B[38;5;28mself\u001B[39m, param, value):\n\u001B[0;32m--> 175\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_context_handle\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mset_param\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvalue\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Unsupported device target Ascend. This process only supports one of the ['CPU']. Please check whether the Ascend environment is installed and configured correctly, and check whether current mindspore wheel package was built with \"-e Ascend\". For details, please refer to \"Device load error message\".\n\n----------------------------------------------------\n- Device load error message:\n----------------------------------------------------\nLoad dynamic library: libmindspore_ascend.so.2 failed. libge_runner.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_ascend.so.1 failed. libge_runner.so: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.11.6 failed. libcublas.so.11: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.11.1 failed. libcublas.so.11: cannot open shared object file: No such file or directory\nLoad dynamic library: libmindspore_gpu.so.10.1 failed. libcublas.so.10: cannot open shared object file: No such file or directory\n\n----------------------------------------------------\n- C++ Call Stack: (For framework developers)\n----------------------------------------------------\nmindspore/core/utils/ms_context.cc:361 SetDeviceTargetFromInner\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4. 数据预处理"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# 数据预览\n",
    "with open(\"./data/rt-polarity.neg\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Negative reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))\n",
    "with open(\"./data/rt-polarity.pos\", 'r', encoding='utf-8') as f:\n",
    "        print(\"Positive reivews:\")\n",
    "        for i in range(5):\n",
    "            print(\"[{0}]:{1}\".format(i,f.readline()))"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:05.468152800Z",
     "start_time": "2024-03-18T06:41:05.436116200Z"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reivews:\n",
      "[0]:simplistic , silly and tedious . \n",
      "\n",
      "[1]:it's so laddish and juvenile , only teenage boys could possibly find it funny . \n",
      "\n",
      "[2]:exploitative and largely devoid of the depth or sophistication that would make watching such a graphic treatment of the crimes bearable . \n",
      "\n",
      "[3]:[garbus] discards the potential for pathological study , exhuming instead , the skewed melodrama of the circumstantial situation . \n",
      "\n",
      "[4]:a visually flashy but narratively opaque and emotionally vapid exercise in style and mystification . \n",
      "\n",
      "Positive reivews:\n",
      "[0]:the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal . \n",
      "\n",
      "[1]:the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth . \n",
      "\n",
      "[2]:effective but too-tepid biopic\n",
      "\n",
      "[3]:if you sometimes like to go to the movies to have fun , wasabi is a good place to start . \n",
      "\n",
      "[4]:emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one . \n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "class Generator():\n",
    "    def __init__(self, input_list):\n",
    "        self.input_list=input_list\n",
    "    def __getitem__(self,item):\n",
    "        return (np.array(self.input_list[item][0],dtype=np.int32),\n",
    "                np.array(self.input_list[item][1],dtype=np.int32))\n",
    "    def __len__(self):\n",
    "        return len(self.input_list)\n",
    "\n",
    "\n",
    "class MovieReview:\n",
    "    '''\n",
    "    影评数据集\n",
    "    '''\n",
    "    def __init__(self, root_dir, maxlen, split):\n",
    "        '''\n",
    "        input:\n",
    "            root_dir: 影评数据目录\n",
    "            maxlen: 设置句子最大长度\n",
    "            split: 设置数据集中训练/评估的比例\n",
    "        '''\n",
    "        self.path = root_dir\n",
    "        self.feelMap = {\n",
    "            'neg':0,\n",
    "            'pos':1\n",
    "        }\n",
    "        self.files = []\n",
    "\n",
    "        self.doConvert = False\n",
    "        \n",
    "        mypath = Path(self.path)\n",
    "        if not mypath.exists() or not mypath.is_dir():\n",
    "            print(\"please check the root_dir!\")\n",
    "            raise ValueError\n",
    "\n",
    "        # 在数据目录中找到文件\n",
    "        for root,_,filename in os.walk(self.path):\n",
    "            for each in filename:\n",
    "                self.files.append(os.path.join(root,each))\n",
    "            break\n",
    "\n",
    "        # 确认是否为两个文件.neg与.pos\n",
    "        if len(self.files) != 2:\n",
    "            print(\"There are {} files in the root_dir\".format(len(self.files)))\n",
    "            raise ValueError\n",
    "\n",
    "        # 读取数据\n",
    "        self.word_num = 0\n",
    "        self.maxlen = 0\n",
    "        self.minlen = float(\"inf\")\n",
    "        self.maxlen = float(\"-inf\")\n",
    "        self.Pos = []\n",
    "        self.Neg = []\n",
    "        for filename in self.files:\n",
    "            f = codecs.open(filename, 'r')\n",
    "            ff = f.read()\n",
    "            file_object = codecs.open(filename, 'w', 'utf-8')\n",
    "            file_object.write(ff)\n",
    "            self.read_data(filename)\n",
    "        self.PosNeg = self.Pos + self.Neg\n",
    "\n",
    "        self.text2vec(maxlen=maxlen)\n",
    "        self.split_dataset(split=split)\n",
    "\n",
    "    def read_data(self, filePath):\n",
    "\n",
    "        with open(filePath,'r') as f:\n",
    "            \n",
    "            for sentence in f.readlines():\n",
    "                sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\n",
    "                sentence = sentence.split(' ')\n",
    "                sentence = list(filter(lambda x: x, sentence))\n",
    "                if sentence:\n",
    "                    self.word_num += len(sentence)\n",
    "                    self.maxlen = self.maxlen if self.maxlen >= len(sentence) else len(sentence)\n",
    "                    self.minlen = self.minlen if self.minlen <= len(sentence) else len(sentence)\n",
    "                    if 'pos' in filePath:\n",
    "                        self.Pos.append([sentence,self.feelMap['pos']])\n",
    "                    else:\n",
    "                        self.Neg.append([sentence,self.feelMap['neg']])\n",
    "\n",
    "    def text2vec(self, maxlen):\n",
    "        '''\n",
    "        将句子转化为向量\n",
    "\n",
    "        '''\n",
    "        # Vocab = {word : index}\n",
    "        self.Vocab = dict()\n",
    "\n",
    "        # self.Vocab['None']\n",
    "        for SentenceLabel in self.Pos+self.Neg:\n",
    "            vector = [0]*maxlen\n",
    "            for index, word in enumerate(SentenceLabel[0]):\n",
    "                if index >= maxlen:\n",
    "                    break\n",
    "                if word not in self.Vocab.keys():\n",
    "                    self.Vocab[word] = len(self.Vocab)\n",
    "                    vector[index] = len(self.Vocab) - 1\n",
    "                else:\n",
    "                    vector[index] = self.Vocab[word]\n",
    "            SentenceLabel[0] = vector\n",
    "        self.doConvert = True\n",
    "\n",
    "    def split_dataset(self, split):\n",
    "        '''\n",
    "        分割为训练集与测试集\n",
    "\n",
    "        '''\n",
    "\n",
    "        trunk_pos_size = math.ceil((1-split)*len(self.Pos))\n",
    "        trunk_neg_size = math.ceil((1-split)*len(self.Neg))\n",
    "        trunk_num = int(1/(1-split))\n",
    "        pos_temp=list()\n",
    "        neg_temp=list()\n",
    "        for index in range(trunk_num):\n",
    "            pos_temp.append(self.Pos[index*trunk_pos_size:(index+1)*trunk_pos_size])\n",
    "            neg_temp.append(self.Neg[index*trunk_neg_size:(index+1)*trunk_neg_size])\n",
    "        self.test = pos_temp.pop(2)+neg_temp.pop(2)\n",
    "        self.train = [i for item in pos_temp+neg_temp for i in item]\n",
    "\n",
    "        random.shuffle(self.train)\n",
    "        # random.shuffle(self.test)\n",
    "\n",
    "    def get_dict_len(self):\n",
    "        '''\n",
    "        获得数据集中文字组成的词典长度\n",
    "        '''\n",
    "        if self.doConvert:\n",
    "            return len(self.Vocab)\n",
    "        else:\n",
    "            print(\"Haven't finished Text2Vec\")\n",
    "            return -1\n",
    "\n",
    "    def create_train_dataset(self, epoch_size, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.train), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.train))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        dataset=dataset.repeat(epoch_size)\n",
    "        return dataset\n",
    "\n",
    "    def create_test_dataset(self, batch_size):\n",
    "        dataset = ds.GeneratorDataset(\n",
    "                                        source=Generator(input_list=self.test), \n",
    "                                        column_names=[\"data\",\"label\"], \n",
    "                                        shuffle=False\n",
    "                                        )\n",
    "#         dataset.set_dataset_size(len(self.test))\n",
    "        dataset=dataset.batch(batch_size=batch_size,drop_remainder=True)\n",
    "        return dataset"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:06.314235400Z",
     "start_time": "2024-03-18T06:41:06.269741Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "instance = MovieReview(root_dir=cfg.data_path, maxlen=cfg.word_len, split=0.9)\n",
    "dataset = instance.create_train_dataset(batch_size=cfg.batch_size,epoch_size=cfg.epoch_size)\n",
    "batch_num = dataset.get_dataset_size() "
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:07.484493600Z",
     "start_time": "2024-03-18T06:41:07.117445100Z"
    }
   },
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vocab_size=instance.get_dict_len()\n",
    "print(\"vocab_size:{0}\".format(vocab_size))\n",
    "item =dataset.create_dict_iterator()\n",
    "for i,data in enumerate(item):\n",
    "    if i<1:\n",
    "        print(data)\n",
    "        print(data['data'][1])\n",
    "    else:\n",
    "        break"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:08.208624300Z",
     "start_time": "2024-03-18T06:41:08.136622100Z"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size:18848\n",
      "{'data': Tensor(shape=[64, 51], dtype=Int32, value=\n",
      "[[    0,    64,    48 ...     0,     0,     0],\n",
      " [ 4505,    36,  9998 ...     0,     0,     0],\n",
      " [ 4581,   121,   305 ...     0,     0,     0],\n",
      " ...\n",
      " [  128,    94,    15 ...     0,     0,     0],\n",
      " [  747,  5929, 13564 ...     0,     0,     0],\n",
      " [   72,  1978,  2450 ...     0,     0,     0]]), 'label': Tensor(shape=[64], dtype=Int32, value= [0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, \n",
      " 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, \n",
      " 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0])}\n",
      "[ 4505    36  9998   122  1290    11    82  1381   384  7726    10    82\n",
      "     2    56   330   243   691    55 11555   126    15  1820  2663  3073\n",
      "   122   887    82   102 11556 11557    32     0  2637    11  2638   227\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.模型训练"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1训练参数设置"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "learning_rate = []\n",
    "warm_up = [1e-3 / math.floor(cfg.epoch_size / 5) * (i + 1) for _ in range(batch_num) \n",
    "           for i in range(math.floor(cfg.epoch_size / 5))]\n",
    "shrink = [1e-3 / (16 * (i + 1)) for _ in range(batch_num) \n",
    "          for i in range(math.floor(cfg.epoch_size * 3 / 5))]\n",
    "normal_run = [1e-3 for _ in range(batch_num) for i in \n",
    "              range(cfg.epoch_size - math.floor(cfg.epoch_size / 5) \n",
    "                    - math.floor(cfg.epoch_size * 2 / 5))]\n",
    "learning_rate = learning_rate + warm_up + normal_run + shrink"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:10.643878300Z",
     "start_time": "2024-03-18T06:41:10.643878300Z"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def _weight_variable(shape, factor=0.01):\n",
    "    init_value = np.random.randn(*shape).astype(np.float32) * factor\n",
    "    return Tensor(init_value)\n",
    "\n",
    "\n",
    "def make_conv_layer(kernel_size):\n",
    "    weight_shape = (96, 1, *kernel_size)\n",
    "    weight = _weight_variable(weight_shape)\n",
    "    return nn.Conv2d(in_channels=1, out_channels=96, kernel_size=kernel_size, padding=1,\n",
    "                     pad_mode=\"pad\", weight_init=weight, has_bias=True)\n",
    "\n",
    "\n",
    "class TextCNN(nn.Cell):\n",
    "    def __init__(self, vocab_len, word_len, num_classes, vec_length):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vec_length = vec_length\n",
    "        self.word_len = word_len\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.unsqueeze = ops.ExpandDims()\n",
    "        self.embedding = nn.Embedding(vocab_len, self.vec_length, embedding_table='normal')\n",
    "\n",
    "        self.slice = ops.Slice()\n",
    "        self.layer1 = self.make_layer(kernel_height=3)\n",
    "        self.layer2 = self.make_layer(kernel_height=4)\n",
    "        self.layer3 = self.make_layer(kernel_height=5)\n",
    "\n",
    "        self.concat = ops.Concat(1)\n",
    "\n",
    "        self.fc = nn.Dense(96*3, self.num_classes)\n",
    "        self.drop = nn.Dropout(keep_prob=0.5)\n",
    "        self.print = ops.Print()\n",
    "        self.reducemean = ops.ReduceMax(keep_dims=False)\n",
    "        \n",
    "    def make_layer(self, kernel_height):\n",
    "        return nn.SequentialCell(\n",
    "            [\n",
    "                make_conv_layer((kernel_height,self.vec_length)),\n",
    "                nn.ReLU(),\n",
    "                nn.MaxPool2d(kernel_size=(self.word_len-kernel_height+1,1)),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def construct(self,x):\n",
    "        x = self.unsqueeze(x, 1)\n",
    "        x = self.embedding(x)\n",
    "        x1 = self.layer1(x)\n",
    "        x2 = self.layer2(x)\n",
    "        x3 = self.layer3(x)\n",
    "\n",
    "        x1 = self.reducemean(x1, (2, 3))\n",
    "        x2 = self.reducemean(x2, (2, 3))\n",
    "        x3 = self.reducemean(x3, (2, 3))\n",
    "\n",
    "        x = self.concat((x1, x2, x3))\n",
    "        x = self.drop(x)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:11.584266400Z",
     "start_time": "2024-03-18T06:41:11.550985200Z"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "net = TextCNN(vocab_len=instance.get_dict_len(), word_len=cfg.word_len, \n",
    "              num_classes=cfg.num_classes, vec_length=cfg.vec_length)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:12.633096700Z",
     "start_time": "2024-03-18T06:41:12.606258400Z"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:12.331.362 [mindspore/nn/layer/basic.py:173] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "print(net)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:13.418889500Z",
     "start_time": "2024-03-18T06:41:13.358891100Z"
    }
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:41:13.653.50 [mindspore/nn/layer/basic.py:199] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TextCNN<\n",
      "  (embedding): Embedding<vocab_size=18848, embedding_size=40, use_one_hot=False, embedding_table=Parameter (name=embedding.embedding_table, shape=(18848, 40), dtype=Float32, requires_grad=True), dtype=Float32, padding_idx=None>\n",
      "  (layer1): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(3, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[-0.00987806  0.00604594  0.01269283 ...  0.01047935 -0.00079414\n",
      "        -0.01178053]\n",
      "       [ 0.0023246   0.01473438  0.00999208 ...  0.00846373  0.0118795\n",
      "        -0.01499506]\n",
      "       [ 0.00467281 -0.00874286 -0.00976426 ...  0.00837825  0.01720829\n",
      "        -0.02546438]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00331736  0.01012791  0.00878935 ... -0.00333645  0.00706262\n",
      "        -0.00581789]\n",
      "       [ 0.00528282 -0.00643445  0.00979384 ... -0.00415171 -0.02167362\n",
      "        -0.00286473]\n",
      "       [-0.01213917  0.0017954   0.00509843 ...  0.00016002  0.00566993\n",
      "         0.0072583 ]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00323063 -0.00724301  0.00414226 ...  0.00646748  0.00255927\n",
      "         0.00623545]\n",
      "       [-0.00279234 -0.01820662  0.01160544 ... -0.00198937 -0.02212431\n",
      "         0.00662907]\n",
      "       [-0.0083223   0.00621312  0.00231984 ...  0.00739012  0.00380732\n",
      "         0.00054919]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 0.01223056  0.00567692  0.00891419 ...  0.00625902  0.01721979\n",
      "         0.00972476]\n",
      "       [-0.00344099  0.00031374 -0.00798991 ...  0.00040391  0.01151788\n",
      "        -0.00801151]\n",
      "       [ 0.00748463  0.01450889  0.00593882 ... -0.01005641 -0.01644546\n",
      "        -0.01068023]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00082341  0.01370119 -0.00400853 ...  0.00594589  0.00270241\n",
      "         0.01021186]\n",
      "       [ 0.00642821  0.02190828 -0.01329542 ...  0.00179583 -0.00424051\n",
      "        -0.00569281]\n",
      "       [ 0.00965408  0.00542384 -0.00082932 ...  0.00016055  0.00051499\n",
      "         0.02097965]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00480591  0.01739701  0.01416487 ... -0.00514057 -0.00688112\n",
      "        -0.00263704]\n",
      "       [-0.00109638  0.01803881 -0.00390224 ... -0.0197487  -0.00242315\n",
      "         0.00011116]\n",
      "       [-0.00416996 -0.00594844  0.00919988 ...  0.00941074 -0.00368322\n",
      "        -0.00700553]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x7fe51f5e6190>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(49, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer2): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(4, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 0.01154197 -0.0068967   0.00872287 ... -0.01593798  0.01362807\n",
      "         0.00591885]\n",
      "       [ 0.01727011 -0.00449348 -0.0072731  ...  0.01069574  0.00680083\n",
      "        -0.01040805]\n",
      "       [ 0.00644687 -0.01209655  0.00605031 ...  0.0086873  -0.00757475\n",
      "        -0.00286124]\n",
      "       [ 0.00870035  0.00065989  0.00450712 ... -0.00684034 -0.00114998\n",
      "         0.00441578]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01831605  0.01001136  0.0096105  ... -0.0076918   0.00548692\n",
      "        -0.00142662]\n",
      "       [ 0.01227854  0.00512255  0.01008996 ... -0.01186586 -0.01716293\n",
      "        -0.0007014 ]\n",
      "       [-0.00343239 -0.00164785 -0.00544504 ...  0.0118551  -0.00091037\n",
      "        -0.01595048]\n",
      "       [-0.00247698  0.0121947   0.0185612  ... -0.01830374 -0.0167146\n",
      "         0.00171074]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.0109091  -0.00497174  0.00291635 ... -0.01838101  0.00401816\n",
      "        -0.02758718]\n",
      "       [ 0.00182438  0.00947802 -0.00725767 ... -0.00251499 -0.00418601\n",
      "        -0.01312004]\n",
      "       [-0.00305834  0.00637376 -0.01278289 ... -0.00893946 -0.00687254\n",
      "         0.00941824]\n",
      "       [ 0.01184243  0.00917775  0.01018301 ... -0.00744484  0.01168538\n",
      "         0.00113237]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[-0.00102553 -0.00662056 -0.00519233 ... -0.00149919 -0.01962165\n",
      "        -0.00494464]\n",
      "       [ 0.01100957  0.00095408  0.00271637 ...  0.00766723 -0.01898072\n",
      "         0.0267184 ]\n",
      "       [-0.00957121  0.00298228  0.0020972  ... -0.00050024 -0.00988544\n",
      "        -0.00995382]\n",
      "       [-0.00039598 -0.00820293  0.01020337 ... -0.00363533 -0.0066677\n",
      "         0.00072922]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01916415 -0.01866221 -0.00751616 ... -0.01195653  0.00343223\n",
      "        -0.00061766]\n",
      "       [ 0.00324892  0.00923713 -0.00960665 ... -0.00260345 -0.01063329\n",
      "        -0.00373688]\n",
      "       [ 0.01025325 -0.01202852 -0.00506931 ... -0.02085671  0.00321749\n",
      "         0.01000707]\n",
      "       [-0.00300447 -0.01714513  0.00491582 ...  0.00817975  0.01154485\n",
      "         0.00434592]]]\n",
      "    \n",
      "    \n",
      "     [[[ 0.00285039 -0.00728193  0.00923158 ... -0.00245201  0.0206824\n",
      "        -0.00935669]\n",
      "       [-0.01387001  0.00154427  0.01493564 ...  0.00715476  0.00543285\n",
      "         0.01170922]\n",
      "       [ 0.00302524  0.01896343  0.01232931 ...  0.00183148  0.00268734\n",
      "         0.01889189]\n",
      "       [-0.01106939 -0.00643381 -0.0203492  ... -0.00019856 -0.010813\n",
      "        -0.00331122]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x7fe60cb48850>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(48, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (layer3): SequentialCell<\n",
      "    (0): Conv2d<input_channels=1, output_channels=96, kernel_size=(5, 40), stride=(1, 1), pad_mode=pad, padding=1, dilation=(1, 1), group=1, has_bias=True, weight_init=[[[[ 0.01305797 -0.00075872 -0.01397448 ... -0.00308931 -0.00936808\n",
      "        -0.00353897]\n",
      "       [-0.00318203 -0.00041527  0.01273539 ...  0.01370453 -0.0042874\n",
      "         0.00300605]\n",
      "       [ 0.01142748  0.00809268 -0.00446373 ...  0.01138002 -0.0034976\n",
      "         0.00103645]\n",
      "       [ 0.0011314  -0.01288687  0.00864425 ...  0.01155355  0.00234615\n",
      "        -0.01122932]\n",
      "       [ 0.01653117  0.00064651  0.00147606 ...  0.0044294  -0.01637285\n",
      "        -0.00951355]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.0053533   0.00236015 -0.01726255 ... -0.00534318  0.01648395\n",
      "         0.01175611]\n",
      "       [ 0.01308259  0.01313651  0.00052304 ...  0.0034065   0.00555984\n",
      "         0.00068252]\n",
      "       [ 0.00514437 -0.02249534  0.00183365 ... -0.01659057 -0.00989675\n",
      "         0.01173122]\n",
      "       [ 0.00304111 -0.00718436 -0.00805197 ...  0.01776516 -0.0012624\n",
      "         0.00586242]\n",
      "       [-0.00927618  0.00326819 -0.00782716 ... -0.00794411  0.01867995\n",
      "         0.00970301]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00756885  0.00296982 -0.0093701  ... -0.00548725 -0.00771266\n",
      "        -0.00149902]\n",
      "       [-0.01040949  0.0054166   0.0042678  ... -0.00662747  0.00619476\n",
      "         0.01935671]\n",
      "       [ 0.00805304 -0.00815176  0.00663121 ...  0.00234596 -0.01174829\n",
      "         0.00269068]\n",
      "       [ 0.0073353  -0.00582518 -0.00817361 ... -0.02089047 -0.00646528\n",
      "        -0.00162068]\n",
      "       [-0.02078642  0.00219295  0.00525231 ... -0.00066207  0.00625602\n",
      "         0.00437408]]]\n",
      "    \n",
      "    \n",
      "     ...\n",
      "    \n",
      "    \n",
      "     [[[ 0.01280106 -0.01889026  0.00569307 ...  0.00444935 -0.00433288\n",
      "         0.00631705]\n",
      "       [-0.01531672  0.0122883   0.01318769 ... -0.01233543  0.01908861\n",
      "        -0.00264553]\n",
      "       [ 0.00123061 -0.00141061 -0.00180661 ...  0.00236264  0.00045042\n",
      "         0.00185783]\n",
      "       [-0.01599961 -0.00472917  0.01535761 ... -0.00371983  0.00252438\n",
      "        -0.01548556]\n",
      "       [ 0.00537819 -0.01189824  0.01011372 ...  0.03300034  0.00715607\n",
      "         0.00730184]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.01183684  0.01213102  0.00257673 ... -0.00651872  0.0144964\n",
      "         0.00402977]\n",
      "       [ 0.00581374 -0.00237884 -0.01433987 ...  0.00801258  0.00983911\n",
      "         0.00114301]\n",
      "       [-0.00892487  0.00681786  0.00666842 ...  0.01904866  0.00818188\n",
      "         0.00528825]\n",
      "       [-0.01273285 -0.01222296  0.0068022  ... -0.00781812  0.00502661\n",
      "         0.00829555]\n",
      "       [ 0.01091399 -0.00242651  0.01580045 ...  0.00683933 -0.00056723\n",
      "        -0.00316996]]]\n",
      "    \n",
      "    \n",
      "     [[[-0.00825183  0.00700031 -0.01289379 ...  0.00870997 -0.0032783\n",
      "        -0.00046307]\n",
      "       [-0.00614596 -0.00886795  0.00555566 ... -0.00776234 -0.00111959\n",
      "         0.00222225]\n",
      "       [ 0.00759289 -0.01232207  0.00646157 ...  0.00418584 -0.00216812\n",
      "         0.00177339]\n",
      "       [-0.00104319  0.00206348 -0.02908178 ...  0.0074318  -0.00647837\n",
      "        -0.02347561]\n",
      "       [-0.01916411  0.00858502 -0.01331005 ... -0.01205353  0.00416385\n",
      "        -0.00733449]]]], bias_init=<mindspore.common.initializer.Uniform object at 0x7fe60cb8b790>, format=NCHW>\n",
      "    (1): ReLU<>\n",
      "    (2): MaxPool2d<kernel_size=(47, 1), stride=1, pad_mode=VALID>\n",
      "    >\n",
      "  (fc): Dense<input_channels=288, output_channels=2, has_bias=True>\n",
      "  (drop): Dropout<keep_prob=0.5>\n",
      "  >\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Continue training if set pre_trained to be True\n",
    "if cfg.pre_trained:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    load_param_into_net(net, param_dict)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:14.381154200Z",
     "start_time": "2024-03-18T06:41:14.365169100Z"
    }
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=learning_rate, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:15.372446900Z",
     "start_time": "2024-03-18T06:41:15.320446100Z"
    }
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model = Model(net, loss_fn=loss, optimizer=opt, metrics={'acc': Accuracy()})"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:16.058940800Z",
     "start_time": "2024-03-18T06:41:16.026898100Z"
    }
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "config_ck = CheckpointConfig(save_checkpoint_steps=int(cfg.epoch_size*batch_num/2), keep_checkpoint_max=cfg.keep_checkpoint_max)\n",
    "time_cb = TimeMonitor(data_size=batch_num)\n",
    "ckpt_save_dir = \"./ckpt\"\n",
    "ckpoint_cb = ModelCheckpoint(prefix=\"train_textcnn\", directory=ckpt_save_dir, config=config_ck)\n",
    "loss_cb = LossMonitor()"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:41:16.481173Z",
     "start_time": "2024-03-18T06:41:16.473177700Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model.train(cfg.epoch_size, dataset, callbacks=[time_cb, ckpoint_cb, loss_cb])\n",
    "print(\"train success\")"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:42:25.980889800Z",
     "start_time": "2024-03-18T06:41:16.970121500Z"
    }
   },
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 0.7005386352539062\n",
      "epoch: 1 step: 2, loss is 0.6984915733337402\n",
      "epoch: 1 step: 3, loss is 0.6915239691734314\n",
      "epoch: 1 step: 4, loss is 0.6907325983047485\n",
      "epoch: 1 step: 5, loss is 0.6889535188674927\n",
      "epoch: 1 step: 6, loss is 0.6943837404251099\n",
      "epoch: 1 step: 7, loss is 0.6954647302627563\n",
      "epoch: 1 step: 8, loss is 0.6942404508590698\n",
      "epoch: 1 step: 9, loss is 0.6933318376541138\n",
      "epoch: 1 step: 10, loss is 0.6962960958480835\n",
      "epoch: 1 step: 11, loss is 0.6960465908050537\n",
      "epoch: 1 step: 12, loss is 0.6896361112594604\n",
      "epoch: 1 step: 13, loss is 0.6968711614608765\n",
      "epoch: 1 step: 14, loss is 0.6962203979492188\n",
      "epoch: 1 step: 15, loss is 0.6957709789276123\n",
      "epoch: 1 step: 16, loss is 0.694354236125946\n",
      "epoch: 1 step: 17, loss is 0.6927305459976196\n",
      "epoch: 1 step: 18, loss is 0.692097544670105\n",
      "epoch: 1 step: 19, loss is 0.6906329393386841\n",
      "epoch: 1 step: 20, loss is 0.6932912468910217\n",
      "epoch: 1 step: 21, loss is 0.6948168277740479\n",
      "epoch: 1 step: 22, loss is 0.6957204937934875\n",
      "epoch: 1 step: 23, loss is 0.6932578086853027\n",
      "epoch: 1 step: 24, loss is 0.6931109428405762\n",
      "epoch: 1 step: 25, loss is 0.6931941509246826\n",
      "epoch: 1 step: 26, loss is 0.6927797794342041\n",
      "epoch: 1 step: 27, loss is 0.6913745403289795\n",
      "epoch: 1 step: 28, loss is 0.6915971040725708\n",
      "epoch: 1 step: 29, loss is 0.6942541599273682\n",
      "epoch: 1 step: 30, loss is 0.6928845643997192\n",
      "epoch: 1 step: 31, loss is 0.6927120089530945\n",
      "epoch: 1 step: 32, loss is 0.6933352947235107\n",
      "epoch: 1 step: 33, loss is 0.6942141652107239\n",
      "epoch: 1 step: 34, loss is 0.6926680207252502\n",
      "epoch: 1 step: 35, loss is 0.6960951089859009\n",
      "epoch: 1 step: 36, loss is 0.6896533370018005\n",
      "epoch: 1 step: 37, loss is 0.6939111948013306\n",
      "epoch: 1 step: 38, loss is 0.6975258588790894\n",
      "epoch: 1 step: 39, loss is 0.6928281784057617\n",
      "epoch: 1 step: 40, loss is 0.6939948201179504\n",
      "epoch: 1 step: 41, loss is 0.6942200064659119\n",
      "epoch: 1 step: 42, loss is 0.6925995349884033\n",
      "epoch: 1 step: 43, loss is 0.692451000213623\n",
      "epoch: 1 step: 44, loss is 0.6941678524017334\n",
      "epoch: 1 step: 45, loss is 0.6951467990875244\n",
      "epoch: 1 step: 46, loss is 0.6908077597618103\n",
      "epoch: 1 step: 47, loss is 0.6915605664253235\n",
      "epoch: 1 step: 48, loss is 0.6925517320632935\n",
      "epoch: 1 step: 49, loss is 0.6891717314720154\n",
      "epoch: 1 step: 50, loss is 0.6934881210327148\n",
      "epoch: 1 step: 51, loss is 0.6925686001777649\n",
      "epoch: 1 step: 52, loss is 0.6949641704559326\n",
      "epoch: 1 step: 53, loss is 0.6893460154533386\n",
      "epoch: 1 step: 54, loss is 0.6913049817085266\n",
      "epoch: 1 step: 55, loss is 0.6891237497329712\n",
      "epoch: 1 step: 56, loss is 0.688644289970398\n",
      "epoch: 1 step: 57, loss is 0.6904609203338623\n",
      "epoch: 1 step: 58, loss is 0.694412112236023\n",
      "epoch: 1 step: 59, loss is 0.692289412021637\n",
      "epoch: 1 step: 60, loss is 0.6927217245101929\n",
      "epoch: 1 step: 61, loss is 0.6913010478019714\n",
      "epoch: 1 step: 62, loss is 0.6879810690879822\n",
      "epoch: 1 step: 63, loss is 0.6892766952514648\n",
      "epoch: 1 step: 64, loss is 0.6896621584892273\n",
      "epoch: 1 step: 65, loss is 0.6907520890235901\n",
      "epoch: 1 step: 66, loss is 0.6921786069869995\n",
      "epoch: 1 step: 67, loss is 0.6839030981063843\n",
      "epoch: 1 step: 68, loss is 0.6875854730606079\n",
      "epoch: 1 step: 69, loss is 0.6810402870178223\n",
      "epoch: 1 step: 70, loss is 0.6865066289901733\n",
      "epoch: 1 step: 71, loss is 0.6812816858291626\n",
      "epoch: 1 step: 72, loss is 0.6920930743217468\n",
      "epoch: 1 step: 73, loss is 0.688499927520752\n",
      "epoch: 1 step: 74, loss is 0.694210410118103\n",
      "epoch: 1 step: 75, loss is 0.6896318197250366\n",
      "epoch: 1 step: 76, loss is 0.6851995587348938\n",
      "epoch: 1 step: 77, loss is 0.6850311160087585\n",
      "epoch: 1 step: 78, loss is 0.6860195398330688\n",
      "epoch: 1 step: 79, loss is 0.6728864312171936\n",
      "epoch: 1 step: 80, loss is 0.6926105618476868\n",
      "epoch: 1 step: 81, loss is 0.6768491864204407\n",
      "epoch: 1 step: 82, loss is 0.6806908249855042\n",
      "epoch: 1 step: 83, loss is 0.6782400608062744\n",
      "epoch: 1 step: 84, loss is 0.676243782043457\n",
      "epoch: 1 step: 85, loss is 0.6822251081466675\n",
      "epoch: 1 step: 86, loss is 0.6762500405311584\n",
      "epoch: 1 step: 87, loss is 0.6900907158851624\n",
      "epoch: 1 step: 88, loss is 0.6714035868644714\n",
      "epoch: 1 step: 89, loss is 0.6837304830551147\n",
      "epoch: 1 step: 90, loss is 0.6731750965118408\n",
      "epoch: 1 step: 91, loss is 0.6670985817909241\n",
      "epoch: 1 step: 92, loss is 0.6747384071350098\n",
      "epoch: 1 step: 93, loss is 0.6504760384559631\n",
      "epoch: 1 step: 94, loss is 0.6553836464881897\n",
      "epoch: 1 step: 95, loss is 0.6558379530906677\n",
      "epoch: 1 step: 96, loss is 0.648478627204895\n",
      "epoch: 1 step: 97, loss is 0.6495418548583984\n",
      "epoch: 1 step: 98, loss is 0.6454397439956665\n",
      "epoch: 1 step: 99, loss is 0.6395455598831177\n",
      "epoch: 1 step: 100, loss is 0.6344641447067261\n",
      "epoch: 1 step: 101, loss is 0.6477690935134888\n",
      "epoch: 1 step: 102, loss is 0.6377146244049072\n",
      "epoch: 1 step: 103, loss is 0.6152121424674988\n",
      "epoch: 1 step: 104, loss is 0.6327663660049438\n",
      "epoch: 1 step: 105, loss is 0.6293030977249146\n",
      "epoch: 1 step: 106, loss is 0.6349468231201172\n",
      "epoch: 1 step: 107, loss is 0.646436333656311\n",
      "epoch: 1 step: 108, loss is 0.6154660582542419\n",
      "epoch: 1 step: 109, loss is 0.5867294073104858\n",
      "epoch: 1 step: 110, loss is 0.6001779437065125\n",
      "epoch: 1 step: 111, loss is 0.5893782377243042\n",
      "epoch: 1 step: 112, loss is 0.6466838717460632\n",
      "epoch: 1 step: 113, loss is 0.530272364616394\n",
      "epoch: 1 step: 114, loss is 0.6149637699127197\n",
      "epoch: 1 step: 115, loss is 0.5597596168518066\n",
      "epoch: 1 step: 116, loss is 0.5931622385978699\n",
      "epoch: 1 step: 117, loss is 0.598254382610321\n",
      "epoch: 1 step: 118, loss is 0.5468671321868896\n",
      "epoch: 1 step: 119, loss is 0.5479224920272827\n",
      "epoch: 1 step: 120, loss is 0.6566981673240662\n",
      "epoch: 1 step: 121, loss is 0.5095094442367554\n",
      "epoch: 1 step: 122, loss is 0.5921903848648071\n",
      "epoch: 1 step: 123, loss is 0.5528651475906372\n",
      "epoch: 1 step: 124, loss is 0.5902186036109924\n",
      "epoch: 1 step: 125, loss is 0.5906057357788086\n",
      "epoch: 1 step: 126, loss is 0.6126166582107544\n",
      "epoch: 1 step: 127, loss is 0.6626044511795044\n",
      "epoch: 1 step: 128, loss is 0.5256378054618835\n",
      "epoch: 1 step: 129, loss is 0.5013964772224426\n",
      "epoch: 1 step: 130, loss is 0.5297213792800903\n",
      "epoch: 1 step: 131, loss is 0.546101450920105\n",
      "epoch: 1 step: 132, loss is 0.6708712577819824\n",
      "epoch: 1 step: 133, loss is 0.6676321029663086\n",
      "epoch: 1 step: 134, loss is 0.6183152198791504\n",
      "epoch: 1 step: 135, loss is 0.5570909380912781\n",
      "epoch: 1 step: 136, loss is 0.5567617416381836\n",
      "epoch: 1 step: 137, loss is 0.5694990754127502\n",
      "epoch: 1 step: 138, loss is 0.5339942574501038\n",
      "epoch: 1 step: 139, loss is 0.5246127843856812\n",
      "epoch: 1 step: 140, loss is 0.5440200567245483\n",
      "epoch: 1 step: 141, loss is 0.6019765138626099\n",
      "epoch: 1 step: 142, loss is 0.4961177706718445\n",
      "epoch: 1 step: 143, loss is 0.5423427820205688\n",
      "epoch: 1 step: 144, loss is 0.5552607774734497\n",
      "epoch: 1 step: 145, loss is 0.5575544834136963\n",
      "epoch: 1 step: 146, loss is 0.49514198303222656\n",
      "epoch: 1 step: 147, loss is 0.5162829160690308\n",
      "epoch: 1 step: 148, loss is 0.4439515471458435\n",
      "epoch: 1 step: 149, loss is 0.6000281572341919\n",
      "epoch: 1 step: 150, loss is 0.6835273504257202\n",
      "epoch: 1 step: 151, loss is 0.46636754274368286\n",
      "epoch: 1 step: 152, loss is 0.5387055277824402\n",
      "epoch: 1 step: 153, loss is 0.5727492570877075\n",
      "epoch: 1 step: 154, loss is 0.5548509359359741\n",
      "epoch: 1 step: 155, loss is 0.46048974990844727\n",
      "epoch: 1 step: 156, loss is 0.4426618218421936\n",
      "epoch: 1 step: 157, loss is 0.49158772826194763\n",
      "epoch: 1 step: 158, loss is 0.5526905655860901\n",
      "epoch: 1 step: 159, loss is 0.5847052335739136\n",
      "epoch: 1 step: 160, loss is 0.46970510482788086\n",
      "epoch: 1 step: 161, loss is 0.5676023364067078\n",
      "epoch: 1 step: 162, loss is 0.3713963031768799\n",
      "epoch: 1 step: 163, loss is 0.563469648361206\n",
      "epoch: 1 step: 164, loss is 0.47348442673683167\n",
      "epoch: 1 step: 165, loss is 0.47636520862579346\n",
      "epoch: 1 step: 166, loss is 0.5037429332733154\n",
      "epoch: 1 step: 167, loss is 0.5068093538284302\n",
      "epoch: 1 step: 168, loss is 0.5791283845901489\n",
      "epoch: 1 step: 169, loss is 0.41092151403427124\n",
      "epoch: 1 step: 170, loss is 0.44935786724090576\n",
      "epoch: 1 step: 171, loss is 0.5460115671157837\n",
      "epoch: 1 step: 172, loss is 0.4560762941837311\n",
      "epoch: 1 step: 173, loss is 0.47203779220581055\n",
      "epoch: 1 step: 174, loss is 0.4344020187854767\n",
      "epoch: 1 step: 175, loss is 0.4409085512161255\n",
      "epoch: 1 step: 176, loss is 0.5046530365943909\n",
      "epoch: 1 step: 177, loss is 0.44271835684776306\n",
      "epoch: 1 step: 178, loss is 0.4062196612358093\n",
      "epoch: 1 step: 179, loss is 0.4299134612083435\n",
      "epoch: 1 step: 180, loss is 0.39911091327667236\n",
      "epoch: 1 step: 181, loss is 0.5621076226234436\n",
      "epoch: 1 step: 182, loss is 0.3643956184387207\n",
      "epoch: 1 step: 183, loss is 0.4687122404575348\n",
      "epoch: 1 step: 184, loss is 0.4739624857902527\n",
      "epoch: 1 step: 185, loss is 0.5285768508911133\n",
      "epoch: 1 step: 186, loss is 0.4356115758419037\n",
      "epoch: 1 step: 187, loss is 0.47192078828811646\n",
      "epoch: 1 step: 188, loss is 0.457184374332428\n",
      "epoch: 1 step: 189, loss is 0.6283218860626221\n",
      "epoch: 1 step: 190, loss is 0.4747762978076935\n",
      "epoch: 1 step: 191, loss is 0.5092973709106445\n",
      "epoch: 1 step: 192, loss is 0.5139466524124146\n",
      "epoch: 1 step: 193, loss is 0.42952728271484375\n",
      "epoch: 1 step: 194, loss is 0.5243483185768127\n",
      "epoch: 1 step: 195, loss is 0.5542731285095215\n",
      "epoch: 1 step: 196, loss is 0.4728679955005646\n",
      "epoch: 1 step: 197, loss is 0.5370668172836304\n",
      "epoch: 1 step: 198, loss is 0.47163087129592896\n",
      "epoch: 1 step: 199, loss is 0.47383183240890503\n",
      "epoch: 1 step: 200, loss is 0.43513765931129456\n",
      "epoch: 1 step: 201, loss is 0.4837920069694519\n",
      "epoch: 1 step: 202, loss is 0.45283961296081543\n",
      "epoch: 1 step: 203, loss is 0.33971959352493286\n",
      "epoch: 1 step: 204, loss is 0.3761216402053833\n",
      "epoch: 1 step: 205, loss is 0.4052150249481201\n",
      "epoch: 1 step: 206, loss is 0.5864062309265137\n",
      "epoch: 1 step: 207, loss is 0.3989657163619995\n",
      "epoch: 1 step: 208, loss is 0.4930773973464966\n",
      "epoch: 1 step: 209, loss is 0.4085867702960968\n",
      "epoch: 1 step: 210, loss is 0.4178831875324249\n",
      "epoch: 1 step: 211, loss is 0.358370840549469\n",
      "epoch: 1 step: 212, loss is 0.4627266526222229\n",
      "epoch: 1 step: 213, loss is 0.4339717924594879\n",
      "epoch: 1 step: 214, loss is 0.5190458297729492\n",
      "epoch: 1 step: 215, loss is 0.42751550674438477\n",
      "epoch: 1 step: 216, loss is 0.40915849804878235\n",
      "epoch: 1 step: 217, loss is 0.41247981786727905\n",
      "epoch: 1 step: 218, loss is 0.4270552098751068\n",
      "epoch: 1 step: 219, loss is 0.378093421459198\n",
      "epoch: 1 step: 220, loss is 0.3493269979953766\n",
      "epoch: 1 step: 221, loss is 0.4006074070930481\n",
      "epoch: 1 step: 222, loss is 0.344759464263916\n",
      "epoch: 1 step: 223, loss is 0.481533020734787\n",
      "epoch: 1 step: 224, loss is 0.43025168776512146\n",
      "epoch: 1 step: 225, loss is 0.4500068426132202\n",
      "epoch: 1 step: 226, loss is 0.3297576308250427\n",
      "epoch: 1 step: 227, loss is 0.3462586998939514\n",
      "epoch: 1 step: 228, loss is 0.4168051481246948\n",
      "epoch: 1 step: 229, loss is 0.3672109544277191\n",
      "epoch: 1 step: 230, loss is 0.47193965315818787\n",
      "epoch: 1 step: 231, loss is 0.3573080003261566\n",
      "epoch: 1 step: 232, loss is 0.37141016125679016\n",
      "epoch: 1 step: 233, loss is 0.278858482837677\n",
      "epoch: 1 step: 234, loss is 0.3215653896331787\n",
      "epoch: 1 step: 235, loss is 0.44963598251342773\n",
      "epoch: 1 step: 236, loss is 0.37782812118530273\n",
      "epoch: 1 step: 237, loss is 0.3623155951499939\n",
      "epoch: 1 step: 238, loss is 0.37918707728385925\n",
      "epoch: 1 step: 239, loss is 0.277132511138916\n",
      "epoch: 1 step: 240, loss is 0.38022106885910034\n",
      "epoch: 1 step: 241, loss is 0.33131587505340576\n",
      "epoch: 1 step: 242, loss is 0.25693538784980774\n",
      "epoch: 1 step: 243, loss is 0.33233875036239624\n",
      "epoch: 1 step: 244, loss is 0.2868938744068146\n",
      "epoch: 1 step: 245, loss is 0.2845149040222168\n",
      "epoch: 1 step: 246, loss is 0.32672393321990967\n",
      "epoch: 1 step: 247, loss is 0.29645565152168274\n",
      "epoch: 1 step: 248, loss is 0.3233014941215515\n",
      "epoch: 1 step: 249, loss is 0.33782970905303955\n",
      "epoch: 1 step: 250, loss is 0.4221738576889038\n",
      "epoch: 1 step: 251, loss is 0.27132904529571533\n",
      "epoch: 1 step: 252, loss is 0.297376811504364\n",
      "epoch: 1 step: 253, loss is 0.33151763677597046\n",
      "epoch: 1 step: 254, loss is 0.3932606279850006\n",
      "epoch: 1 step: 255, loss is 0.3755573034286499\n",
      "epoch: 1 step: 256, loss is 0.35277098417282104\n",
      "epoch: 1 step: 257, loss is 0.25265318155288696\n",
      "epoch: 1 step: 258, loss is 0.3000327944755554\n",
      "epoch: 1 step: 259, loss is 0.360970139503479\n",
      "epoch: 1 step: 260, loss is 0.3229324221611023\n",
      "epoch: 1 step: 261, loss is 0.32161056995391846\n",
      "epoch: 1 step: 262, loss is 0.25727730989456177\n",
      "epoch: 1 step: 263, loss is 0.35516729950904846\n",
      "epoch: 1 step: 264, loss is 0.2599228024482727\n",
      "epoch: 1 step: 265, loss is 0.3501538038253784\n",
      "epoch: 1 step: 266, loss is 0.37321627140045166\n",
      "epoch: 1 step: 267, loss is 0.34405311942100525\n",
      "epoch: 1 step: 268, loss is 0.3937710225582123\n",
      "epoch: 1 step: 269, loss is 0.35750848054885864\n",
      "epoch: 1 step: 270, loss is 0.2280900478363037\n",
      "epoch: 1 step: 271, loss is 0.40288424491882324\n",
      "epoch: 1 step: 272, loss is 0.29787856340408325\n",
      "epoch: 1 step: 273, loss is 0.3513565957546234\n",
      "epoch: 1 step: 274, loss is 0.2838766872882843\n",
      "epoch: 1 step: 275, loss is 0.36434686183929443\n",
      "epoch: 1 step: 276, loss is 0.4154587984085083\n",
      "epoch: 1 step: 277, loss is 0.24576494097709656\n",
      "epoch: 1 step: 278, loss is 0.2378281056880951\n",
      "epoch: 1 step: 279, loss is 0.3387148678302765\n",
      "epoch: 1 step: 280, loss is 0.2933000326156616\n",
      "epoch: 1 step: 281, loss is 0.40416771173477173\n",
      "epoch: 1 step: 282, loss is 0.5014321804046631\n",
      "epoch: 1 step: 283, loss is 0.38519322872161865\n",
      "epoch: 1 step: 284, loss is 0.39753517508506775\n",
      "epoch: 1 step: 285, loss is 0.3795408308506012\n",
      "epoch: 1 step: 286, loss is 0.21984852850437164\n",
      "epoch: 1 step: 287, loss is 0.29441940784454346\n",
      "epoch: 1 step: 288, loss is 0.3133161962032318\n",
      "epoch: 1 step: 289, loss is 0.2697622776031494\n",
      "epoch: 1 step: 290, loss is 0.3959876000881195\n",
      "epoch: 1 step: 291, loss is 0.20569875836372375\n",
      "epoch: 1 step: 292, loss is 0.30070167779922485\n",
      "epoch: 1 step: 293, loss is 0.42623820900917053\n",
      "epoch: 1 step: 294, loss is 0.3839774429798126\n",
      "epoch: 1 step: 295, loss is 0.22145508229732513\n",
      "epoch: 1 step: 296, loss is 0.369057297706604\n",
      "epoch: 1 step: 297, loss is 0.1998269408941269\n",
      "epoch: 1 step: 298, loss is 0.33046460151672363\n",
      "epoch: 1 step: 299, loss is 0.40122556686401367\n",
      "epoch: 1 step: 300, loss is 0.23445916175842285\n",
      "epoch: 1 step: 301, loss is 0.27516835927963257\n",
      "epoch: 1 step: 302, loss is 0.25850430130958557\n",
      "epoch: 1 step: 303, loss is 0.35387444496154785\n",
      "epoch: 1 step: 304, loss is 0.2210286408662796\n",
      "epoch: 1 step: 305, loss is 0.2881050109863281\n",
      "epoch: 1 step: 306, loss is 0.2732706069946289\n",
      "epoch: 1 step: 307, loss is 0.2860637307167053\n",
      "epoch: 1 step: 308, loss is 0.2992192506790161\n",
      "epoch: 1 step: 309, loss is 0.23063695430755615\n",
      "epoch: 1 step: 310, loss is 0.2852301001548767\n",
      "epoch: 1 step: 311, loss is 0.12987610697746277\n",
      "epoch: 1 step: 312, loss is 0.34904050827026367\n",
      "epoch: 1 step: 313, loss is 0.22563061118125916\n",
      "epoch: 1 step: 314, loss is 0.2380324900150299\n",
      "epoch: 1 step: 315, loss is 0.23879273235797882\n",
      "epoch: 1 step: 316, loss is 0.2764020562171936\n",
      "epoch: 1 step: 317, loss is 0.3466779291629791\n",
      "epoch: 1 step: 318, loss is 0.2592734694480896\n",
      "epoch: 1 step: 319, loss is 0.367750883102417\n",
      "epoch: 1 step: 320, loss is 0.32737070322036743\n",
      "epoch: 1 step: 321, loss is 0.3161391019821167\n",
      "epoch: 1 step: 322, loss is 0.289761483669281\n",
      "epoch: 1 step: 323, loss is 0.2497188150882721\n",
      "epoch: 1 step: 324, loss is 0.20717203617095947\n",
      "epoch: 1 step: 325, loss is 0.26832616329193115\n",
      "epoch: 1 step: 326, loss is 0.18359479308128357\n",
      "epoch: 1 step: 327, loss is 0.2373877763748169\n",
      "epoch: 1 step: 328, loss is 0.21617871522903442\n",
      "epoch: 1 step: 329, loss is 0.21965034306049347\n",
      "epoch: 1 step: 330, loss is 0.34051164984703064\n",
      "epoch: 1 step: 331, loss is 0.1691390573978424\n",
      "epoch: 1 step: 332, loss is 0.2686649262905121\n",
      "epoch: 1 step: 333, loss is 0.2291787713766098\n",
      "epoch: 1 step: 334, loss is 0.33981087803840637\n",
      "epoch: 1 step: 335, loss is 0.22306105494499207\n",
      "epoch: 1 step: 336, loss is 0.22595839202404022\n",
      "epoch: 1 step: 337, loss is 0.2713067829608917\n",
      "epoch: 1 step: 338, loss is 0.40441203117370605\n",
      "epoch: 1 step: 339, loss is 0.313069224357605\n",
      "epoch: 1 step: 340, loss is 0.29856374859809875\n",
      "epoch: 1 step: 341, loss is 0.25695958733558655\n",
      "epoch: 1 step: 342, loss is 0.1719946265220642\n",
      "epoch: 1 step: 343, loss is 0.37344589829444885\n",
      "epoch: 1 step: 344, loss is 0.38232970237731934\n",
      "epoch: 1 step: 345, loss is 0.21324008703231812\n",
      "epoch: 1 step: 346, loss is 0.30195075273513794\n",
      "epoch: 1 step: 347, loss is 0.29625535011291504\n",
      "epoch: 1 step: 348, loss is 0.24970677495002747\n",
      "epoch: 1 step: 349, loss is 0.20647265017032623\n",
      "epoch: 1 step: 350, loss is 0.3299577832221985\n",
      "epoch: 1 step: 351, loss is 0.25234049558639526\n",
      "epoch: 1 step: 352, loss is 0.10724625736474991\n",
      "epoch: 1 step: 353, loss is 0.2631866931915283\n",
      "epoch: 1 step: 354, loss is 0.17291806638240814\n",
      "epoch: 1 step: 355, loss is 0.3466891348361969\n",
      "epoch: 1 step: 356, loss is 0.18331943452358246\n",
      "epoch: 1 step: 357, loss is 0.21361076831817627\n",
      "epoch: 1 step: 358, loss is 0.19305264949798584\n",
      "epoch: 1 step: 359, loss is 0.24292686581611633\n",
      "epoch: 1 step: 360, loss is 0.15588784217834473\n",
      "epoch: 1 step: 361, loss is 0.24381917715072632\n",
      "epoch: 1 step: 362, loss is 0.2554025948047638\n",
      "epoch: 1 step: 363, loss is 0.2860323190689087\n",
      "epoch: 1 step: 364, loss is 0.21388781070709229\n",
      "epoch: 1 step: 365, loss is 0.32090675830841064\n",
      "epoch: 1 step: 366, loss is 0.18337127566337585\n",
      "epoch: 1 step: 367, loss is 0.24681356549263\n",
      "epoch: 1 step: 368, loss is 0.13489575684070587\n",
      "epoch: 1 step: 369, loss is 0.1373111456632614\n",
      "epoch: 1 step: 370, loss is 0.20884796977043152\n",
      "epoch: 1 step: 371, loss is 0.2377336025238037\n",
      "epoch: 1 step: 372, loss is 0.34249526262283325\n",
      "epoch: 1 step: 373, loss is 0.34330907464027405\n",
      "epoch: 1 step: 374, loss is 0.2425730973482132\n",
      "epoch: 1 step: 375, loss is 0.1112527996301651\n",
      "epoch: 1 step: 376, loss is 0.1872349977493286\n",
      "epoch: 1 step: 377, loss is 0.24512463808059692\n",
      "epoch: 1 step: 378, loss is 0.24293570220470428\n",
      "epoch: 1 step: 379, loss is 0.29982078075408936\n",
      "epoch: 1 step: 380, loss is 0.24583251774311066\n",
      "epoch: 1 step: 381, loss is 0.18108323216438293\n",
      "epoch: 1 step: 382, loss is 0.10067444294691086\n",
      "epoch: 1 step: 383, loss is 0.13761915266513824\n",
      "epoch: 1 step: 384, loss is 0.22000908851623535\n",
      "epoch: 1 step: 385, loss is 0.22580568492412567\n",
      "epoch: 1 step: 386, loss is 0.21065863966941833\n",
      "epoch: 1 step: 387, loss is 0.19077199697494507\n",
      "epoch: 1 step: 388, loss is 0.1294940710067749\n",
      "epoch: 1 step: 389, loss is 0.20601999759674072\n",
      "epoch: 1 step: 390, loss is 0.18046261370182037\n",
      "epoch: 1 step: 391, loss is 0.1428697407245636\n",
      "epoch: 1 step: 392, loss is 0.16128407418727875\n",
      "epoch: 1 step: 393, loss is 0.1941705346107483\n",
      "epoch: 1 step: 394, loss is 0.17315903306007385\n",
      "epoch: 1 step: 395, loss is 0.23473206162452698\n",
      "epoch: 1 step: 396, loss is 0.1543426215648651\n",
      "epoch: 1 step: 397, loss is 0.14402641355991364\n",
      "epoch: 1 step: 398, loss is 0.16948270797729492\n",
      "epoch: 1 step: 399, loss is 0.1681361198425293\n",
      "epoch: 1 step: 400, loss is 0.0966225415468216\n",
      "epoch: 1 step: 401, loss is 0.1102437898516655\n",
      "epoch: 1 step: 402, loss is 0.1689453125\n",
      "epoch: 1 step: 403, loss is 0.19615593552589417\n",
      "epoch: 1 step: 404, loss is 0.11364099383354187\n",
      "epoch: 1 step: 405, loss is 0.19019845128059387\n",
      "epoch: 1 step: 406, loss is 0.14499342441558838\n",
      "epoch: 1 step: 407, loss is 0.1314917802810669\n",
      "epoch: 1 step: 408, loss is 0.2608860731124878\n",
      "epoch: 1 step: 409, loss is 0.1643066555261612\n",
      "epoch: 1 step: 410, loss is 0.13992998003959656\n",
      "epoch: 1 step: 411, loss is 0.15061041712760925\n",
      "epoch: 1 step: 412, loss is 0.21491609513759613\n",
      "epoch: 1 step: 413, loss is 0.0748409777879715\n",
      "epoch: 1 step: 414, loss is 0.15074579417705536\n",
      "epoch: 1 step: 415, loss is 0.24124354124069214\n",
      "epoch: 1 step: 416, loss is 0.20813007652759552\n",
      "epoch: 1 step: 417, loss is 0.2644888758659363\n",
      "epoch: 1 step: 418, loss is 0.12145722657442093\n",
      "epoch: 1 step: 419, loss is 0.08082862198352814\n",
      "epoch: 1 step: 420, loss is 0.2586272656917572\n",
      "epoch: 1 step: 421, loss is 0.1598474234342575\n",
      "epoch: 1 step: 422, loss is 0.1604929119348526\n",
      "epoch: 1 step: 423, loss is 0.14399738609790802\n",
      "epoch: 1 step: 424, loss is 0.1884050965309143\n",
      "epoch: 1 step: 425, loss is 0.16243304312229156\n",
      "epoch: 1 step: 426, loss is 0.1330331265926361\n",
      "epoch: 1 step: 427, loss is 0.06345313787460327\n",
      "epoch: 1 step: 428, loss is 0.14978522062301636\n",
      "epoch: 1 step: 429, loss is 0.18333402276039124\n",
      "epoch: 1 step: 430, loss is 0.21506541967391968\n",
      "epoch: 1 step: 431, loss is 0.3480314314365387\n",
      "epoch: 1 step: 432, loss is 0.27739179134368896\n",
      "epoch: 1 step: 433, loss is 0.15258628129959106\n",
      "epoch: 1 step: 434, loss is 0.2761951684951782\n",
      "epoch: 1 step: 435, loss is 0.13641691207885742\n",
      "epoch: 1 step: 436, loss is 0.08662068843841553\n",
      "epoch: 1 step: 437, loss is 0.11861759424209595\n",
      "epoch: 1 step: 438, loss is 0.10128018260002136\n",
      "epoch: 1 step: 439, loss is 0.2597866654396057\n",
      "epoch: 1 step: 440, loss is 0.1740015149116516\n",
      "epoch: 1 step: 441, loss is 0.18633878231048584\n",
      "epoch: 1 step: 442, loss is 0.2587538957595825\n",
      "epoch: 1 step: 443, loss is 0.18822064995765686\n",
      "epoch: 1 step: 444, loss is 0.1311878114938736\n",
      "epoch: 1 step: 445, loss is 0.16431297361850739\n",
      "epoch: 1 step: 446, loss is 0.09889373928308487\n",
      "epoch: 1 step: 447, loss is 0.14483170211315155\n",
      "epoch: 1 step: 448, loss is 0.26887819170951843\n",
      "epoch: 1 step: 449, loss is 0.07971625030040741\n",
      "epoch: 1 step: 450, loss is 0.11952461302280426\n",
      "epoch: 1 step: 451, loss is 0.10023880004882812\n",
      "epoch: 1 step: 452, loss is 0.21120993793010712\n",
      "epoch: 1 step: 453, loss is 0.07665245234966278\n",
      "epoch: 1 step: 454, loss is 0.12610526382923126\n",
      "epoch: 1 step: 455, loss is 0.1091865822672844\n",
      "epoch: 1 step: 456, loss is 0.09685961902141571\n",
      "epoch: 1 step: 457, loss is 0.10895933210849762\n",
      "epoch: 1 step: 458, loss is 0.09685219824314117\n",
      "epoch: 1 step: 459, loss is 0.15750166773796082\n",
      "epoch: 1 step: 460, loss is 0.06357288360595703\n",
      "epoch: 1 step: 461, loss is 0.11710276454687119\n",
      "epoch: 1 step: 462, loss is 0.09217645227909088\n",
      "epoch: 1 step: 463, loss is 0.08592692017555237\n",
      "epoch: 1 step: 464, loss is 0.12853321433067322\n",
      "epoch: 1 step: 465, loss is 0.12008854746818542\n",
      "epoch: 1 step: 466, loss is 0.1439114511013031\n",
      "epoch: 1 step: 467, loss is 0.1096247136592865\n",
      "epoch: 1 step: 468, loss is 0.2048836648464203\n",
      "epoch: 1 step: 469, loss is 0.17382842302322388\n",
      "epoch: 1 step: 470, loss is 0.20720987021923065\n",
      "epoch: 1 step: 471, loss is 0.14707303047180176\n",
      "epoch: 1 step: 472, loss is 0.17788487672805786\n",
      "epoch: 1 step: 473, loss is 0.08206289261579514\n",
      "epoch: 1 step: 474, loss is 0.0884784460067749\n",
      "epoch: 1 step: 475, loss is 0.07835682481527328\n",
      "epoch: 1 step: 476, loss is 0.13322384655475616\n",
      "epoch: 1 step: 477, loss is 0.10454177111387253\n",
      "epoch: 1 step: 478, loss is 0.08972044289112091\n",
      "epoch: 1 step: 479, loss is 0.2398386001586914\n",
      "epoch: 1 step: 480, loss is 0.08693617582321167\n",
      "epoch: 1 step: 481, loss is 0.07043543457984924\n",
      "epoch: 1 step: 482, loss is 0.09436631202697754\n",
      "epoch: 1 step: 483, loss is 0.1688041239976883\n",
      "epoch: 1 step: 484, loss is 0.09287946671247482\n",
      "epoch: 1 step: 485, loss is 0.11431675404310226\n",
      "epoch: 1 step: 486, loss is 0.13724231719970703\n",
      "epoch: 1 step: 487, loss is 0.212296262383461\n",
      "epoch: 1 step: 488, loss is 0.14545992016792297\n",
      "epoch: 1 step: 489, loss is 0.13312892615795135\n",
      "epoch: 1 step: 490, loss is 0.07080155611038208\n",
      "epoch: 1 step: 491, loss is 0.12517628073692322\n",
      "epoch: 1 step: 492, loss is 0.2254897803068161\n",
      "epoch: 1 step: 493, loss is 0.143692284822464\n",
      "epoch: 1 step: 494, loss is 0.05701497942209244\n",
      "epoch: 1 step: 495, loss is 0.10824722051620483\n",
      "epoch: 1 step: 496, loss is 0.1898200809955597\n",
      "epoch: 1 step: 497, loss is 0.1607605516910553\n",
      "epoch: 1 step: 498, loss is 0.09916582703590393\n",
      "epoch: 1 step: 499, loss is 0.18126440048217773\n",
      "epoch: 1 step: 500, loss is 0.10408661514520645\n",
      "epoch: 1 step: 501, loss is 0.04389162361621857\n",
      "epoch: 1 step: 502, loss is 0.1454082429409027\n",
      "epoch: 1 step: 503, loss is 0.051765646785497665\n",
      "epoch: 1 step: 504, loss is 0.19643643498420715\n",
      "epoch: 1 step: 505, loss is 0.07208164036273956\n",
      "epoch: 1 step: 506, loss is 0.0760902613401413\n",
      "epoch: 1 step: 507, loss is 0.08362738788127899\n",
      "epoch: 1 step: 508, loss is 0.1455729901790619\n",
      "epoch: 1 step: 509, loss is 0.10407266765832901\n",
      "epoch: 1 step: 510, loss is 0.10909457504749298\n",
      "epoch: 1 step: 511, loss is 0.13753053545951843\n",
      "epoch: 1 step: 512, loss is 0.13183341920375824\n",
      "epoch: 1 step: 513, loss is 0.13997085392475128\n",
      "epoch: 1 step: 514, loss is 0.2532179355621338\n",
      "epoch: 1 step: 515, loss is 0.06289839744567871\n",
      "epoch: 1 step: 516, loss is 0.11264453828334808\n",
      "epoch: 1 step: 517, loss is 0.040560074150562286\n",
      "epoch: 1 step: 518, loss is 0.055088624358177185\n",
      "epoch: 1 step: 519, loss is 0.08590799570083618\n",
      "epoch: 1 step: 520, loss is 0.1314520388841629\n",
      "epoch: 1 step: 521, loss is 0.18493184447288513\n",
      "epoch: 1 step: 522, loss is 0.13548454642295837\n",
      "epoch: 1 step: 523, loss is 0.10824824869632721\n",
      "epoch: 1 step: 524, loss is 0.06215759739279747\n",
      "epoch: 1 step: 525, loss is 0.10540042817592621\n",
      "epoch: 1 step: 526, loss is 0.11341056227684021\n",
      "epoch: 1 step: 527, loss is 0.05788688361644745\n",
      "epoch: 1 step: 528, loss is 0.1424526572227478\n",
      "epoch: 1 step: 529, loss is 0.09435033053159714\n",
      "epoch: 1 step: 530, loss is 0.056876227259635925\n",
      "epoch: 1 step: 531, loss is 0.06950321048498154\n",
      "epoch: 1 step: 532, loss is 0.07731276005506516\n",
      "epoch: 1 step: 533, loss is 0.11731410026550293\n",
      "epoch: 1 step: 534, loss is 0.19918721914291382\n",
      "epoch: 1 step: 535, loss is 0.12831196188926697\n",
      "epoch: 1 step: 536, loss is 0.08048803359270096\n",
      "epoch: 1 step: 537, loss is 0.05978561192750931\n",
      "epoch: 1 step: 538, loss is 0.12029175460338593\n",
      "epoch: 1 step: 539, loss is 0.11701644957065582\n",
      "epoch: 1 step: 540, loss is 0.060771286487579346\n",
      "epoch: 1 step: 541, loss is 0.04863534867763519\n",
      "epoch: 1 step: 542, loss is 0.12143515795469284\n",
      "epoch: 1 step: 543, loss is 0.07691947370767593\n",
      "epoch: 1 step: 544, loss is 0.09157662838697433\n",
      "epoch: 1 step: 545, loss is 0.03598977252840996\n",
      "epoch: 1 step: 546, loss is 0.06407187879085541\n",
      "epoch: 1 step: 547, loss is 0.05921723321080208\n",
      "epoch: 1 step: 548, loss is 0.0567280538380146\n",
      "epoch: 1 step: 549, loss is 0.03177729994058609\n",
      "epoch: 1 step: 550, loss is 0.0772421658039093\n",
      "epoch: 1 step: 551, loss is 0.05498529598116875\n",
      "epoch: 1 step: 552, loss is 0.13233083486557007\n",
      "epoch: 1 step: 553, loss is 0.05547347292304039\n",
      "epoch: 1 step: 554, loss is 0.04729875177145004\n",
      "epoch: 1 step: 555, loss is 0.04432937502861023\n",
      "epoch: 1 step: 556, loss is 0.05698193609714508\n",
      "epoch: 1 step: 557, loss is 0.22841283679008484\n",
      "epoch: 1 step: 558, loss is 0.060320183634757996\n",
      "epoch: 1 step: 559, loss is 0.040224939584732056\n",
      "epoch: 1 step: 560, loss is 0.07600879669189453\n",
      "epoch: 1 step: 561, loss is 0.061067454516887665\n",
      "epoch: 1 step: 562, loss is 0.03992995619773865\n",
      "epoch: 1 step: 563, loss is 0.05858743563294411\n",
      "epoch: 1 step: 564, loss is 0.09990881383419037\n",
      "epoch: 1 step: 565, loss is 0.1232578456401825\n",
      "epoch: 1 step: 566, loss is 0.15262708067893982\n",
      "epoch: 1 step: 567, loss is 0.07608777284622192\n",
      "epoch: 1 step: 568, loss is 0.04476381093263626\n",
      "epoch: 1 step: 569, loss is 0.14213189482688904\n",
      "epoch: 1 step: 570, loss is 0.042347501963377\n",
      "epoch: 1 step: 571, loss is 0.05248256027698517\n",
      "epoch: 1 step: 572, loss is 0.04393316060304642\n",
      "epoch: 1 step: 573, loss is 0.09991278499364853\n",
      "epoch: 1 step: 574, loss is 0.12229494005441666\n",
      "epoch: 1 step: 575, loss is 0.07022727280855179\n",
      "epoch: 1 step: 576, loss is 0.04107104241847992\n",
      "epoch: 1 step: 577, loss is 0.059569284319877625\n",
      "epoch: 1 step: 578, loss is 0.12365533411502838\n",
      "epoch: 1 step: 579, loss is 0.10413055866956711\n",
      "epoch: 1 step: 580, loss is 0.11564210057258606\n",
      "epoch: 1 step: 581, loss is 0.1276136338710785\n",
      "epoch: 1 step: 582, loss is 0.11546558141708374\n",
      "epoch: 1 step: 583, loss is 0.21706578135490417\n",
      "epoch: 1 step: 584, loss is 0.043492063879966736\n",
      "epoch: 1 step: 585, loss is 0.049839384853839874\n",
      "epoch: 1 step: 586, loss is 0.08132931590080261\n",
      "epoch: 1 step: 587, loss is 0.039494793862104416\n",
      "epoch: 1 step: 588, loss is 0.16726015508174896\n",
      "epoch: 1 step: 589, loss is 0.06448765099048615\n",
      "epoch: 1 step: 590, loss is 0.08199450373649597\n",
      "epoch: 1 step: 591, loss is 0.12491098791360855\n",
      "epoch: 1 step: 592, loss is 0.11484023928642273\n",
      "epoch: 1 step: 593, loss is 0.09126123785972595\n",
      "epoch: 1 step: 594, loss is 0.04232397675514221\n",
      "epoch: 1 step: 595, loss is 0.03925527632236481\n",
      "epoch: 1 step: 596, loss is 0.0656115859746933\n",
      "Train epoch time: 18649.674 ms, per step time: 31.291 ms\n",
      "epoch: 2 step: 1, loss is 0.09966374933719635\n",
      "epoch: 2 step: 2, loss is 0.0480489656329155\n",
      "epoch: 2 step: 3, loss is 0.03903389349579811\n",
      "epoch: 2 step: 4, loss is 0.040166303515434265\n",
      "epoch: 2 step: 5, loss is 0.11207979172468185\n",
      "epoch: 2 step: 6, loss is 0.031205713748931885\n",
      "epoch: 2 step: 7, loss is 0.0628073588013649\n",
      "epoch: 2 step: 8, loss is 0.03919033706188202\n",
      "epoch: 2 step: 9, loss is 0.04426856338977814\n",
      "epoch: 2 step: 10, loss is 0.048946984112262726\n",
      "epoch: 2 step: 11, loss is 0.03536554425954819\n",
      "epoch: 2 step: 12, loss is 0.06214576214551926\n",
      "epoch: 2 step: 13, loss is 0.03699645400047302\n",
      "epoch: 2 step: 14, loss is 0.030760159716010094\n",
      "epoch: 2 step: 15, loss is 0.02877482771873474\n",
      "epoch: 2 step: 16, loss is 0.060893215239048004\n",
      "epoch: 2 step: 17, loss is 0.05521218851208687\n",
      "epoch: 2 step: 18, loss is 0.041756827384233475\n",
      "epoch: 2 step: 19, loss is 0.06751707196235657\n",
      "epoch: 2 step: 20, loss is 0.029425952583551407\n",
      "epoch: 2 step: 21, loss is 0.1287156492471695\n",
      "epoch: 2 step: 22, loss is 0.12531670928001404\n",
      "epoch: 2 step: 23, loss is 0.048759132623672485\n",
      "epoch: 2 step: 24, loss is 0.09628630429506302\n",
      "epoch: 2 step: 25, loss is 0.10280393064022064\n",
      "epoch: 2 step: 26, loss is 0.04906906187534332\n",
      "epoch: 2 step: 27, loss is 0.022719498723745346\n",
      "epoch: 2 step: 28, loss is 0.031269460916519165\n",
      "epoch: 2 step: 29, loss is 0.10851377993822098\n",
      "epoch: 2 step: 30, loss is 0.03601229190826416\n",
      "epoch: 2 step: 31, loss is 0.02352236397564411\n",
      "epoch: 2 step: 32, loss is 0.11134989559650421\n",
      "epoch: 2 step: 33, loss is 0.050538454204797745\n",
      "epoch: 2 step: 34, loss is 0.015733815729618073\n",
      "epoch: 2 step: 35, loss is 0.04810919240117073\n",
      "epoch: 2 step: 36, loss is 0.07523369789123535\n",
      "epoch: 2 step: 37, loss is 0.032795459032058716\n",
      "epoch: 2 step: 38, loss is 0.022136332467198372\n",
      "epoch: 2 step: 39, loss is 0.09183578193187714\n",
      "epoch: 2 step: 40, loss is 0.06061431020498276\n",
      "epoch: 2 step: 41, loss is 0.03214229270815849\n",
      "epoch: 2 step: 42, loss is 0.03276550769805908\n",
      "epoch: 2 step: 43, loss is 0.013262401334941387\n",
      "epoch: 2 step: 44, loss is 0.07811764627695084\n",
      "epoch: 2 step: 45, loss is 0.11891444027423859\n",
      "epoch: 2 step: 46, loss is 0.038325510919094086\n",
      "epoch: 2 step: 47, loss is 0.014381014741957188\n",
      "epoch: 2 step: 48, loss is 0.062486499547958374\n",
      "epoch: 2 step: 49, loss is 0.09723812341690063\n",
      "epoch: 2 step: 50, loss is 0.09180459380149841\n",
      "epoch: 2 step: 51, loss is 0.025942709296941757\n",
      "epoch: 2 step: 52, loss is 0.05360642075538635\n",
      "epoch: 2 step: 53, loss is 0.06783018261194229\n",
      "epoch: 2 step: 54, loss is 0.01974073052406311\n",
      "epoch: 2 step: 55, loss is 0.08788992464542389\n",
      "epoch: 2 step: 56, loss is 0.040817782282829285\n",
      "epoch: 2 step: 57, loss is 0.0933597981929779\n",
      "epoch: 2 step: 58, loss is 0.051447413861751556\n",
      "epoch: 2 step: 59, loss is 0.024714265018701553\n",
      "epoch: 2 step: 60, loss is 0.025569157674908638\n",
      "epoch: 2 step: 61, loss is 0.09728123247623444\n",
      "epoch: 2 step: 62, loss is 0.041339606046676636\n",
      "epoch: 2 step: 63, loss is 0.0377131924033165\n",
      "epoch: 2 step: 64, loss is 0.07439793646335602\n",
      "epoch: 2 step: 65, loss is 0.03850717842578888\n",
      "epoch: 2 step: 66, loss is 0.057390592992305756\n",
      "epoch: 2 step: 67, loss is 0.07581214606761932\n",
      "epoch: 2 step: 68, loss is 0.1017111986875534\n",
      "epoch: 2 step: 69, loss is 0.03143782913684845\n",
      "epoch: 2 step: 70, loss is 0.017658434808254242\n",
      "epoch: 2 step: 71, loss is 0.021031539887189865\n",
      "epoch: 2 step: 72, loss is 0.03814126178622246\n",
      "epoch: 2 step: 73, loss is 0.06159133464097977\n",
      "epoch: 2 step: 74, loss is 0.057384975254535675\n",
      "epoch: 2 step: 75, loss is 0.06811947375535965\n",
      "epoch: 2 step: 76, loss is 0.03926416486501694\n",
      "epoch: 2 step: 77, loss is 0.020892148837447166\n",
      "epoch: 2 step: 78, loss is 0.06918421387672424\n",
      "epoch: 2 step: 79, loss is 0.04979312792420387\n",
      "epoch: 2 step: 80, loss is 0.036973580718040466\n",
      "epoch: 2 step: 81, loss is 0.062205180525779724\n",
      "epoch: 2 step: 82, loss is 0.044731494039297104\n",
      "epoch: 2 step: 83, loss is 0.038381971418857574\n",
      "epoch: 2 step: 84, loss is 0.026862550526857376\n",
      "epoch: 2 step: 85, loss is 0.049072716385126114\n",
      "epoch: 2 step: 86, loss is 0.05255761742591858\n",
      "epoch: 2 step: 87, loss is 0.09629928320646286\n",
      "epoch: 2 step: 88, loss is 0.022173762321472168\n",
      "epoch: 2 step: 89, loss is 0.03270407393574715\n",
      "epoch: 2 step: 90, loss is 0.013812258839607239\n",
      "epoch: 2 step: 91, loss is 0.06583577394485474\n",
      "epoch: 2 step: 92, loss is 0.04266342893242836\n",
      "epoch: 2 step: 93, loss is 0.044929519295692444\n",
      "epoch: 2 step: 94, loss is 0.010642818175256252\n",
      "epoch: 2 step: 95, loss is 0.04711327329277992\n",
      "epoch: 2 step: 96, loss is 0.041107531636953354\n",
      "epoch: 2 step: 97, loss is 0.057107195258140564\n",
      "epoch: 2 step: 98, loss is 0.018905792385339737\n",
      "epoch: 2 step: 99, loss is 0.01618904620409012\n",
      "epoch: 2 step: 100, loss is 0.018821612000465393\n",
      "epoch: 2 step: 101, loss is 0.02385668456554413\n",
      "epoch: 2 step: 102, loss is 0.0107553256675601\n",
      "epoch: 2 step: 103, loss is 0.02709370292723179\n",
      "epoch: 2 step: 104, loss is 0.01736782118678093\n",
      "epoch: 2 step: 105, loss is 0.06914001703262329\n",
      "epoch: 2 step: 106, loss is 0.01998613029718399\n",
      "epoch: 2 step: 107, loss is 0.03203835338354111\n",
      "epoch: 2 step: 108, loss is 0.055057257413864136\n",
      "epoch: 2 step: 109, loss is 0.0357888825237751\n",
      "epoch: 2 step: 110, loss is 0.13775072991847992\n",
      "epoch: 2 step: 111, loss is 0.012969656847417355\n",
      "epoch: 2 step: 112, loss is 0.030621107667684555\n",
      "epoch: 2 step: 113, loss is 0.016493026167154312\n",
      "epoch: 2 step: 114, loss is 0.06588572263717651\n",
      "epoch: 2 step: 115, loss is 0.01835295930504799\n",
      "epoch: 2 step: 116, loss is 0.025497009977698326\n",
      "epoch: 2 step: 117, loss is 0.04800986871123314\n",
      "epoch: 2 step: 118, loss is 0.09264656901359558\n",
      "epoch: 2 step: 119, loss is 0.06831775605678558\n",
      "epoch: 2 step: 120, loss is 0.027957146987318993\n",
      "epoch: 2 step: 121, loss is 0.014031932689249516\n",
      "epoch: 2 step: 122, loss is 0.14446550607681274\n",
      "epoch: 2 step: 123, loss is 0.03507249429821968\n",
      "epoch: 2 step: 124, loss is 0.03987675905227661\n",
      "epoch: 2 step: 125, loss is 0.01535802148282528\n",
      "epoch: 2 step: 126, loss is 0.05421608313918114\n",
      "epoch: 2 step: 127, loss is 0.021057548001408577\n",
      "epoch: 2 step: 128, loss is 0.05054590851068497\n",
      "epoch: 2 step: 129, loss is 0.013708751648664474\n",
      "epoch: 2 step: 130, loss is 0.02967052161693573\n",
      "epoch: 2 step: 131, loss is 0.13735567033290863\n",
      "epoch: 2 step: 132, loss is 0.04357438534498215\n",
      "epoch: 2 step: 133, loss is 0.0609644316136837\n",
      "epoch: 2 step: 134, loss is 0.03572876751422882\n",
      "epoch: 2 step: 135, loss is 0.020701944828033447\n",
      "epoch: 2 step: 136, loss is 0.10490400344133377\n",
      "epoch: 2 step: 137, loss is 0.05007142946124077\n",
      "epoch: 2 step: 138, loss is 0.03167419508099556\n",
      "epoch: 2 step: 139, loss is 0.017476599663496017\n",
      "epoch: 2 step: 140, loss is 0.018996722996234894\n",
      "epoch: 2 step: 141, loss is 0.09610648453235626\n",
      "epoch: 2 step: 142, loss is 0.017111122608184814\n",
      "epoch: 2 step: 143, loss is 0.03482682257890701\n",
      "epoch: 2 step: 144, loss is 0.05779145658016205\n",
      "epoch: 2 step: 145, loss is 0.03172651678323746\n",
      "epoch: 2 step: 146, loss is 0.02922675386071205\n",
      "epoch: 2 step: 147, loss is 0.015570906922221184\n",
      "epoch: 2 step: 148, loss is 0.013763796538114548\n",
      "epoch: 2 step: 149, loss is 0.04069705307483673\n",
      "epoch: 2 step: 150, loss is 0.057568684220314026\n",
      "epoch: 2 step: 151, loss is 0.017565948888659477\n",
      "epoch: 2 step: 152, loss is 0.021490558981895447\n",
      "epoch: 2 step: 153, loss is 0.02513449266552925\n",
      "epoch: 2 step: 154, loss is 0.04323907941579819\n",
      "epoch: 2 step: 155, loss is 0.009309150278568268\n",
      "epoch: 2 step: 156, loss is 0.027723025530576706\n",
      "epoch: 2 step: 157, loss is 0.015357187949120998\n",
      "epoch: 2 step: 158, loss is 0.00706695020198822\n",
      "epoch: 2 step: 159, loss is 0.028278585523366928\n",
      "epoch: 2 step: 160, loss is 0.01447766087949276\n",
      "epoch: 2 step: 161, loss is 0.03651728481054306\n",
      "epoch: 2 step: 162, loss is 0.01715763658285141\n",
      "epoch: 2 step: 163, loss is 0.01801704615354538\n",
      "epoch: 2 step: 164, loss is 0.015694510191679\n",
      "epoch: 2 step: 165, loss is 0.008565567433834076\n",
      "epoch: 2 step: 166, loss is 0.036553218960762024\n",
      "epoch: 2 step: 167, loss is 0.03305847570300102\n",
      "epoch: 2 step: 168, loss is 0.02098778821527958\n",
      "epoch: 2 step: 169, loss is 0.007757200859487057\n",
      "epoch: 2 step: 170, loss is 0.02638046070933342\n",
      "epoch: 2 step: 171, loss is 0.06800173223018646\n",
      "epoch: 2 step: 172, loss is 0.04630570858716965\n",
      "epoch: 2 step: 173, loss is 0.024904713034629822\n",
      "epoch: 2 step: 174, loss is 0.06491342186927795\n",
      "epoch: 2 step: 175, loss is 0.0240916945040226\n",
      "epoch: 2 step: 176, loss is 0.015369448810815811\n",
      "epoch: 2 step: 177, loss is 0.009676048532128334\n",
      "epoch: 2 step: 178, loss is 0.06561268866062164\n",
      "epoch: 2 step: 179, loss is 0.010402239859104156\n",
      "epoch: 2 step: 180, loss is 0.019328933209180832\n",
      "epoch: 2 step: 181, loss is 0.06974560767412186\n",
      "epoch: 2 step: 182, loss is 0.0499708354473114\n",
      "epoch: 2 step: 183, loss is 0.016055580228567123\n",
      "epoch: 2 step: 184, loss is 0.015185168944299221\n",
      "epoch: 2 step: 185, loss is 0.02787002921104431\n",
      "epoch: 2 step: 186, loss is 0.01921037584543228\n",
      "epoch: 2 step: 187, loss is 0.013668069615960121\n",
      "epoch: 2 step: 188, loss is 0.017856154590845108\n",
      "epoch: 2 step: 189, loss is 0.037714749574661255\n",
      "epoch: 2 step: 190, loss is 0.02059105783700943\n",
      "epoch: 2 step: 191, loss is 0.010504585690796375\n",
      "epoch: 2 step: 192, loss is 0.007148331496864557\n",
      "epoch: 2 step: 193, loss is 0.031195800751447678\n",
      "epoch: 2 step: 194, loss is 0.08894079178571701\n",
      "epoch: 2 step: 195, loss is 0.01733590103685856\n",
      "epoch: 2 step: 196, loss is 0.0058259861543774605\n",
      "epoch: 2 step: 197, loss is 0.012610006146132946\n",
      "epoch: 2 step: 198, loss is 0.04693780094385147\n",
      "epoch: 2 step: 199, loss is 0.05948407202959061\n",
      "epoch: 2 step: 200, loss is 0.012813002802431583\n",
      "epoch: 2 step: 201, loss is 0.012280109338462353\n",
      "epoch: 2 step: 202, loss is 0.032239824533462524\n",
      "epoch: 2 step: 203, loss is 0.012646290473639965\n",
      "epoch: 2 step: 204, loss is 0.020283004269003868\n",
      "epoch: 2 step: 205, loss is 0.009690821170806885\n",
      "epoch: 2 step: 206, loss is 0.01481323316693306\n",
      "epoch: 2 step: 207, loss is 0.014808209612965584\n",
      "epoch: 2 step: 208, loss is 0.009546336717903614\n",
      "epoch: 2 step: 209, loss is 0.023132510483264923\n",
      "epoch: 2 step: 210, loss is 0.0991840586066246\n",
      "epoch: 2 step: 211, loss is 0.008255976252257824\n",
      "epoch: 2 step: 212, loss is 0.016495153307914734\n",
      "epoch: 2 step: 213, loss is 0.02068544551730156\n",
      "epoch: 2 step: 214, loss is 0.02387085370719433\n",
      "epoch: 2 step: 215, loss is 0.03220156952738762\n",
      "epoch: 2 step: 216, loss is 0.05242113023996353\n",
      "epoch: 2 step: 217, loss is 0.02527959644794464\n",
      "epoch: 2 step: 218, loss is 0.010446451604366302\n",
      "epoch: 2 step: 219, loss is 0.00882716104388237\n",
      "epoch: 2 step: 220, loss is 0.012111708521842957\n",
      "epoch: 2 step: 221, loss is 0.03534915670752525\n",
      "epoch: 2 step: 222, loss is 0.021895568817853928\n",
      "epoch: 2 step: 223, loss is 0.027663057669997215\n",
      "epoch: 2 step: 224, loss is 0.014062338508665562\n",
      "epoch: 2 step: 225, loss is 0.015304411761462688\n",
      "epoch: 2 step: 226, loss is 0.004370692186057568\n",
      "epoch: 2 step: 227, loss is 0.017008915543556213\n",
      "epoch: 2 step: 228, loss is 0.021243713796138763\n",
      "epoch: 2 step: 229, loss is 0.04952653869986534\n",
      "epoch: 2 step: 230, loss is 0.017577970400452614\n",
      "epoch: 2 step: 231, loss is 0.01260373555123806\n",
      "epoch: 2 step: 232, loss is 0.008253419771790504\n",
      "epoch: 2 step: 233, loss is 0.013205739669501781\n",
      "epoch: 2 step: 234, loss is 0.008406873792409897\n",
      "epoch: 2 step: 235, loss is 0.04736977815628052\n",
      "epoch: 2 step: 236, loss is 0.042560283094644547\n",
      "epoch: 2 step: 237, loss is 0.020788373425602913\n",
      "epoch: 2 step: 238, loss is 0.012913490645587444\n",
      "epoch: 2 step: 239, loss is 0.023280911147594452\n",
      "epoch: 2 step: 240, loss is 0.05033909156918526\n",
      "epoch: 2 step: 241, loss is 0.007756340317428112\n",
      "epoch: 2 step: 242, loss is 0.012619434855878353\n",
      "epoch: 2 step: 243, loss is 0.006038616877049208\n",
      "epoch: 2 step: 244, loss is 0.010728600434958935\n",
      "epoch: 2 step: 245, loss is 0.10002291202545166\n",
      "epoch: 2 step: 246, loss is 0.01842125691473484\n",
      "epoch: 2 step: 247, loss is 0.00605252617970109\n",
      "epoch: 2 step: 248, loss is 0.017681263387203217\n",
      "epoch: 2 step: 249, loss is 0.020435964688658714\n",
      "epoch: 2 step: 250, loss is 0.02512887492775917\n",
      "epoch: 2 step: 251, loss is 0.009198930114507675\n",
      "epoch: 2 step: 252, loss is 0.024997394531965256\n",
      "epoch: 2 step: 253, loss is 0.007460772525519133\n",
      "epoch: 2 step: 254, loss is 0.06327834725379944\n",
      "epoch: 2 step: 255, loss is 0.022754166275262833\n",
      "epoch: 2 step: 256, loss is 0.021389469504356384\n",
      "epoch: 2 step: 257, loss is 0.04006010293960571\n",
      "epoch: 2 step: 258, loss is 0.009318096563220024\n",
      "epoch: 2 step: 259, loss is 0.11205671727657318\n",
      "epoch: 2 step: 260, loss is 0.008282848633825779\n",
      "epoch: 2 step: 261, loss is 0.02813824452459812\n",
      "epoch: 2 step: 262, loss is 0.011229143477976322\n",
      "epoch: 2 step: 263, loss is 0.030828014016151428\n",
      "epoch: 2 step: 264, loss is 0.006773349829018116\n",
      "epoch: 2 step: 265, loss is 0.00782748218625784\n",
      "epoch: 2 step: 266, loss is 0.03653758391737938\n",
      "epoch: 2 step: 267, loss is 0.04295075684785843\n",
      "epoch: 2 step: 268, loss is 0.015583686530590057\n",
      "epoch: 2 step: 269, loss is 0.015317011624574661\n",
      "epoch: 2 step: 270, loss is 0.01360325887799263\n",
      "epoch: 2 step: 271, loss is 0.07982528209686279\n",
      "epoch: 2 step: 272, loss is 0.008228263817727566\n",
      "epoch: 2 step: 273, loss is 0.01446204911917448\n",
      "epoch: 2 step: 274, loss is 0.014031403698027134\n",
      "epoch: 2 step: 275, loss is 0.036654964089393616\n",
      "epoch: 2 step: 276, loss is 0.011826632544398308\n",
      "epoch: 2 step: 277, loss is 0.00931151956319809\n",
      "epoch: 2 step: 278, loss is 0.019514882937073708\n",
      "epoch: 2 step: 279, loss is 0.00944688729941845\n",
      "epoch: 2 step: 280, loss is 0.08130806684494019\n",
      "epoch: 2 step: 281, loss is 0.01233527809381485\n",
      "epoch: 2 step: 282, loss is 0.011657469905912876\n",
      "epoch: 2 step: 283, loss is 0.03796558082103729\n",
      "epoch: 2 step: 284, loss is 0.010394280776381493\n",
      "epoch: 2 step: 285, loss is 0.06474112719297409\n",
      "epoch: 2 step: 286, loss is 0.024735508486628532\n",
      "epoch: 2 step: 287, loss is 0.011437835171818733\n",
      "epoch: 2 step: 288, loss is 0.012468455359339714\n",
      "epoch: 2 step: 289, loss is 0.005639992188662291\n",
      "epoch: 2 step: 290, loss is 0.06259861588478088\n",
      "epoch: 2 step: 291, loss is 0.02004466950893402\n",
      "epoch: 2 step: 292, loss is 0.017905158922076225\n",
      "epoch: 2 step: 293, loss is 0.03740653023123741\n",
      "epoch: 2 step: 294, loss is 0.02757946588099003\n",
      "epoch: 2 step: 295, loss is 0.01616983488202095\n",
      "epoch: 2 step: 296, loss is 0.00996658205986023\n",
      "epoch: 2 step: 297, loss is 0.011163095012307167\n",
      "epoch: 2 step: 298, loss is 0.007764110341668129\n",
      "epoch: 2 step: 299, loss is 0.020153667777776718\n",
      "epoch: 2 step: 300, loss is 0.033268511295318604\n",
      "epoch: 2 step: 301, loss is 0.007020785007625818\n",
      "epoch: 2 step: 302, loss is 0.05126070976257324\n",
      "epoch: 2 step: 303, loss is 0.013779368251562119\n",
      "epoch: 2 step: 304, loss is 0.003186523914337158\n",
      "epoch: 2 step: 305, loss is 0.02053195610642433\n",
      "epoch: 2 step: 306, loss is 0.011727129109203815\n",
      "epoch: 2 step: 307, loss is 0.003114627208560705\n",
      "epoch: 2 step: 308, loss is 0.019324202090501785\n",
      "epoch: 2 step: 309, loss is 0.006426723673939705\n",
      "epoch: 2 step: 310, loss is 0.018572621047496796\n",
      "epoch: 2 step: 311, loss is 0.012132702395319939\n",
      "epoch: 2 step: 312, loss is 0.019491706043481827\n",
      "epoch: 2 step: 313, loss is 0.006711293011903763\n",
      "epoch: 2 step: 314, loss is 0.009476035833358765\n",
      "epoch: 2 step: 315, loss is 0.011197170242667198\n",
      "epoch: 2 step: 316, loss is 0.015169834718108177\n",
      "epoch: 2 step: 317, loss is 0.01017923653125763\n",
      "epoch: 2 step: 318, loss is 0.004626254551112652\n",
      "epoch: 2 step: 319, loss is 0.005178047809749842\n",
      "epoch: 2 step: 320, loss is 0.04358559846878052\n",
      "epoch: 2 step: 321, loss is 0.03630712628364563\n",
      "epoch: 2 step: 322, loss is 0.006165788508951664\n",
      "epoch: 2 step: 323, loss is 0.050999511033296585\n",
      "epoch: 2 step: 324, loss is 0.014873119071125984\n",
      "epoch: 2 step: 325, loss is 0.0115909269079566\n",
      "epoch: 2 step: 326, loss is 0.004915248136967421\n",
      "epoch: 2 step: 327, loss is 0.0049324920400977135\n",
      "epoch: 2 step: 328, loss is 0.011377495713531971\n",
      "epoch: 2 step: 329, loss is 0.014435931108891964\n",
      "epoch: 2 step: 330, loss is 0.0312507189810276\n",
      "epoch: 2 step: 331, loss is 0.011366909369826317\n",
      "epoch: 2 step: 332, loss is 0.0193946436047554\n",
      "epoch: 2 step: 333, loss is 0.036970458924770355\n",
      "epoch: 2 step: 334, loss is 0.026179755106568336\n",
      "epoch: 2 step: 335, loss is 0.010406230576336384\n",
      "epoch: 2 step: 336, loss is 0.007423168048262596\n",
      "epoch: 2 step: 337, loss is 0.009557372890412807\n",
      "epoch: 2 step: 338, loss is 0.014326049014925957\n",
      "epoch: 2 step: 339, loss is 0.003948355093598366\n",
      "epoch: 2 step: 340, loss is 0.014219324104487896\n",
      "epoch: 2 step: 341, loss is 0.005913755390793085\n",
      "epoch: 2 step: 342, loss is 0.013969888910651207\n",
      "epoch: 2 step: 343, loss is 0.01790078729391098\n",
      "epoch: 2 step: 344, loss is 0.009650096297264099\n",
      "epoch: 2 step: 345, loss is 0.0033534662798047066\n",
      "epoch: 2 step: 346, loss is 0.010292314924299717\n",
      "epoch: 2 step: 347, loss is 0.023365184664726257\n",
      "epoch: 2 step: 348, loss is 0.012932751327753067\n",
      "epoch: 2 step: 349, loss is 0.009376445785164833\n",
      "epoch: 2 step: 350, loss is 0.007366378325968981\n",
      "epoch: 2 step: 351, loss is 0.018282931298017502\n",
      "epoch: 2 step: 352, loss is 0.008503532037138939\n",
      "epoch: 2 step: 353, loss is 0.010385285131633282\n",
      "epoch: 2 step: 354, loss is 0.0026579806581139565\n",
      "epoch: 2 step: 355, loss is 0.012785954400897026\n",
      "epoch: 2 step: 356, loss is 0.009520705789327621\n",
      "epoch: 2 step: 357, loss is 0.004206350073218346\n",
      "epoch: 2 step: 358, loss is 0.012442190200090408\n",
      "epoch: 2 step: 359, loss is 0.04065423458814621\n",
      "epoch: 2 step: 360, loss is 0.010577580891549587\n",
      "epoch: 2 step: 361, loss is 0.0290885791182518\n",
      "epoch: 2 step: 362, loss is 0.016470562666654587\n",
      "epoch: 2 step: 363, loss is 0.008210078813135624\n",
      "epoch: 2 step: 364, loss is 0.06217829883098602\n",
      "epoch: 2 step: 365, loss is 0.0353100448846817\n",
      "epoch: 2 step: 366, loss is 0.002253766404464841\n",
      "epoch: 2 step: 367, loss is 0.008320010267198086\n",
      "epoch: 2 step: 368, loss is 0.0043203989043831825\n",
      "epoch: 2 step: 369, loss is 0.01093196775764227\n",
      "epoch: 2 step: 370, loss is 0.008612468838691711\n",
      "epoch: 2 step: 371, loss is 0.012638073414564133\n",
      "epoch: 2 step: 372, loss is 0.004925470799207687\n",
      "epoch: 2 step: 373, loss is 0.011315550655126572\n",
      "epoch: 2 step: 374, loss is 0.01040108036249876\n",
      "epoch: 2 step: 375, loss is 0.0025343457236886024\n",
      "epoch: 2 step: 376, loss is 0.01016728114336729\n",
      "epoch: 2 step: 377, loss is 0.005460850428789854\n",
      "epoch: 2 step: 378, loss is 0.01958521082997322\n",
      "epoch: 2 step: 379, loss is 0.005029948893934488\n",
      "epoch: 2 step: 380, loss is 0.014491712674498558\n",
      "epoch: 2 step: 381, loss is 0.006933784112334251\n",
      "epoch: 2 step: 382, loss is 0.011916483752429485\n",
      "epoch: 2 step: 383, loss is 0.006273541133850813\n",
      "epoch: 2 step: 384, loss is 0.00820840522646904\n",
      "epoch: 2 step: 385, loss is 0.0077690682373940945\n",
      "epoch: 2 step: 386, loss is 0.004360692575573921\n",
      "epoch: 2 step: 387, loss is 0.014207789674401283\n",
      "epoch: 2 step: 388, loss is 0.0060961246490478516\n",
      "epoch: 2 step: 389, loss is 0.028736582025885582\n",
      "epoch: 2 step: 390, loss is 0.015180306509137154\n",
      "epoch: 2 step: 391, loss is 0.0029286504723131657\n",
      "epoch: 2 step: 392, loss is 0.00464156549423933\n",
      "epoch: 2 step: 393, loss is 0.017350152134895325\n",
      "epoch: 2 step: 394, loss is 0.011961884796619415\n",
      "epoch: 2 step: 395, loss is 0.01596415974199772\n",
      "epoch: 2 step: 396, loss is 0.013292308896780014\n",
      "epoch: 2 step: 397, loss is 0.004282210487872362\n",
      "epoch: 2 step: 398, loss is 0.010161999613046646\n",
      "epoch: 2 step: 399, loss is 0.008100030943751335\n",
      "epoch: 2 step: 400, loss is 0.0036461814306676388\n",
      "epoch: 2 step: 401, loss is 0.016348231583833694\n",
      "epoch: 2 step: 402, loss is 0.0029106088913977146\n",
      "epoch: 2 step: 403, loss is 0.05968226119875908\n",
      "epoch: 2 step: 404, loss is 0.004957117140293121\n",
      "epoch: 2 step: 405, loss is 0.010423403233289719\n",
      "epoch: 2 step: 406, loss is 0.007396114058792591\n",
      "epoch: 2 step: 407, loss is 0.011664649471640587\n",
      "epoch: 2 step: 408, loss is 0.08078843355178833\n",
      "epoch: 2 step: 409, loss is 0.0038312850520014763\n",
      "epoch: 2 step: 410, loss is 0.005708528682589531\n",
      "epoch: 2 step: 411, loss is 0.002795131877064705\n",
      "epoch: 2 step: 412, loss is 0.022908879444003105\n",
      "epoch: 2 step: 413, loss is 0.004835139028728008\n",
      "epoch: 2 step: 414, loss is 0.008072849363088608\n",
      "epoch: 2 step: 415, loss is 0.014621151611208916\n",
      "epoch: 2 step: 416, loss is 0.01477004773914814\n",
      "epoch: 2 step: 417, loss is 0.01538938656449318\n",
      "epoch: 2 step: 418, loss is 0.015939444303512573\n",
      "epoch: 2 step: 419, loss is 0.004882296547293663\n",
      "epoch: 2 step: 420, loss is 0.06177138909697533\n",
      "epoch: 2 step: 421, loss is 0.007524013519287109\n",
      "epoch: 2 step: 422, loss is 0.002212623367086053\n",
      "epoch: 2 step: 423, loss is 0.003888036124408245\n",
      "epoch: 2 step: 424, loss is 0.009816965088248253\n",
      "epoch: 2 step: 425, loss is 0.011105302721261978\n",
      "epoch: 2 step: 426, loss is 0.005055115558207035\n",
      "epoch: 2 step: 427, loss is 0.012955009005963802\n",
      "epoch: 2 step: 428, loss is 0.016999222338199615\n",
      "epoch: 2 step: 429, loss is 0.09542390704154968\n",
      "epoch: 2 step: 430, loss is 0.013478042557835579\n",
      "epoch: 2 step: 431, loss is 0.005607233848422766\n",
      "epoch: 2 step: 432, loss is 0.017497625201940536\n",
      "epoch: 2 step: 433, loss is 0.004889230709522963\n",
      "epoch: 2 step: 434, loss is 0.007435316685587168\n",
      "epoch: 2 step: 435, loss is 0.004140073899179697\n",
      "epoch: 2 step: 436, loss is 0.007782462518662214\n",
      "epoch: 2 step: 437, loss is 0.007476072758436203\n",
      "epoch: 2 step: 438, loss is 0.0030919723212718964\n",
      "epoch: 2 step: 439, loss is 0.021868593990802765\n",
      "epoch: 2 step: 440, loss is 0.015826119109988213\n",
      "epoch: 2 step: 441, loss is 0.003917543683201075\n",
      "epoch: 2 step: 442, loss is 0.0022040032781660557\n",
      "epoch: 2 step: 443, loss is 0.019500384107232094\n",
      "epoch: 2 step: 444, loss is 0.0088669927790761\n",
      "epoch: 2 step: 445, loss is 0.010949607007205486\n",
      "epoch: 2 step: 446, loss is 0.005729639902710915\n",
      "epoch: 2 step: 447, loss is 0.003683899063616991\n",
      "epoch: 2 step: 448, loss is 0.021721411496400833\n",
      "epoch: 2 step: 449, loss is 0.003608396975323558\n",
      "epoch: 2 step: 450, loss is 0.0030702860094606876\n",
      "epoch: 2 step: 451, loss is 0.010441970080137253\n",
      "epoch: 2 step: 452, loss is 0.010289913043379784\n",
      "epoch: 2 step: 453, loss is 0.0013255038065835834\n",
      "epoch: 2 step: 454, loss is 0.022635484114289284\n",
      "epoch: 2 step: 455, loss is 0.00857602246105671\n",
      "epoch: 2 step: 456, loss is 0.004906311631202698\n",
      "epoch: 2 step: 457, loss is 0.006216476671397686\n",
      "epoch: 2 step: 458, loss is 0.022112060338258743\n",
      "epoch: 2 step: 459, loss is 0.024662770330905914\n",
      "epoch: 2 step: 460, loss is 0.004688287619501352\n",
      "epoch: 2 step: 461, loss is 0.008730577304959297\n",
      "epoch: 2 step: 462, loss is 0.011255314573645592\n",
      "epoch: 2 step: 463, loss is 0.0104109151288867\n",
      "epoch: 2 step: 464, loss is 0.00503621157258749\n",
      "epoch: 2 step: 465, loss is 0.00892210379242897\n",
      "epoch: 2 step: 466, loss is 0.008630011230707169\n",
      "epoch: 2 step: 467, loss is 0.002243177965283394\n",
      "epoch: 2 step: 468, loss is 0.011163346469402313\n",
      "epoch: 2 step: 469, loss is 0.0033009611070156097\n",
      "epoch: 2 step: 470, loss is 0.011168273165822029\n",
      "epoch: 2 step: 471, loss is 0.004967540502548218\n",
      "epoch: 2 step: 472, loss is 0.005619802512228489\n",
      "epoch: 2 step: 473, loss is 0.018451955169439316\n",
      "epoch: 2 step: 474, loss is 0.004139236640185118\n",
      "epoch: 2 step: 475, loss is 0.013259959407150745\n",
      "epoch: 2 step: 476, loss is 0.004540671594440937\n",
      "epoch: 2 step: 477, loss is 0.0032379310578107834\n",
      "epoch: 2 step: 478, loss is 0.008830338716506958\n",
      "epoch: 2 step: 479, loss is 0.009793815203011036\n",
      "epoch: 2 step: 480, loss is 0.008727740496397018\n",
      "epoch: 2 step: 481, loss is 0.0017551917117089033\n",
      "epoch: 2 step: 482, loss is 0.0017775677843019366\n",
      "epoch: 2 step: 483, loss is 0.005022995639592409\n",
      "epoch: 2 step: 484, loss is 0.0038619893603026867\n",
      "epoch: 2 step: 485, loss is 0.010059358552098274\n",
      "epoch: 2 step: 486, loss is 0.002557348692789674\n",
      "epoch: 2 step: 487, loss is 0.0043310802429914474\n",
      "epoch: 2 step: 488, loss is 0.0032227099873125553\n",
      "epoch: 2 step: 489, loss is 0.0026940081734210253\n",
      "epoch: 2 step: 490, loss is 0.006655039731413126\n",
      "epoch: 2 step: 491, loss is 0.007058525457978249\n",
      "epoch: 2 step: 492, loss is 0.007815388962626457\n",
      "epoch: 2 step: 493, loss is 0.0099380724132061\n",
      "epoch: 2 step: 494, loss is 0.0017419976647943258\n",
      "epoch: 2 step: 495, loss is 0.0036683594807982445\n",
      "epoch: 2 step: 496, loss is 0.009280211292207241\n",
      "epoch: 2 step: 497, loss is 0.010454842820763588\n",
      "epoch: 2 step: 498, loss is 0.0023162774741649628\n",
      "epoch: 2 step: 499, loss is 0.0035679771099239588\n",
      "epoch: 2 step: 500, loss is 0.007717531640082598\n",
      "epoch: 2 step: 501, loss is 0.003266782034188509\n",
      "epoch: 2 step: 502, loss is 0.0065470170229673386\n",
      "epoch: 2 step: 503, loss is 0.004311426542699337\n",
      "epoch: 2 step: 504, loss is 0.01125000137835741\n",
      "epoch: 2 step: 505, loss is 0.0031116839963942766\n",
      "epoch: 2 step: 506, loss is 0.005405547097325325\n",
      "epoch: 2 step: 507, loss is 0.002225489355623722\n",
      "epoch: 2 step: 508, loss is 0.007797651924192905\n",
      "epoch: 2 step: 509, loss is 0.0027307511772960424\n",
      "epoch: 2 step: 510, loss is 0.012475918047130108\n",
      "epoch: 2 step: 511, loss is 0.013261128216981888\n",
      "epoch: 2 step: 512, loss is 0.005118925590068102\n",
      "epoch: 2 step: 513, loss is 0.0035733424592763186\n",
      "epoch: 2 step: 514, loss is 0.010514840483665466\n",
      "epoch: 2 step: 515, loss is 0.0019273656653240323\n",
      "epoch: 2 step: 516, loss is 0.022725576534867287\n",
      "epoch: 2 step: 517, loss is 0.0016133434837684035\n",
      "epoch: 2 step: 518, loss is 0.003208016976714134\n",
      "epoch: 2 step: 519, loss is 0.02317812107503414\n",
      "epoch: 2 step: 520, loss is 0.0025271475315093994\n",
      "epoch: 2 step: 521, loss is 0.007007589563727379\n",
      "epoch: 2 step: 522, loss is 0.002864296082407236\n",
      "epoch: 2 step: 523, loss is 0.019312549382448196\n",
      "epoch: 2 step: 524, loss is 0.006419326178729534\n",
      "epoch: 2 step: 525, loss is 0.007135129999369383\n",
      "epoch: 2 step: 526, loss is 0.0037157409824430943\n",
      "epoch: 2 step: 527, loss is 0.0059067546389997005\n",
      "epoch: 2 step: 528, loss is 0.0049636485055089\n",
      "epoch: 2 step: 529, loss is 0.0014034598134458065\n",
      "epoch: 2 step: 530, loss is 0.0029632365331053734\n",
      "epoch: 2 step: 531, loss is 0.006547314114868641\n",
      "epoch: 2 step: 532, loss is 0.009066635742783546\n",
      "epoch: 2 step: 533, loss is 0.01056609209626913\n",
      "epoch: 2 step: 534, loss is 0.006035905331373215\n",
      "epoch: 2 step: 535, loss is 0.003800659906119108\n",
      "epoch: 2 step: 536, loss is 0.008997701108455658\n",
      "epoch: 2 step: 537, loss is 0.0033841514959931374\n",
      "epoch: 2 step: 538, loss is 0.024329161271452904\n",
      "epoch: 2 step: 539, loss is 0.007083523087203503\n",
      "epoch: 2 step: 540, loss is 0.0031491569243371487\n",
      "epoch: 2 step: 541, loss is 0.004505363292992115\n",
      "epoch: 2 step: 542, loss is 0.009683113545179367\n",
      "epoch: 2 step: 543, loss is 0.005361836403608322\n",
      "epoch: 2 step: 544, loss is 0.01068423967808485\n",
      "epoch: 2 step: 545, loss is 0.004748806357383728\n",
      "epoch: 2 step: 546, loss is 0.005888719577342272\n",
      "epoch: 2 step: 547, loss is 0.003333645174279809\n",
      "epoch: 2 step: 548, loss is 0.010071061551570892\n",
      "epoch: 2 step: 549, loss is 0.0017498787492513657\n",
      "epoch: 2 step: 550, loss is 0.015518861822783947\n",
      "epoch: 2 step: 551, loss is 0.002269195858389139\n",
      "epoch: 2 step: 552, loss is 0.020936978980898857\n",
      "epoch: 2 step: 553, loss is 0.003706251736730337\n",
      "epoch: 2 step: 554, loss is 0.0031507008243352175\n",
      "epoch: 2 step: 555, loss is 0.0031508817337453365\n",
      "epoch: 2 step: 556, loss is 0.014856470748782158\n",
      "epoch: 2 step: 557, loss is 0.05640539154410362\n",
      "epoch: 2 step: 558, loss is 0.001228791894391179\n",
      "epoch: 2 step: 559, loss is 0.00567869795486331\n",
      "epoch: 2 step: 560, loss is 0.0017908952431753278\n",
      "epoch: 2 step: 561, loss is 0.024819785729050636\n",
      "epoch: 2 step: 562, loss is 0.004575165454298258\n",
      "epoch: 2 step: 563, loss is 0.003722688416019082\n",
      "epoch: 2 step: 564, loss is 0.008590598590672016\n",
      "epoch: 2 step: 565, loss is 0.003935986198484898\n",
      "epoch: 2 step: 566, loss is 0.0069545553997159\n",
      "epoch: 2 step: 567, loss is 0.012726400047540665\n",
      "epoch: 2 step: 568, loss is 0.001307957572862506\n",
      "epoch: 2 step: 569, loss is 0.03497622534632683\n",
      "epoch: 2 step: 570, loss is 0.0027566400822252035\n",
      "epoch: 2 step: 571, loss is 0.0038538435474038124\n",
      "epoch: 2 step: 572, loss is 0.0017941996920853853\n",
      "epoch: 2 step: 573, loss is 0.004986775107681751\n",
      "epoch: 2 step: 574, loss is 0.008139519020915031\n",
      "epoch: 2 step: 575, loss is 0.006594621576368809\n",
      "epoch: 2 step: 576, loss is 0.0069058439694345\n",
      "epoch: 2 step: 577, loss is 0.003124006325379014\n",
      "epoch: 2 step: 578, loss is 0.055515844374895096\n",
      "epoch: 2 step: 579, loss is 0.004048019181936979\n",
      "epoch: 2 step: 580, loss is 0.006637101992964745\n",
      "epoch: 2 step: 581, loss is 0.016035156324505806\n",
      "epoch: 2 step: 582, loss is 0.0037504336796700954\n",
      "epoch: 2 step: 583, loss is 0.013476375490427017\n",
      "epoch: 2 step: 584, loss is 0.0015074452385306358\n",
      "epoch: 2 step: 585, loss is 0.009733114391565323\n",
      "epoch: 2 step: 586, loss is 0.0016388188814744353\n",
      "epoch: 2 step: 587, loss is 0.003809937508776784\n",
      "epoch: 2 step: 588, loss is 0.00452122837305069\n",
      "epoch: 2 step: 589, loss is 0.006867053918540478\n",
      "epoch: 2 step: 590, loss is 0.001961976755410433\n",
      "epoch: 2 step: 591, loss is 0.0025902274064719677\n",
      "epoch: 2 step: 592, loss is 0.00814517866820097\n",
      "epoch: 2 step: 593, loss is 0.005542794242501259\n",
      "epoch: 2 step: 594, loss is 0.002792631508782506\n",
      "epoch: 2 step: 595, loss is 0.0018069259822368622\n",
      "epoch: 2 step: 596, loss is 0.004394421353936195\n",
      "Train epoch time: 16676.608 ms, per step time: 27.981 ms\n",
      "epoch: 3 step: 1, loss is 0.003280180972069502\n",
      "epoch: 3 step: 2, loss is 0.0031473906710743904\n",
      "epoch: 3 step: 3, loss is 0.004371312912553549\n",
      "epoch: 3 step: 4, loss is 0.01402629166841507\n",
      "epoch: 3 step: 5, loss is 0.004578670021146536\n",
      "epoch: 3 step: 6, loss is 0.0018606837838888168\n",
      "epoch: 3 step: 7, loss is 0.009676148183643818\n",
      "epoch: 3 step: 8, loss is 0.0022198259830474854\n",
      "epoch: 3 step: 9, loss is 0.0019829161465168\n",
      "epoch: 3 step: 10, loss is 0.004466813523322344\n",
      "epoch: 3 step: 11, loss is 0.00264070276170969\n",
      "epoch: 3 step: 12, loss is 0.004737933166325092\n",
      "epoch: 3 step: 13, loss is 0.001254511415027082\n",
      "epoch: 3 step: 14, loss is 0.0017512149643152952\n",
      "epoch: 3 step: 15, loss is 0.0018455811077728868\n",
      "epoch: 3 step: 16, loss is 0.0023183473385870457\n",
      "epoch: 3 step: 17, loss is 0.018959050998091698\n",
      "epoch: 3 step: 18, loss is 0.007973738014698029\n",
      "epoch: 3 step: 19, loss is 0.004977853037416935\n",
      "epoch: 3 step: 20, loss is 0.0008396966150030494\n",
      "epoch: 3 step: 21, loss is 0.001480686361901462\n",
      "epoch: 3 step: 22, loss is 0.0030664249788969755\n",
      "epoch: 3 step: 23, loss is 0.009751801379024982\n",
      "epoch: 3 step: 24, loss is 0.009803074412047863\n",
      "epoch: 3 step: 25, loss is 0.01892205886542797\n",
      "epoch: 3 step: 26, loss is 0.00814756564795971\n",
      "epoch: 3 step: 27, loss is 0.0027556216809898615\n",
      "epoch: 3 step: 28, loss is 0.0010648851748555899\n",
      "epoch: 3 step: 29, loss is 0.0030687786638736725\n",
      "epoch: 3 step: 30, loss is 0.001874624751508236\n",
      "epoch: 3 step: 31, loss is 0.004037096165120602\n",
      "epoch: 3 step: 32, loss is 0.01578429900109768\n",
      "epoch: 3 step: 33, loss is 0.005657105706632137\n",
      "epoch: 3 step: 34, loss is 0.00843789242208004\n",
      "epoch: 3 step: 35, loss is 0.003253188217058778\n",
      "epoch: 3 step: 36, loss is 0.004408902954310179\n",
      "epoch: 3 step: 37, loss is 0.0020480165258049965\n",
      "epoch: 3 step: 38, loss is 0.001470876159146428\n",
      "epoch: 3 step: 39, loss is 0.0021809714380651712\n",
      "epoch: 3 step: 40, loss is 0.004032674711197615\n",
      "epoch: 3 step: 41, loss is 0.0016020495677366853\n",
      "epoch: 3 step: 42, loss is 0.0008142133010551333\n",
      "epoch: 3 step: 43, loss is 0.002950012683868408\n",
      "epoch: 3 step: 44, loss is 0.003972524777054787\n",
      "epoch: 3 step: 45, loss is 0.004647535737603903\n",
      "epoch: 3 step: 46, loss is 0.003734804457053542\n",
      "epoch: 3 step: 47, loss is 0.0018365188734605908\n",
      "epoch: 3 step: 48, loss is 0.00876331515610218\n",
      "epoch: 3 step: 49, loss is 0.005158798769116402\n",
      "epoch: 3 step: 50, loss is 0.005252162925899029\n",
      "epoch: 3 step: 51, loss is 0.0012571604456752539\n",
      "epoch: 3 step: 52, loss is 0.0012834256049245596\n",
      "epoch: 3 step: 53, loss is 0.0041269040666520596\n",
      "epoch: 3 step: 54, loss is 0.0012161434860900044\n",
      "epoch: 3 step: 55, loss is 0.0032304839696735144\n",
      "epoch: 3 step: 56, loss is 0.0008764707017689943\n",
      "epoch: 3 step: 57, loss is 0.0040312763303518295\n",
      "epoch: 3 step: 58, loss is 0.0007111348095349967\n",
      "epoch: 3 step: 59, loss is 0.004225987941026688\n",
      "epoch: 3 step: 60, loss is 0.002597492653876543\n",
      "epoch: 3 step: 61, loss is 0.009040843695402145\n",
      "epoch: 3 step: 62, loss is 0.002603741828352213\n",
      "epoch: 3 step: 63, loss is 0.014518829062581062\n",
      "epoch: 3 step: 64, loss is 0.006529686972498894\n",
      "epoch: 3 step: 65, loss is 0.010184071958065033\n",
      "epoch: 3 step: 66, loss is 0.011755529791116714\n",
      "epoch: 3 step: 67, loss is 0.014730924740433693\n",
      "epoch: 3 step: 68, loss is 0.00315765175037086\n",
      "epoch: 3 step: 69, loss is 0.016619933769106865\n",
      "epoch: 3 step: 70, loss is 0.0009267284185625613\n",
      "epoch: 3 step: 71, loss is 0.0023814155720174313\n",
      "epoch: 3 step: 72, loss is 0.001894207438454032\n",
      "epoch: 3 step: 73, loss is 0.019170362502336502\n",
      "epoch: 3 step: 74, loss is 0.0034477724693715572\n",
      "epoch: 3 step: 75, loss is 0.008799919858574867\n",
      "epoch: 3 step: 76, loss is 0.00869588740170002\n",
      "epoch: 3 step: 77, loss is 0.001031160936690867\n",
      "epoch: 3 step: 78, loss is 0.002272692508995533\n",
      "epoch: 3 step: 79, loss is 0.0018676891922950745\n",
      "epoch: 3 step: 80, loss is 0.011826559901237488\n",
      "epoch: 3 step: 81, loss is 0.002359963720664382\n",
      "epoch: 3 step: 82, loss is 0.004557333886623383\n",
      "epoch: 3 step: 83, loss is 0.0021859684493392706\n",
      "epoch: 3 step: 84, loss is 0.002319954801350832\n",
      "epoch: 3 step: 85, loss is 0.0027143768966197968\n",
      "epoch: 3 step: 86, loss is 0.0016249516047537327\n",
      "epoch: 3 step: 87, loss is 0.003064730204641819\n",
      "epoch: 3 step: 88, loss is 0.003356759436428547\n",
      "epoch: 3 step: 89, loss is 0.008865357376635075\n",
      "epoch: 3 step: 90, loss is 0.0025764356832951307\n",
      "epoch: 3 step: 91, loss is 0.0016311752842739224\n",
      "epoch: 3 step: 92, loss is 0.006696207448840141\n",
      "epoch: 3 step: 93, loss is 0.0011304831132292747\n",
      "epoch: 3 step: 94, loss is 0.004516527056694031\n",
      "epoch: 3 step: 95, loss is 0.005105725955218077\n",
      "epoch: 3 step: 96, loss is 0.004399051424115896\n",
      "epoch: 3 step: 97, loss is 0.008201816119253635\n",
      "epoch: 3 step: 98, loss is 0.0036580096930265427\n",
      "epoch: 3 step: 99, loss is 0.010986323468387127\n",
      "epoch: 3 step: 100, loss is 0.002421800745651126\n",
      "epoch: 3 step: 101, loss is 0.0024990940000861883\n",
      "epoch: 3 step: 102, loss is 0.0006615262245759368\n",
      "epoch: 3 step: 103, loss is 0.013805720955133438\n",
      "epoch: 3 step: 104, loss is 0.001832315931096673\n",
      "epoch: 3 step: 105, loss is 0.016185060143470764\n",
      "epoch: 3 step: 106, loss is 0.005272825248539448\n",
      "epoch: 3 step: 107, loss is 0.0025132556911557913\n",
      "epoch: 3 step: 108, loss is 0.0017396676121279597\n",
      "epoch: 3 step: 109, loss is 0.0021510012447834015\n",
      "epoch: 3 step: 110, loss is 0.039865314960479736\n",
      "epoch: 3 step: 111, loss is 0.001974252751097083\n",
      "epoch: 3 step: 112, loss is 0.004559003747999668\n",
      "epoch: 3 step: 113, loss is 0.0015545247588306665\n",
      "epoch: 3 step: 114, loss is 0.0033365818671882153\n",
      "epoch: 3 step: 115, loss is 0.007673928514122963\n",
      "epoch: 3 step: 116, loss is 0.002107763197273016\n",
      "epoch: 3 step: 117, loss is 0.017305005341768265\n",
      "epoch: 3 step: 118, loss is 0.00358751998282969\n",
      "epoch: 3 step: 119, loss is 0.005064275115728378\n",
      "epoch: 3 step: 120, loss is 0.007727668620646\n",
      "epoch: 3 step: 121, loss is 0.0031652208417654037\n",
      "epoch: 3 step: 122, loss is 0.007526815868914127\n",
      "epoch: 3 step: 123, loss is 0.0018955543637275696\n",
      "epoch: 3 step: 124, loss is 0.002906437497586012\n",
      "epoch: 3 step: 125, loss is 0.0023754083085805178\n",
      "epoch: 3 step: 126, loss is 0.005418991670012474\n",
      "epoch: 3 step: 127, loss is 0.0008568816119804978\n",
      "epoch: 3 step: 128, loss is 0.0026797016616910696\n",
      "epoch: 3 step: 129, loss is 0.0015263353707268834\n",
      "epoch: 3 step: 130, loss is 0.002891186624765396\n",
      "epoch: 3 step: 131, loss is 0.040781572461128235\n",
      "epoch: 3 step: 132, loss is 0.012507835403084755\n",
      "epoch: 3 step: 133, loss is 0.002159870695322752\n",
      "epoch: 3 step: 134, loss is 0.016931984573602676\n",
      "epoch: 3 step: 135, loss is 0.00430658832192421\n",
      "epoch: 3 step: 136, loss is 0.002747742924839258\n",
      "epoch: 3 step: 137, loss is 0.003796528559178114\n",
      "epoch: 3 step: 138, loss is 0.006998744793236256\n",
      "epoch: 3 step: 139, loss is 0.0010541633237153292\n",
      "epoch: 3 step: 140, loss is 0.004613293334841728\n",
      "epoch: 3 step: 141, loss is 0.0030017816461622715\n",
      "epoch: 3 step: 142, loss is 0.00172118516638875\n",
      "epoch: 3 step: 143, loss is 0.0017407566774636507\n",
      "epoch: 3 step: 144, loss is 0.002897880505770445\n",
      "epoch: 3 step: 145, loss is 0.004972831346094608\n",
      "epoch: 3 step: 146, loss is 0.0028869030065834522\n",
      "epoch: 3 step: 147, loss is 0.0033470315393060446\n",
      "epoch: 3 step: 148, loss is 0.0005342356162145734\n",
      "epoch: 3 step: 149, loss is 0.006763902492821217\n",
      "epoch: 3 step: 150, loss is 0.0039013230707496405\n",
      "epoch: 3 step: 151, loss is 0.0010329073993489146\n",
      "epoch: 3 step: 152, loss is 0.0026854530442506075\n",
      "epoch: 3 step: 153, loss is 0.003647740464657545\n",
      "epoch: 3 step: 154, loss is 0.0019031576812267303\n",
      "epoch: 3 step: 155, loss is 0.0014100046828389168\n",
      "epoch: 3 step: 156, loss is 0.006471728906035423\n",
      "epoch: 3 step: 157, loss is 0.000947331078350544\n",
      "epoch: 3 step: 158, loss is 0.0018797593656927347\n",
      "epoch: 3 step: 159, loss is 0.00041403138311579823\n",
      "epoch: 3 step: 160, loss is 0.0027548258658498526\n",
      "epoch: 3 step: 161, loss is 0.008035632781684399\n",
      "epoch: 3 step: 162, loss is 0.002667482942342758\n",
      "epoch: 3 step: 163, loss is 0.0018185924272984266\n",
      "epoch: 3 step: 164, loss is 0.0019879096653312445\n",
      "epoch: 3 step: 165, loss is 0.0009983674390241504\n",
      "epoch: 3 step: 166, loss is 0.00249755522236228\n",
      "epoch: 3 step: 167, loss is 0.0022937157191336155\n",
      "epoch: 3 step: 168, loss is 0.001176237128674984\n",
      "epoch: 3 step: 169, loss is 0.005505524575710297\n",
      "epoch: 3 step: 170, loss is 0.0013642498524859548\n",
      "epoch: 3 step: 171, loss is 0.006338638253509998\n",
      "epoch: 3 step: 172, loss is 0.007112131454050541\n",
      "epoch: 3 step: 173, loss is 0.002249914687126875\n",
      "epoch: 3 step: 174, loss is 0.0019112047739326954\n",
      "epoch: 3 step: 175, loss is 0.003747503738850355\n",
      "epoch: 3 step: 176, loss is 0.005995666608214378\n",
      "epoch: 3 step: 177, loss is 0.0003434768586885184\n",
      "epoch: 3 step: 178, loss is 0.00231963605619967\n",
      "epoch: 3 step: 179, loss is 0.0025774184614419937\n",
      "epoch: 3 step: 180, loss is 0.008346274495124817\n",
      "epoch: 3 step: 181, loss is 0.010348068550229073\n",
      "epoch: 3 step: 182, loss is 0.0023769475519657135\n",
      "epoch: 3 step: 183, loss is 0.0023619853891432285\n",
      "epoch: 3 step: 184, loss is 0.0006680843653157353\n",
      "epoch: 3 step: 185, loss is 0.0021763567347079515\n",
      "epoch: 3 step: 186, loss is 0.001076135435141623\n",
      "epoch: 3 step: 187, loss is 0.0014944365248084068\n",
      "epoch: 3 step: 188, loss is 0.004644312895834446\n",
      "epoch: 3 step: 189, loss is 0.0018095531268045306\n",
      "epoch: 3 step: 190, loss is 0.021152563393115997\n",
      "epoch: 3 step: 191, loss is 0.0005201643798500299\n",
      "epoch: 3 step: 192, loss is 0.002571995137259364\n",
      "epoch: 3 step: 193, loss is 0.0042107244953513145\n",
      "epoch: 3 step: 194, loss is 0.004818737972527742\n",
      "epoch: 3 step: 195, loss is 0.001844712533056736\n",
      "epoch: 3 step: 196, loss is 0.0026683704927563667\n",
      "epoch: 3 step: 197, loss is 0.000705743848811835\n",
      "epoch: 3 step: 198, loss is 0.0027286596596240997\n",
      "epoch: 3 step: 199, loss is 0.0068593560717999935\n",
      "epoch: 3 step: 200, loss is 0.0015541708562523127\n",
      "epoch: 3 step: 201, loss is 0.0013392333639785647\n",
      "epoch: 3 step: 202, loss is 0.0035554692149162292\n",
      "epoch: 3 step: 203, loss is 0.0008335734019055963\n",
      "epoch: 3 step: 204, loss is 0.004679598845541477\n",
      "epoch: 3 step: 205, loss is 0.001020750729367137\n",
      "epoch: 3 step: 206, loss is 0.003988026641309261\n",
      "epoch: 3 step: 207, loss is 0.012575757689774036\n",
      "epoch: 3 step: 208, loss is 0.0017637660494074225\n",
      "epoch: 3 step: 209, loss is 0.0012999504106119275\n",
      "epoch: 3 step: 210, loss is 0.008033590391278267\n",
      "epoch: 3 step: 211, loss is 0.000976360053755343\n",
      "epoch: 3 step: 212, loss is 0.005499683320522308\n",
      "epoch: 3 step: 213, loss is 0.0018591773696243763\n",
      "epoch: 3 step: 214, loss is 0.0030369749292731285\n",
      "epoch: 3 step: 215, loss is 0.0037937068846076727\n",
      "epoch: 3 step: 216, loss is 0.00273146596737206\n",
      "epoch: 3 step: 217, loss is 0.0009260056540369987\n",
      "epoch: 3 step: 218, loss is 0.0020345738157629967\n",
      "epoch: 3 step: 219, loss is 0.0016466068336740136\n",
      "epoch: 3 step: 220, loss is 0.0023604263551533222\n",
      "epoch: 3 step: 221, loss is 0.008524085395038128\n",
      "epoch: 3 step: 222, loss is 0.00107396743260324\n",
      "epoch: 3 step: 223, loss is 0.0034472725819796324\n",
      "epoch: 3 step: 224, loss is 0.001613759552128613\n",
      "epoch: 3 step: 225, loss is 0.0037603736855089664\n",
      "epoch: 3 step: 226, loss is 0.0032087566796690226\n",
      "epoch: 3 step: 227, loss is 0.0020742572378367186\n",
      "epoch: 3 step: 228, loss is 0.003245706669986248\n",
      "epoch: 3 step: 229, loss is 0.005544567946344614\n",
      "epoch: 3 step: 230, loss is 0.01580400951206684\n",
      "epoch: 3 step: 231, loss is 0.0009490511147305369\n",
      "epoch: 3 step: 232, loss is 0.001076934626325965\n",
      "epoch: 3 step: 233, loss is 0.0037082238122820854\n",
      "epoch: 3 step: 234, loss is 0.001990954391658306\n",
      "epoch: 3 step: 235, loss is 0.0008150184876285493\n",
      "epoch: 3 step: 236, loss is 0.008792639710009098\n",
      "epoch: 3 step: 237, loss is 0.002396117430180311\n",
      "epoch: 3 step: 238, loss is 0.003213900839909911\n",
      "epoch: 3 step: 239, loss is 0.0017655694391578436\n",
      "epoch: 3 step: 240, loss is 0.0032227151095867157\n",
      "epoch: 3 step: 241, loss is 0.005959612783044577\n",
      "epoch: 3 step: 242, loss is 0.0016829222440719604\n",
      "epoch: 3 step: 243, loss is 0.000991633627563715\n",
      "epoch: 3 step: 244, loss is 0.0018374613719061017\n",
      "epoch: 3 step: 245, loss is 0.0017232918180525303\n",
      "epoch: 3 step: 246, loss is 0.0015940184239298105\n",
      "epoch: 3 step: 247, loss is 0.0008965898887254298\n",
      "epoch: 3 step: 248, loss is 0.002481620293110609\n",
      "epoch: 3 step: 249, loss is 0.0023005090188235044\n",
      "epoch: 3 step: 250, loss is 0.004233793821185827\n",
      "epoch: 3 step: 251, loss is 0.0026067059952765703\n",
      "epoch: 3 step: 252, loss is 0.01262480579316616\n",
      "epoch: 3 step: 253, loss is 0.0010575111955404282\n",
      "epoch: 3 step: 254, loss is 0.01550290733575821\n",
      "epoch: 3 step: 255, loss is 0.00745677575469017\n",
      "epoch: 3 step: 256, loss is 0.001364454161375761\n",
      "epoch: 3 step: 257, loss is 0.03155721724033356\n",
      "epoch: 3 step: 258, loss is 0.002156113274395466\n",
      "epoch: 3 step: 259, loss is 0.013630629517138004\n",
      "epoch: 3 step: 260, loss is 0.0011509214527904987\n",
      "epoch: 3 step: 261, loss is 0.00407476956024766\n",
      "epoch: 3 step: 262, loss is 0.0008720684563741088\n",
      "epoch: 3 step: 263, loss is 0.005565647967159748\n",
      "epoch: 3 step: 264, loss is 0.001000034622848034\n",
      "epoch: 3 step: 265, loss is 0.001228405861184001\n",
      "epoch: 3 step: 266, loss is 0.0033929585479199886\n",
      "epoch: 3 step: 267, loss is 0.0022744673769921064\n",
      "epoch: 3 step: 268, loss is 0.0061824689619243145\n",
      "epoch: 3 step: 269, loss is 0.0011690501123666763\n",
      "epoch: 3 step: 270, loss is 0.001708147581666708\n",
      "epoch: 3 step: 271, loss is 0.0073945168405771255\n",
      "epoch: 3 step: 272, loss is 0.001069638878107071\n",
      "epoch: 3 step: 273, loss is 0.001990951132029295\n",
      "epoch: 3 step: 274, loss is 0.0015947786159813404\n",
      "epoch: 3 step: 275, loss is 0.006249140948057175\n",
      "epoch: 3 step: 276, loss is 0.000703181023709476\n",
      "epoch: 3 step: 277, loss is 0.007821641862392426\n",
      "epoch: 3 step: 278, loss is 0.002034800825640559\n",
      "epoch: 3 step: 279, loss is 0.0038254684768617153\n",
      "epoch: 3 step: 280, loss is 0.0296262726187706\n",
      "epoch: 3 step: 281, loss is 0.0014862727839499712\n",
      "epoch: 3 step: 282, loss is 0.0017810421995818615\n",
      "epoch: 3 step: 283, loss is 0.00735477264970541\n",
      "epoch: 3 step: 284, loss is 0.04606005921959877\n",
      "epoch: 3 step: 285, loss is 0.0063173603266477585\n",
      "epoch: 3 step: 286, loss is 0.0019239484099671245\n",
      "epoch: 3 step: 287, loss is 0.0023227562196552753\n",
      "epoch: 3 step: 288, loss is 0.0015819602413102984\n",
      "epoch: 3 step: 289, loss is 0.001878284034319222\n",
      "epoch: 3 step: 290, loss is 0.003589579602703452\n",
      "epoch: 3 step: 291, loss is 0.0013992569874972105\n",
      "epoch: 3 step: 292, loss is 0.0009917306015267968\n",
      "epoch: 3 step: 293, loss is 0.00245044962503016\n",
      "epoch: 3 step: 294, loss is 0.001030882354825735\n",
      "epoch: 3 step: 295, loss is 0.0048780436627566814\n",
      "epoch: 3 step: 296, loss is 0.013964397832751274\n",
      "epoch: 3 step: 297, loss is 0.0012369032483547926\n",
      "epoch: 3 step: 298, loss is 0.003039796371012926\n",
      "epoch: 3 step: 299, loss is 0.0026210458017885685\n",
      "epoch: 3 step: 300, loss is 0.0008183958707377315\n",
      "epoch: 3 step: 301, loss is 0.0023042424581944942\n",
      "epoch: 3 step: 302, loss is 0.00431418139487505\n",
      "epoch: 3 step: 303, loss is 0.005799930077046156\n",
      "epoch: 3 step: 304, loss is 0.0013806644128635526\n",
      "epoch: 3 step: 305, loss is 0.0011916917283087969\n",
      "epoch: 3 step: 306, loss is 0.00296480068936944\n",
      "epoch: 3 step: 307, loss is 0.004776867106556892\n",
      "epoch: 3 step: 308, loss is 0.0012299151858314872\n",
      "epoch: 3 step: 309, loss is 0.001103771384805441\n",
      "epoch: 3 step: 310, loss is 0.0007403052877634764\n",
      "epoch: 3 step: 311, loss is 0.003923882730305195\n",
      "epoch: 3 step: 312, loss is 0.001055690459907055\n",
      "epoch: 3 step: 313, loss is 0.0008001463720574975\n",
      "epoch: 3 step: 314, loss is 0.0014388184063136578\n",
      "epoch: 3 step: 315, loss is 0.001699293265119195\n",
      "epoch: 3 step: 316, loss is 0.007678574416786432\n",
      "epoch: 3 step: 317, loss is 0.0019085665699094534\n",
      "epoch: 3 step: 318, loss is 0.0014485018327832222\n",
      "epoch: 3 step: 319, loss is 0.0019275846425443888\n",
      "epoch: 3 step: 320, loss is 0.0015046587213873863\n",
      "epoch: 3 step: 321, loss is 0.0011430917074903846\n",
      "epoch: 3 step: 322, loss is 0.002321213949471712\n",
      "epoch: 3 step: 323, loss is 0.0043762908317148685\n",
      "epoch: 3 step: 324, loss is 0.003500788239762187\n",
      "epoch: 3 step: 325, loss is 0.0028243418782949448\n",
      "epoch: 3 step: 326, loss is 0.00033765961416065693\n",
      "epoch: 3 step: 327, loss is 0.0016519664786756039\n",
      "epoch: 3 step: 328, loss is 0.002008050447329879\n",
      "epoch: 3 step: 329, loss is 0.004153810441493988\n",
      "epoch: 3 step: 330, loss is 0.009498070925474167\n",
      "epoch: 3 step: 331, loss is 0.0020705536007881165\n",
      "epoch: 3 step: 332, loss is 0.00011802209337474778\n",
      "epoch: 3 step: 333, loss is 0.0005171031225472689\n",
      "epoch: 3 step: 334, loss is 0.0012020689900964499\n",
      "epoch: 3 step: 335, loss is 0.0007563865510746837\n",
      "epoch: 3 step: 336, loss is 0.001443027053028345\n",
      "epoch: 3 step: 337, loss is 0.00371155864559114\n",
      "epoch: 3 step: 338, loss is 0.0016643095295876265\n",
      "epoch: 3 step: 339, loss is 0.0011598425917327404\n",
      "epoch: 3 step: 340, loss is 0.0014234925620257854\n",
      "epoch: 3 step: 341, loss is 0.000813325576018542\n",
      "epoch: 3 step: 342, loss is 0.00696140993386507\n",
      "epoch: 3 step: 343, loss is 0.0012305494165048003\n",
      "epoch: 3 step: 344, loss is 0.0032810643315315247\n",
      "epoch: 3 step: 345, loss is 0.0010747574269771576\n",
      "epoch: 3 step: 346, loss is 0.0023538239765912294\n",
      "epoch: 3 step: 347, loss is 0.003429859410971403\n",
      "epoch: 3 step: 348, loss is 0.005411236546933651\n",
      "epoch: 3 step: 349, loss is 0.0007208723109215498\n",
      "epoch: 3 step: 350, loss is 0.0012372361961752176\n",
      "epoch: 3 step: 351, loss is 0.0040010856464505196\n",
      "epoch: 3 step: 352, loss is 0.0013332763919606805\n",
      "epoch: 3 step: 353, loss is 0.0015461642760783434\n",
      "epoch: 3 step: 354, loss is 0.002044295659288764\n",
      "epoch: 3 step: 355, loss is 0.003700731787830591\n",
      "epoch: 3 step: 356, loss is 0.0005364296957850456\n",
      "epoch: 3 step: 357, loss is 0.0012241285294294357\n",
      "epoch: 3 step: 358, loss is 0.0010908462572842836\n",
      "epoch: 3 step: 359, loss is 0.02134079858660698\n",
      "epoch: 3 step: 360, loss is 0.0013208275195211172\n",
      "epoch: 3 step: 361, loss is 0.003197602229192853\n",
      "epoch: 3 step: 362, loss is 0.002064815256744623\n",
      "epoch: 3 step: 363, loss is 0.0013865395449101925\n",
      "epoch: 3 step: 364, loss is 0.0017017567297443748\n",
      "epoch: 3 step: 365, loss is 0.0027254726737737656\n",
      "epoch: 3 step: 366, loss is 0.0006434888346120715\n",
      "epoch: 3 step: 367, loss is 0.00581740029156208\n",
      "epoch: 3 step: 368, loss is 0.000855230784509331\n",
      "epoch: 3 step: 369, loss is 0.008920889347791672\n",
      "epoch: 3 step: 370, loss is 0.002703503705561161\n",
      "epoch: 3 step: 371, loss is 0.0007223043940030038\n",
      "epoch: 3 step: 372, loss is 0.002229614881798625\n",
      "epoch: 3 step: 373, loss is 0.002221922157332301\n",
      "epoch: 3 step: 374, loss is 0.003642889205366373\n",
      "epoch: 3 step: 375, loss is 0.0002586267364677042\n",
      "epoch: 3 step: 376, loss is 0.0012372225755825639\n",
      "epoch: 3 step: 377, loss is 0.0012283542891964316\n",
      "epoch: 3 step: 378, loss is 0.0013644809368997812\n",
      "epoch: 3 step: 379, loss is 0.0015163000207394361\n",
      "epoch: 3 step: 380, loss is 0.00045778497587889433\n",
      "epoch: 3 step: 381, loss is 0.0010503890225663781\n",
      "epoch: 3 step: 382, loss is 0.00385389756411314\n",
      "epoch: 3 step: 383, loss is 0.0009663716773502529\n",
      "epoch: 3 step: 384, loss is 0.0008681131876073778\n",
      "epoch: 3 step: 385, loss is 0.0013462693896144629\n",
      "epoch: 3 step: 386, loss is 0.0009659803472459316\n",
      "epoch: 3 step: 387, loss is 0.002481561154127121\n",
      "epoch: 3 step: 388, loss is 0.0016421456821262836\n",
      "epoch: 3 step: 389, loss is 0.0010384880006313324\n",
      "epoch: 3 step: 390, loss is 0.0018823693972080946\n",
      "epoch: 3 step: 391, loss is 0.0005342240328900516\n",
      "epoch: 3 step: 392, loss is 0.0005618837894871831\n",
      "epoch: 3 step: 393, loss is 0.0028178866486996412\n",
      "epoch: 3 step: 394, loss is 0.00325606158003211\n",
      "epoch: 3 step: 395, loss is 0.0013103258097544312\n",
      "epoch: 3 step: 396, loss is 0.0022445947397500277\n",
      "epoch: 3 step: 397, loss is 0.002575671300292015\n",
      "epoch: 3 step: 398, loss is 0.0007827794761396945\n",
      "epoch: 3 step: 399, loss is 0.0008979251724667847\n",
      "epoch: 3 step: 400, loss is 0.0004945076070725918\n",
      "epoch: 3 step: 401, loss is 0.01139254029840231\n",
      "epoch: 3 step: 402, loss is 0.0008188413339667022\n",
      "epoch: 3 step: 403, loss is 0.013456596061587334\n",
      "epoch: 3 step: 404, loss is 0.0016524099046364427\n",
      "epoch: 3 step: 405, loss is 0.0006816044915467501\n",
      "epoch: 3 step: 406, loss is 0.004423036240041256\n",
      "epoch: 3 step: 407, loss is 0.0051697855815291405\n",
      "epoch: 3 step: 408, loss is 0.010520895011723042\n",
      "epoch: 3 step: 409, loss is 0.0005688156234100461\n",
      "epoch: 3 step: 410, loss is 0.0013363361358642578\n",
      "epoch: 3 step: 411, loss is 0.0003547237138263881\n",
      "epoch: 3 step: 412, loss is 0.001938856323249638\n",
      "epoch: 3 step: 413, loss is 0.0010538266506046057\n",
      "epoch: 3 step: 414, loss is 0.0032106454018503428\n",
      "epoch: 3 step: 415, loss is 0.0011606058105826378\n",
      "epoch: 3 step: 416, loss is 0.0024215641897171736\n",
      "epoch: 3 step: 417, loss is 0.0012349742464721203\n",
      "epoch: 3 step: 418, loss is 0.002119493205100298\n",
      "epoch: 3 step: 419, loss is 0.0012088908115401864\n",
      "epoch: 3 step: 420, loss is 0.015656644478440285\n",
      "epoch: 3 step: 421, loss is 0.0014346475945785642\n",
      "epoch: 3 step: 422, loss is 0.005319416522979736\n",
      "epoch: 3 step: 423, loss is 0.0021456109825521708\n",
      "epoch: 3 step: 424, loss is 0.0024618294555693865\n",
      "epoch: 3 step: 425, loss is 0.0018994570709764957\n",
      "epoch: 3 step: 426, loss is 0.0027212677523493767\n",
      "epoch: 3 step: 427, loss is 0.0033266141545027494\n",
      "epoch: 3 step: 428, loss is 0.005899601150304079\n",
      "epoch: 3 step: 429, loss is 0.026289600878953934\n",
      "epoch: 3 step: 430, loss is 0.0013535618782043457\n",
      "epoch: 3 step: 431, loss is 0.006108047440648079\n",
      "epoch: 3 step: 432, loss is 0.002218890469521284\n",
      "epoch: 3 step: 433, loss is 0.000933036906644702\n",
      "epoch: 3 step: 434, loss is 0.0046041556634008884\n",
      "epoch: 3 step: 435, loss is 0.0005586098995991051\n",
      "epoch: 3 step: 436, loss is 0.0010717506520450115\n",
      "epoch: 3 step: 437, loss is 0.000482505711261183\n",
      "epoch: 3 step: 438, loss is 0.0024022958241403103\n",
      "epoch: 3 step: 439, loss is 0.005513109266757965\n",
      "epoch: 3 step: 440, loss is 0.0001916110486490652\n",
      "epoch: 3 step: 441, loss is 0.0007776333950459957\n",
      "epoch: 3 step: 442, loss is 0.00041201154817827046\n",
      "epoch: 3 step: 443, loss is 0.004698056261986494\n",
      "epoch: 3 step: 444, loss is 0.010922038927674294\n",
      "epoch: 3 step: 445, loss is 0.0008384413085877895\n",
      "epoch: 3 step: 446, loss is 0.0005191977834329009\n",
      "epoch: 3 step: 447, loss is 0.0019602251704782248\n",
      "epoch: 3 step: 448, loss is 0.0011164634488523006\n",
      "epoch: 3 step: 449, loss is 0.0003698520886246115\n",
      "epoch: 3 step: 450, loss is 0.0012599451001733541\n",
      "epoch: 3 step: 451, loss is 0.0014050842728465796\n",
      "epoch: 3 step: 452, loss is 0.0013126893900334835\n",
      "epoch: 3 step: 453, loss is 0.001579492585733533\n",
      "epoch: 3 step: 454, loss is 0.0009201454231515527\n",
      "epoch: 3 step: 455, loss is 0.0028230533935129642\n",
      "epoch: 3 step: 456, loss is 0.0012711662566289306\n",
      "epoch: 3 step: 457, loss is 0.011185740120708942\n",
      "epoch: 3 step: 458, loss is 0.008982876315712929\n",
      "epoch: 3 step: 459, loss is 0.0026967148296535015\n",
      "epoch: 3 step: 460, loss is 0.001387883210554719\n",
      "epoch: 3 step: 461, loss is 0.001439724350348115\n",
      "epoch: 3 step: 462, loss is 0.0004536354390438646\n",
      "epoch: 3 step: 463, loss is 0.004622367676347494\n",
      "epoch: 3 step: 464, loss is 0.002783452160656452\n",
      "epoch: 3 step: 465, loss is 0.0015042631421238184\n",
      "epoch: 3 step: 466, loss is 0.001792945433408022\n",
      "epoch: 3 step: 467, loss is 0.0006935628480277956\n",
      "epoch: 3 step: 468, loss is 0.0003676096093840897\n",
      "epoch: 3 step: 469, loss is 0.0012256423942744732\n",
      "epoch: 3 step: 470, loss is 0.006527533754706383\n",
      "epoch: 3 step: 471, loss is 0.0013018203899264336\n",
      "epoch: 3 step: 472, loss is 0.0005789975402876735\n",
      "epoch: 3 step: 473, loss is 0.001297665759921074\n",
      "epoch: 3 step: 474, loss is 0.00038677945849485695\n",
      "epoch: 3 step: 475, loss is 0.003323496086522937\n",
      "epoch: 3 step: 476, loss is 0.0007592356996610761\n",
      "epoch: 3 step: 477, loss is 0.0007157196523621678\n",
      "epoch: 3 step: 478, loss is 0.003939701244235039\n",
      "epoch: 3 step: 479, loss is 0.0010872806888073683\n",
      "epoch: 3 step: 480, loss is 0.00033465976594015956\n",
      "epoch: 3 step: 481, loss is 0.0006571119301952422\n",
      "epoch: 3 step: 482, loss is 0.005429385229945183\n",
      "epoch: 3 step: 483, loss is 0.0013215590734034777\n",
      "epoch: 3 step: 484, loss is 0.0009456896805204451\n",
      "epoch: 3 step: 485, loss is 0.0018100535962730646\n",
      "epoch: 3 step: 486, loss is 0.0015295760240405798\n",
      "epoch: 3 step: 487, loss is 0.0013789471704512835\n",
      "epoch: 3 step: 488, loss is 0.000925521191675216\n",
      "epoch: 3 step: 489, loss is 0.0012258028145879507\n",
      "epoch: 3 step: 490, loss is 0.0015755926724523306\n",
      "epoch: 3 step: 491, loss is 0.004408542066812515\n",
      "epoch: 3 step: 492, loss is 0.002790895290672779\n",
      "epoch: 3 step: 493, loss is 0.0008146644104272127\n",
      "epoch: 3 step: 494, loss is 0.0009612663416191936\n",
      "epoch: 3 step: 495, loss is 0.0007694356609135866\n",
      "epoch: 3 step: 496, loss is 0.003946242388337851\n",
      "epoch: 3 step: 497, loss is 0.005950547754764557\n",
      "epoch: 3 step: 498, loss is 0.001397832646034658\n",
      "epoch: 3 step: 499, loss is 0.0007741544977761805\n",
      "epoch: 3 step: 500, loss is 0.0005479108076542616\n",
      "epoch: 3 step: 501, loss is 0.01948220282793045\n",
      "epoch: 3 step: 502, loss is 0.0013518081977963448\n",
      "epoch: 3 step: 503, loss is 0.0003142557688988745\n",
      "epoch: 3 step: 504, loss is 0.00684878695756197\n",
      "epoch: 3 step: 505, loss is 0.0002705473161768168\n",
      "epoch: 3 step: 506, loss is 0.0014937748201191425\n",
      "epoch: 3 step: 507, loss is 0.0003415938699617982\n",
      "epoch: 3 step: 508, loss is 0.002762360032647848\n",
      "epoch: 3 step: 509, loss is 0.0007016717572696507\n",
      "epoch: 3 step: 510, loss is 0.0024601127952337265\n",
      "epoch: 3 step: 511, loss is 0.00045480759581550956\n",
      "epoch: 3 step: 512, loss is 0.0011581170838326216\n",
      "epoch: 3 step: 513, loss is 0.0029546902514994144\n",
      "epoch: 3 step: 514, loss is 0.005161664914339781\n",
      "epoch: 3 step: 515, loss is 0.0005262961494736373\n",
      "epoch: 3 step: 516, loss is 0.000854064361192286\n",
      "epoch: 3 step: 517, loss is 0.009323966689407825\n",
      "epoch: 3 step: 518, loss is 0.0005816001212224364\n",
      "epoch: 3 step: 519, loss is 0.017019862309098244\n",
      "epoch: 3 step: 520, loss is 0.0007250062189996243\n",
      "epoch: 3 step: 521, loss is 0.0016836306313052773\n",
      "epoch: 3 step: 522, loss is 0.002521348651498556\n",
      "epoch: 3 step: 523, loss is 0.0023179820273071527\n",
      "epoch: 3 step: 524, loss is 0.0005839936784468591\n",
      "epoch: 3 step: 525, loss is 0.0030180271714925766\n",
      "epoch: 3 step: 526, loss is 0.0007754801772534847\n",
      "epoch: 3 step: 527, loss is 0.001005076803267002\n",
      "epoch: 3 step: 528, loss is 0.0008430156158283353\n",
      "epoch: 3 step: 529, loss is 0.0008860509842634201\n",
      "epoch: 3 step: 530, loss is 0.0007456301245838404\n",
      "epoch: 3 step: 531, loss is 0.0010761043522506952\n",
      "epoch: 3 step: 532, loss is 0.0007576439529657364\n",
      "epoch: 3 step: 533, loss is 0.0004006510425824672\n",
      "epoch: 3 step: 534, loss is 0.0003752533229999244\n",
      "epoch: 3 step: 535, loss is 0.0009362202836200595\n",
      "epoch: 3 step: 536, loss is 0.0008395444601774216\n",
      "epoch: 3 step: 537, loss is 0.00078305893111974\n",
      "epoch: 3 step: 538, loss is 0.003379466710612178\n",
      "epoch: 3 step: 539, loss is 0.006782909855246544\n",
      "epoch: 3 step: 540, loss is 0.0010751275112852454\n",
      "epoch: 3 step: 541, loss is 0.0038557404186576605\n",
      "epoch: 3 step: 542, loss is 0.0017824368551373482\n",
      "epoch: 3 step: 543, loss is 0.0015907499473541975\n",
      "epoch: 3 step: 544, loss is 0.0013381345197558403\n",
      "epoch: 3 step: 545, loss is 0.0003221955557819456\n",
      "epoch: 3 step: 546, loss is 0.00033202991471625865\n",
      "epoch: 3 step: 547, loss is 0.000593780365306884\n",
      "epoch: 3 step: 548, loss is 0.00044791342224925756\n",
      "epoch: 3 step: 549, loss is 0.00034923566272482276\n",
      "epoch: 3 step: 550, loss is 0.011915478855371475\n",
      "epoch: 3 step: 551, loss is 0.002074344316497445\n",
      "epoch: 3 step: 552, loss is 0.015250733122229576\n",
      "epoch: 3 step: 553, loss is 0.003090537153184414\n",
      "epoch: 3 step: 554, loss is 0.0018012290820479393\n",
      "epoch: 3 step: 555, loss is 0.00029724554042331874\n",
      "epoch: 3 step: 556, loss is 0.0010088762501254678\n",
      "epoch: 3 step: 557, loss is 0.0033565156627446413\n",
      "epoch: 3 step: 558, loss is 0.0016151077579706907\n",
      "epoch: 3 step: 559, loss is 0.0006748547311872244\n",
      "epoch: 3 step: 560, loss is 0.0010878824396058917\n",
      "epoch: 3 step: 561, loss is 0.004076358396559954\n",
      "epoch: 3 step: 562, loss is 0.00033919475390575826\n",
      "epoch: 3 step: 563, loss is 0.0012359835673123598\n",
      "epoch: 3 step: 564, loss is 0.0012375407386571169\n",
      "epoch: 3 step: 565, loss is 0.0009538560407236218\n",
      "epoch: 3 step: 566, loss is 0.0017387691186740994\n",
      "epoch: 3 step: 567, loss is 0.0016406887443736196\n",
      "epoch: 3 step: 568, loss is 0.00029091042233631015\n",
      "epoch: 3 step: 569, loss is 0.0035072993487119675\n",
      "epoch: 3 step: 570, loss is 0.0017768798861652613\n",
      "epoch: 3 step: 571, loss is 0.0023814933374524117\n",
      "epoch: 3 step: 572, loss is 0.0005912076449021697\n",
      "epoch: 3 step: 573, loss is 0.0008937902748584747\n",
      "epoch: 3 step: 574, loss is 0.0003386799362488091\n",
      "epoch: 3 step: 575, loss is 0.0008923103450797498\n",
      "epoch: 3 step: 576, loss is 0.000886689405888319\n",
      "epoch: 3 step: 577, loss is 0.0014434602344408631\n",
      "epoch: 3 step: 578, loss is 0.005820687394589186\n",
      "epoch: 3 step: 579, loss is 0.0022466066293418407\n",
      "epoch: 3 step: 580, loss is 0.0022038088645786047\n",
      "epoch: 3 step: 581, loss is 0.0016274552326649427\n",
      "epoch: 3 step: 582, loss is 0.0005245513166300952\n",
      "epoch: 3 step: 583, loss is 0.0009304628474637866\n",
      "epoch: 3 step: 584, loss is 0.0005750145064666867\n",
      "epoch: 3 step: 585, loss is 0.0026067306753247976\n",
      "epoch: 3 step: 586, loss is 0.0018110278761014342\n",
      "epoch: 3 step: 587, loss is 0.002131308428943157\n",
      "epoch: 3 step: 588, loss is 0.0018587035592645407\n",
      "epoch: 3 step: 589, loss is 0.00026594355585984886\n",
      "epoch: 3 step: 590, loss is 0.001059073838405311\n",
      "epoch: 3 step: 591, loss is 0.0010896357707679272\n",
      "epoch: 3 step: 592, loss is 0.0009747315780259669\n",
      "epoch: 3 step: 593, loss is 0.004549732431769371\n",
      "epoch: 3 step: 594, loss is 0.0010180280078202486\n",
      "epoch: 3 step: 595, loss is 0.0005519227124750614\n",
      "epoch: 3 step: 596, loss is 0.0003580391639843583\n",
      "Train epoch time: 16673.064 ms, per step time: 27.975 ms\n",
      "epoch: 4 step: 1, loss is 0.0009238263010047376\n",
      "epoch: 4 step: 2, loss is 0.0003912781539838761\n",
      "epoch: 4 step: 3, loss is 0.0007726024487055838\n",
      "epoch: 4 step: 4, loss is 0.0012226775288581848\n",
      "epoch: 4 step: 5, loss is 0.000537702813744545\n",
      "epoch: 4 step: 6, loss is 0.0009269833099097013\n",
      "epoch: 4 step: 7, loss is 0.0005657660658471286\n",
      "epoch: 4 step: 8, loss is 0.0006122582708485425\n",
      "epoch: 4 step: 9, loss is 0.00038832693826407194\n",
      "epoch: 4 step: 10, loss is 0.0031780079007148743\n",
      "epoch: 4 step: 11, loss is 0.000703665311448276\n",
      "epoch: 4 step: 12, loss is 0.000850302807521075\n",
      "epoch: 4 step: 13, loss is 0.00034110932028852403\n",
      "epoch: 4 step: 14, loss is 0.00041421031346544623\n",
      "epoch: 4 step: 15, loss is 0.002793647116050124\n",
      "epoch: 4 step: 16, loss is 0.0008878427906893194\n",
      "epoch: 4 step: 17, loss is 0.0005702290218323469\n",
      "epoch: 4 step: 18, loss is 0.0012309553567320108\n",
      "epoch: 4 step: 19, loss is 0.0004742656019516289\n",
      "epoch: 4 step: 20, loss is 0.00030263204826042056\n",
      "epoch: 4 step: 21, loss is 0.00031846697675064206\n",
      "epoch: 4 step: 22, loss is 0.0041175223886966705\n",
      "epoch: 4 step: 23, loss is 0.0005406155833043158\n",
      "epoch: 4 step: 24, loss is 0.0024527725763618946\n",
      "epoch: 4 step: 25, loss is 0.0007025286322459579\n",
      "epoch: 4 step: 26, loss is 0.0008100425475277007\n",
      "epoch: 4 step: 27, loss is 0.00025308458134531975\n",
      "epoch: 4 step: 28, loss is 0.00010704186570364982\n",
      "epoch: 4 step: 29, loss is 0.0005006106803193688\n",
      "epoch: 4 step: 30, loss is 0.0006074990378692746\n",
      "epoch: 4 step: 31, loss is 0.0009487044299021363\n",
      "epoch: 4 step: 32, loss is 0.002035517245531082\n",
      "epoch: 4 step: 33, loss is 0.0003467377391643822\n",
      "epoch: 4 step: 34, loss is 0.0002679515164345503\n",
      "epoch: 4 step: 35, loss is 0.0002457954687997699\n",
      "epoch: 4 step: 36, loss is 0.0017954836366698146\n",
      "epoch: 4 step: 37, loss is 0.0029700016602873802\n",
      "epoch: 4 step: 38, loss is 0.009354421868920326\n",
      "epoch: 4 step: 39, loss is 0.0003145577502436936\n",
      "epoch: 4 step: 40, loss is 0.00274180481210351\n",
      "epoch: 4 step: 41, loss is 0.0009756348445080221\n",
      "epoch: 4 step: 42, loss is 0.0003308347659185529\n",
      "epoch: 4 step: 43, loss is 0.0008025661809369922\n",
      "epoch: 4 step: 44, loss is 0.001985055860131979\n",
      "epoch: 4 step: 45, loss is 0.005806739907711744\n",
      "epoch: 4 step: 46, loss is 0.0003026332415174693\n",
      "epoch: 4 step: 47, loss is 0.0003563619393389672\n",
      "epoch: 4 step: 48, loss is 0.0008674189448356628\n",
      "epoch: 4 step: 49, loss is 0.005482276435941458\n",
      "epoch: 4 step: 50, loss is 0.0018205069936811924\n",
      "epoch: 4 step: 51, loss is 0.0005266206571832299\n",
      "epoch: 4 step: 52, loss is 0.0013708770275115967\n",
      "epoch: 4 step: 53, loss is 0.0012695114128291607\n",
      "epoch: 4 step: 54, loss is 0.0004106102860532701\n",
      "epoch: 4 step: 55, loss is 0.002771633444353938\n",
      "epoch: 4 step: 56, loss is 0.000576245307456702\n",
      "epoch: 4 step: 57, loss is 0.0004049903654959053\n",
      "epoch: 4 step: 58, loss is 9.729804878588766e-05\n",
      "epoch: 4 step: 59, loss is 0.00035679919528774917\n",
      "epoch: 4 step: 60, loss is 0.0005297516472637653\n",
      "epoch: 4 step: 61, loss is 0.0025493339635431767\n",
      "epoch: 4 step: 62, loss is 0.001643025316298008\n",
      "epoch: 4 step: 63, loss is 0.001398811349645257\n",
      "epoch: 4 step: 64, loss is 0.001377925742417574\n",
      "epoch: 4 step: 65, loss is 0.0011695484863594174\n",
      "epoch: 4 step: 66, loss is 0.0011442438699305058\n",
      "epoch: 4 step: 67, loss is 0.0019359532743692398\n",
      "epoch: 4 step: 68, loss is 0.00391888152807951\n",
      "epoch: 4 step: 69, loss is 0.000501663307659328\n",
      "epoch: 4 step: 70, loss is 0.0002121704164892435\n",
      "epoch: 4 step: 71, loss is 0.000519620138220489\n",
      "epoch: 4 step: 72, loss is 0.006015169434249401\n",
      "epoch: 4 step: 73, loss is 0.0016708422917872667\n",
      "epoch: 4 step: 74, loss is 0.002297867788001895\n",
      "epoch: 4 step: 75, loss is 0.0009553880081512034\n",
      "epoch: 4 step: 76, loss is 0.0008263053023256361\n",
      "epoch: 4 step: 77, loss is 0.0003315496433060616\n",
      "epoch: 4 step: 78, loss is 0.0009442538139410317\n",
      "epoch: 4 step: 79, loss is 0.0017965788720175624\n",
      "epoch: 4 step: 80, loss is 0.0007830943213775754\n",
      "epoch: 4 step: 81, loss is 0.0051976158283650875\n",
      "epoch: 4 step: 82, loss is 0.006126564461737871\n",
      "epoch: 4 step: 83, loss is 0.0017200192669406533\n",
      "epoch: 4 step: 84, loss is 0.0010027600219473243\n",
      "epoch: 4 step: 85, loss is 0.00047346315113827586\n",
      "epoch: 4 step: 86, loss is 0.0009243696695193648\n",
      "epoch: 4 step: 87, loss is 0.0008285099174827337\n",
      "epoch: 4 step: 88, loss is 0.0005411095335148275\n",
      "epoch: 4 step: 89, loss is 0.00039401871617883444\n",
      "epoch: 4 step: 90, loss is 0.0009420820861123502\n",
      "epoch: 4 step: 91, loss is 0.0004101564991287887\n",
      "epoch: 4 step: 92, loss is 0.0006502552423626184\n",
      "epoch: 4 step: 93, loss is 0.0005346622783690691\n",
      "epoch: 4 step: 94, loss is 0.0006033852696418762\n",
      "epoch: 4 step: 95, loss is 0.0013017300516366959\n",
      "epoch: 4 step: 96, loss is 0.0010258693946525455\n",
      "epoch: 4 step: 97, loss is 0.0009587430977262557\n",
      "epoch: 4 step: 98, loss is 0.00029275970882736146\n",
      "epoch: 4 step: 99, loss is 0.0024375496432185173\n",
      "epoch: 4 step: 100, loss is 0.00032403451041318476\n",
      "epoch: 4 step: 101, loss is 0.0003241299418732524\n",
      "epoch: 4 step: 102, loss is 0.00035209840280003846\n",
      "epoch: 4 step: 103, loss is 0.008520887233316898\n",
      "epoch: 4 step: 104, loss is 0.0005324165103957057\n",
      "epoch: 4 step: 105, loss is 0.012138156220316887\n",
      "epoch: 4 step: 106, loss is 0.00016152138414327055\n",
      "epoch: 4 step: 107, loss is 0.0015538831939920783\n",
      "epoch: 4 step: 108, loss is 0.0009886499028652906\n",
      "epoch: 4 step: 109, loss is 0.0011896381620317698\n",
      "epoch: 4 step: 110, loss is 0.0013277198886498809\n",
      "epoch: 4 step: 111, loss is 0.0012026827316731215\n",
      "epoch: 4 step: 112, loss is 0.0014625441981479526\n",
      "epoch: 4 step: 113, loss is 0.00021348791779018939\n",
      "epoch: 4 step: 114, loss is 0.0012968545779585838\n",
      "epoch: 4 step: 115, loss is 0.00041934679029509425\n",
      "epoch: 4 step: 116, loss is 0.0007534933974966407\n",
      "epoch: 4 step: 117, loss is 0.009938721545040607\n",
      "epoch: 4 step: 118, loss is 0.0004209838807582855\n",
      "epoch: 4 step: 119, loss is 0.0013513237936422229\n",
      "epoch: 4 step: 120, loss is 0.0005839822115376592\n",
      "epoch: 4 step: 121, loss is 0.00026792328571900725\n",
      "epoch: 4 step: 122, loss is 0.001391413970850408\n",
      "epoch: 4 step: 123, loss is 0.0010606410214677453\n",
      "epoch: 4 step: 124, loss is 0.0002465012948960066\n",
      "epoch: 4 step: 125, loss is 0.003949086647480726\n",
      "epoch: 4 step: 126, loss is 0.0007435328443534672\n",
      "epoch: 4 step: 127, loss is 0.00019043972133658826\n",
      "epoch: 4 step: 128, loss is 0.0008829862345010042\n",
      "epoch: 4 step: 129, loss is 0.000798235007096082\n",
      "epoch: 4 step: 130, loss is 0.0023867785930633545\n",
      "epoch: 4 step: 131, loss is 0.001524094259366393\n",
      "epoch: 4 step: 132, loss is 0.0005552669754251838\n",
      "epoch: 4 step: 133, loss is 0.0023892302997410297\n",
      "epoch: 4 step: 134, loss is 0.0018254296155646443\n",
      "epoch: 4 step: 135, loss is 0.0004560105735436082\n",
      "epoch: 4 step: 136, loss is 0.0006853974773548543\n",
      "epoch: 4 step: 137, loss is 0.001194075564853847\n",
      "epoch: 4 step: 138, loss is 0.0006206344114616513\n",
      "epoch: 4 step: 139, loss is 0.0002464450662955642\n",
      "epoch: 4 step: 140, loss is 0.0009029317880049348\n",
      "epoch: 4 step: 141, loss is 0.000749521073885262\n",
      "epoch: 4 step: 142, loss is 0.00044848996913060546\n",
      "epoch: 4 step: 143, loss is 0.0003067199431825429\n",
      "epoch: 4 step: 144, loss is 0.0005387641722336411\n",
      "epoch: 4 step: 145, loss is 0.0005951399798505008\n",
      "epoch: 4 step: 146, loss is 0.0009493261459283531\n",
      "epoch: 4 step: 147, loss is 0.0016772592207416892\n",
      "epoch: 4 step: 148, loss is 0.0010280885035172105\n",
      "epoch: 4 step: 149, loss is 0.0016803620383143425\n",
      "epoch: 4 step: 150, loss is 0.0010901781497523189\n",
      "epoch: 4 step: 151, loss is 0.00022063858341425657\n",
      "epoch: 4 step: 152, loss is 0.002693451475352049\n",
      "epoch: 4 step: 153, loss is 0.0012235607719048858\n",
      "epoch: 4 step: 154, loss is 0.0004582992405630648\n",
      "epoch: 4 step: 155, loss is 0.0010341794695705175\n",
      "epoch: 4 step: 156, loss is 0.0006305683054961264\n",
      "epoch: 4 step: 157, loss is 0.00043892787653021514\n",
      "epoch: 4 step: 158, loss is 0.00020123645663261414\n",
      "epoch: 4 step: 159, loss is 0.0008577607804909348\n",
      "epoch: 4 step: 160, loss is 0.0007527056150138378\n",
      "epoch: 4 step: 161, loss is 0.0009730048477649689\n",
      "epoch: 4 step: 162, loss is 0.00047636026283726096\n",
      "epoch: 4 step: 163, loss is 0.0004372037365101278\n",
      "epoch: 4 step: 164, loss is 0.0015944532351568341\n",
      "epoch: 4 step: 165, loss is 0.000540457374881953\n",
      "epoch: 4 step: 166, loss is 0.000605280976742506\n",
      "epoch: 4 step: 167, loss is 0.00022212184558156878\n",
      "epoch: 4 step: 168, loss is 0.003335121087729931\n",
      "epoch: 4 step: 169, loss is 0.0003531213151291013\n",
      "epoch: 4 step: 170, loss is 0.0003564664802979678\n",
      "epoch: 4 step: 171, loss is 0.0023806432727724314\n",
      "epoch: 4 step: 172, loss is 0.0002925392473116517\n",
      "epoch: 4 step: 173, loss is 0.003947407938539982\n",
      "epoch: 4 step: 174, loss is 0.0007073204033076763\n",
      "epoch: 4 step: 175, loss is 0.0006709069712087512\n",
      "epoch: 4 step: 176, loss is 0.00040061207255348563\n",
      "epoch: 4 step: 177, loss is 0.00016533787129446864\n",
      "epoch: 4 step: 178, loss is 0.0006766606820747256\n",
      "epoch: 4 step: 179, loss is 0.0004243821313139051\n",
      "epoch: 4 step: 180, loss is 0.0013777504209429026\n",
      "epoch: 4 step: 181, loss is 0.0015317057259380817\n",
      "epoch: 4 step: 182, loss is 0.000334947690134868\n",
      "epoch: 4 step: 183, loss is 0.0006223758682608604\n",
      "epoch: 4 step: 184, loss is 0.00021545974595937878\n",
      "epoch: 4 step: 185, loss is 0.0012699371436610818\n",
      "epoch: 4 step: 186, loss is 0.0009431758662685752\n",
      "epoch: 4 step: 187, loss is 0.0035771226976066828\n",
      "epoch: 4 step: 188, loss is 0.00012026351760141551\n",
      "epoch: 4 step: 189, loss is 0.001011578133329749\n",
      "epoch: 4 step: 190, loss is 0.001653264625929296\n",
      "epoch: 4 step: 191, loss is 0.0005794736789539456\n",
      "epoch: 4 step: 192, loss is 0.00048602986498735845\n",
      "epoch: 4 step: 193, loss is 0.003192728850990534\n",
      "epoch: 4 step: 194, loss is 0.0013973901513963938\n",
      "epoch: 4 step: 195, loss is 0.00019813487597275525\n",
      "epoch: 4 step: 196, loss is 0.00027172680711373687\n",
      "epoch: 4 step: 197, loss is 0.005703850649297237\n",
      "epoch: 4 step: 198, loss is 0.0025337222032248974\n",
      "epoch: 4 step: 199, loss is 0.0013781482120975852\n",
      "epoch: 4 step: 200, loss is 0.0005126888281665742\n",
      "epoch: 4 step: 201, loss is 0.0003491607494652271\n",
      "epoch: 4 step: 202, loss is 0.0026833107694983482\n",
      "epoch: 4 step: 203, loss is 0.0009633470326662064\n",
      "epoch: 4 step: 204, loss is 0.0022450201213359833\n",
      "epoch: 4 step: 205, loss is 0.00029678744613192976\n",
      "epoch: 4 step: 206, loss is 0.0005429355660453439\n",
      "epoch: 4 step: 207, loss is 0.0001465106033720076\n",
      "epoch: 4 step: 208, loss is 0.0004145550774410367\n",
      "epoch: 4 step: 209, loss is 0.0008634966798126698\n",
      "epoch: 4 step: 210, loss is 0.0013026166707277298\n",
      "epoch: 4 step: 211, loss is 0.0006568024400621653\n",
      "epoch: 4 step: 212, loss is 0.001779558602720499\n",
      "epoch: 4 step: 213, loss is 0.0002452475600875914\n",
      "epoch: 4 step: 214, loss is 0.0006391592905856669\n",
      "epoch: 4 step: 215, loss is 0.0008075452642515302\n",
      "epoch: 4 step: 216, loss is 0.0012235105969011784\n",
      "epoch: 4 step: 217, loss is 0.0002926121524069458\n",
      "epoch: 4 step: 218, loss is 0.0008215712150558829\n",
      "epoch: 4 step: 219, loss is 0.00031274830689653754\n",
      "epoch: 4 step: 220, loss is 0.00033121235901489854\n",
      "epoch: 4 step: 221, loss is 0.0005829985020682216\n",
      "epoch: 4 step: 222, loss is 0.0007254293304868042\n",
      "epoch: 4 step: 223, loss is 0.0010548216523602605\n",
      "epoch: 4 step: 224, loss is 0.0011724448995664716\n",
      "epoch: 4 step: 225, loss is 0.0011245940113440156\n",
      "epoch: 4 step: 226, loss is 0.00035685731563717127\n",
      "epoch: 4 step: 227, loss is 0.0007934292661957443\n",
      "epoch: 4 step: 228, loss is 0.0005544886225834489\n",
      "epoch: 4 step: 229, loss is 0.0012092696269974113\n",
      "epoch: 4 step: 230, loss is 0.0009013342205435038\n",
      "epoch: 4 step: 231, loss is 0.00034633075119927526\n",
      "epoch: 4 step: 232, loss is 0.0011047523003071547\n",
      "epoch: 4 step: 233, loss is 0.0007970996084623039\n",
      "epoch: 4 step: 234, loss is 0.0006320553366094828\n",
      "epoch: 4 step: 235, loss is 0.0004094793985132128\n",
      "epoch: 4 step: 236, loss is 0.00043909464147873223\n",
      "epoch: 4 step: 237, loss is 0.0010766582563519478\n",
      "epoch: 4 step: 238, loss is 0.0005474545760080218\n",
      "epoch: 4 step: 239, loss is 0.0026430038269609213\n",
      "epoch: 4 step: 240, loss is 0.0006378267426043749\n",
      "epoch: 4 step: 241, loss is 0.00045977855916135013\n",
      "epoch: 4 step: 242, loss is 0.0007532882154919207\n",
      "epoch: 4 step: 243, loss is 0.0012610425474122167\n",
      "epoch: 4 step: 244, loss is 0.0009483284084126353\n",
      "epoch: 4 step: 245, loss is 0.0015212728176265955\n",
      "epoch: 4 step: 246, loss is 0.0005843708058819175\n",
      "epoch: 4 step: 247, loss is 0.0004320504958741367\n",
      "epoch: 4 step: 248, loss is 0.0005136046092957258\n",
      "epoch: 4 step: 249, loss is 0.0005550135392695665\n",
      "epoch: 4 step: 250, loss is 0.0006517904112115502\n",
      "epoch: 4 step: 251, loss is 0.0003192632575519383\n",
      "epoch: 4 step: 252, loss is 0.007893991656601429\n",
      "epoch: 4 step: 253, loss is 0.0010957738850265741\n",
      "epoch: 4 step: 254, loss is 0.011686335317790508\n",
      "epoch: 4 step: 255, loss is 0.00018645905947778374\n",
      "epoch: 4 step: 256, loss is 0.0013022448401898146\n",
      "epoch: 4 step: 257, loss is 0.0009031803929246962\n",
      "epoch: 4 step: 258, loss is 0.0008687138324603438\n",
      "epoch: 4 step: 259, loss is 0.0017089758766815066\n",
      "epoch: 4 step: 260, loss is 0.00036177271977066994\n",
      "epoch: 4 step: 261, loss is 0.0011473201448097825\n",
      "epoch: 4 step: 262, loss is 0.0005291133420541883\n",
      "epoch: 4 step: 263, loss is 0.0005998016567900777\n",
      "epoch: 4 step: 264, loss is 0.00047224017907865345\n",
      "epoch: 4 step: 265, loss is 0.0007353509427048266\n",
      "epoch: 4 step: 266, loss is 0.0012939563021063805\n",
      "epoch: 4 step: 267, loss is 0.0005436521023511887\n",
      "epoch: 4 step: 268, loss is 0.0009155211737379432\n",
      "epoch: 4 step: 269, loss is 0.0006329327588900924\n",
      "epoch: 4 step: 270, loss is 0.00031155551550909877\n",
      "epoch: 4 step: 271, loss is 0.002292888006195426\n",
      "epoch: 4 step: 272, loss is 0.0007583898259326816\n",
      "epoch: 4 step: 273, loss is 0.0001751965464791283\n",
      "epoch: 4 step: 274, loss is 0.0006937033031135798\n",
      "epoch: 4 step: 275, loss is 0.0006516100838780403\n",
      "epoch: 4 step: 276, loss is 0.0004726858751382679\n",
      "epoch: 4 step: 277, loss is 0.0009901709854602814\n",
      "epoch: 4 step: 278, loss is 0.00026777820312418044\n",
      "epoch: 4 step: 279, loss is 0.0005251223919913173\n",
      "epoch: 4 step: 280, loss is 0.0007914999732747674\n",
      "epoch: 4 step: 281, loss is 0.0008142173755913973\n",
      "epoch: 4 step: 282, loss is 0.001119966502301395\n",
      "epoch: 4 step: 283, loss is 0.0009347087470814586\n",
      "epoch: 4 step: 284, loss is 0.0007751584053039551\n",
      "epoch: 4 step: 285, loss is 0.0006167676765471697\n",
      "epoch: 4 step: 286, loss is 0.00038216670509427786\n",
      "epoch: 4 step: 287, loss is 0.000337955221766606\n",
      "epoch: 4 step: 288, loss is 0.000349021254805848\n",
      "epoch: 4 step: 289, loss is 0.0005816782359033823\n",
      "epoch: 4 step: 290, loss is 0.0015818080864846706\n",
      "epoch: 4 step: 291, loss is 0.00038270268123596907\n",
      "epoch: 4 step: 292, loss is 0.00087365647777915\n",
      "epoch: 4 step: 293, loss is 0.0003954945132136345\n",
      "epoch: 4 step: 294, loss is 0.0008382261148653924\n",
      "epoch: 4 step: 295, loss is 0.0006009616772644222\n",
      "epoch: 4 step: 296, loss is 0.000699339434504509\n",
      "epoch: 4 step: 297, loss is 0.00019781179435085505\n",
      "epoch: 4 step: 298, loss is 0.0013735740212723613\n",
      "epoch: 4 step: 299, loss is 0.0015623713843524456\n",
      "epoch: 4 step: 300, loss is 0.00048398476792499423\n",
      "epoch: 4 step: 301, loss is 0.0009065921185538173\n",
      "epoch: 4 step: 302, loss is 0.0009535880526527762\n",
      "epoch: 4 step: 303, loss is 0.0006762668490409851\n",
      "epoch: 4 step: 304, loss is 0.0013595633208751678\n",
      "epoch: 4 step: 305, loss is 0.0006766282021999359\n",
      "epoch: 4 step: 306, loss is 0.001012235414236784\n",
      "epoch: 4 step: 307, loss is 0.0004905309760943055\n",
      "epoch: 4 step: 308, loss is 0.0009361224947497249\n",
      "epoch: 4 step: 309, loss is 0.0004178740200586617\n",
      "epoch: 4 step: 310, loss is 0.001509316498413682\n",
      "epoch: 4 step: 311, loss is 0.00041116689681075513\n",
      "epoch: 4 step: 312, loss is 0.0007476569153368473\n",
      "epoch: 4 step: 313, loss is 0.0011537938844412565\n",
      "epoch: 4 step: 314, loss is 0.0008422872051596642\n",
      "epoch: 4 step: 315, loss is 0.0011350220302119851\n",
      "epoch: 4 step: 316, loss is 0.0017785478848963976\n",
      "epoch: 4 step: 317, loss is 0.0026612423826009035\n",
      "epoch: 4 step: 318, loss is 0.000457013986306265\n",
      "epoch: 4 step: 319, loss is 0.0006017372361384332\n",
      "epoch: 4 step: 320, loss is 0.0008450149325653911\n",
      "epoch: 4 step: 321, loss is 0.0007648557075299323\n",
      "epoch: 4 step: 322, loss is 0.0009148106910288334\n",
      "epoch: 4 step: 323, loss is 0.0004939861828461289\n",
      "epoch: 4 step: 324, loss is 0.0005154446698725224\n",
      "epoch: 4 step: 325, loss is 0.0007412533741444349\n",
      "epoch: 4 step: 326, loss is 6.440588913392276e-05\n",
      "epoch: 4 step: 327, loss is 0.0007932051667012274\n",
      "epoch: 4 step: 328, loss is 0.00044678006088361144\n",
      "epoch: 4 step: 329, loss is 0.0007246469613164663\n",
      "epoch: 4 step: 330, loss is 0.0022988514974713326\n",
      "epoch: 4 step: 331, loss is 0.0005275264848023653\n",
      "epoch: 4 step: 332, loss is 0.0006527507212013006\n",
      "epoch: 4 step: 333, loss is 0.0005069025792181492\n",
      "epoch: 4 step: 334, loss is 0.0015281651867553592\n",
      "epoch: 4 step: 335, loss is 0.001055402448400855\n",
      "epoch: 4 step: 336, loss is 0.0006995968287810683\n",
      "epoch: 4 step: 337, loss is 0.00015197679749689996\n",
      "epoch: 4 step: 338, loss is 0.0009181724744848907\n",
      "epoch: 4 step: 339, loss is 0.0005494423676282167\n",
      "epoch: 4 step: 340, loss is 0.0002530281781218946\n",
      "epoch: 4 step: 341, loss is 0.00046783380093984306\n",
      "epoch: 4 step: 342, loss is 0.002328865695744753\n",
      "epoch: 4 step: 343, loss is 0.000941279053222388\n",
      "epoch: 4 step: 344, loss is 0.00032275973353534937\n",
      "epoch: 4 step: 345, loss is 0.00047593406634405255\n",
      "epoch: 4 step: 346, loss is 0.007112724706530571\n",
      "epoch: 4 step: 347, loss is 0.003832317888736725\n",
      "epoch: 4 step: 348, loss is 0.0015237311599776149\n",
      "epoch: 4 step: 349, loss is 0.0005965585587546229\n",
      "epoch: 4 step: 350, loss is 0.00140562339220196\n",
      "epoch: 4 step: 351, loss is 0.0020744376815855503\n",
      "epoch: 4 step: 352, loss is 0.0005335971945896745\n",
      "epoch: 4 step: 353, loss is 0.0006258783396333456\n",
      "epoch: 4 step: 354, loss is 0.008688877336680889\n",
      "epoch: 4 step: 355, loss is 0.001428837887942791\n",
      "epoch: 4 step: 356, loss is 0.00013601072714664042\n",
      "epoch: 4 step: 357, loss is 0.0004909062990918756\n",
      "epoch: 4 step: 358, loss is 0.00017943800776265562\n",
      "epoch: 4 step: 359, loss is 0.0011818436905741692\n",
      "epoch: 4 step: 360, loss is 0.0009112312691286206\n",
      "epoch: 4 step: 361, loss is 0.0020428046118468046\n",
      "epoch: 4 step: 362, loss is 0.00039824729901738465\n",
      "epoch: 4 step: 363, loss is 0.0007262793369591236\n",
      "epoch: 4 step: 364, loss is 0.0016750105423852801\n",
      "epoch: 4 step: 365, loss is 0.0007858618628233671\n",
      "epoch: 4 step: 366, loss is 0.0002382560633122921\n",
      "epoch: 4 step: 367, loss is 0.0004950273432768881\n",
      "epoch: 4 step: 368, loss is 0.00022984562383498996\n",
      "epoch: 4 step: 369, loss is 0.00027636572485789657\n",
      "epoch: 4 step: 370, loss is 0.0009916166309267282\n",
      "epoch: 4 step: 371, loss is 0.0007612258777953684\n",
      "epoch: 4 step: 372, loss is 0.0013745891628786922\n",
      "epoch: 4 step: 373, loss is 0.000788713397923857\n",
      "epoch: 4 step: 374, loss is 0.0012448879424482584\n",
      "epoch: 4 step: 375, loss is 0.00013971402950119227\n",
      "epoch: 4 step: 376, loss is 0.0005435883649624884\n",
      "epoch: 4 step: 377, loss is 0.0005317465402185917\n",
      "epoch: 4 step: 378, loss is 0.002129876520484686\n",
      "epoch: 4 step: 379, loss is 0.001683188951574266\n",
      "epoch: 4 step: 380, loss is 0.000591884832829237\n",
      "epoch: 4 step: 381, loss is 0.0006843589944764972\n",
      "epoch: 4 step: 382, loss is 0.0007913414156064391\n",
      "epoch: 4 step: 383, loss is 0.0010648047318682075\n",
      "epoch: 4 step: 384, loss is 0.0006476494017988443\n",
      "epoch: 4 step: 385, loss is 0.00047138225636444986\n",
      "epoch: 4 step: 386, loss is 0.0012422861764207482\n",
      "epoch: 4 step: 387, loss is 0.0004041905631311238\n",
      "epoch: 4 step: 388, loss is 0.0007743657333776355\n",
      "epoch: 4 step: 389, loss is 0.00028678029775619507\n",
      "epoch: 4 step: 390, loss is 0.0005860532401129603\n",
      "epoch: 4 step: 391, loss is 0.0013527845731005073\n",
      "epoch: 4 step: 392, loss is 0.0024626816157251596\n",
      "epoch: 4 step: 393, loss is 0.0014601459261029959\n",
      "epoch: 4 step: 394, loss is 0.0018048158381134272\n",
      "epoch: 4 step: 395, loss is 0.0007281709113158286\n",
      "epoch: 4 step: 396, loss is 0.0005428976728580892\n",
      "epoch: 4 step: 397, loss is 0.00025609275326132774\n",
      "epoch: 4 step: 398, loss is 0.00030070467619225383\n",
      "epoch: 4 step: 399, loss is 0.0007838052697479725\n",
      "epoch: 4 step: 400, loss is 0.00027670274721458554\n",
      "epoch: 4 step: 401, loss is 0.008252725936472416\n",
      "epoch: 4 step: 402, loss is 0.004090713802725077\n",
      "epoch: 4 step: 403, loss is 0.01216896902769804\n",
      "epoch: 4 step: 404, loss is 0.00020804133964702487\n",
      "epoch: 4 step: 405, loss is 0.002263085450977087\n",
      "epoch: 4 step: 406, loss is 0.000570340605918318\n",
      "epoch: 4 step: 407, loss is 0.0010469432454556227\n",
      "epoch: 4 step: 408, loss is 0.002360332291573286\n",
      "epoch: 4 step: 409, loss is 0.0007100834627635777\n",
      "epoch: 4 step: 410, loss is 0.0009298569057136774\n",
      "epoch: 4 step: 411, loss is 0.00048293962026946247\n",
      "epoch: 4 step: 412, loss is 0.0010149432346224785\n",
      "epoch: 4 step: 413, loss is 0.0005518686375580728\n",
      "epoch: 4 step: 414, loss is 0.0007986879791133106\n",
      "epoch: 4 step: 415, loss is 0.0007485826499760151\n",
      "epoch: 4 step: 416, loss is 0.0004325122863519937\n",
      "epoch: 4 step: 417, loss is 0.0009266724810004234\n",
      "epoch: 4 step: 418, loss is 0.0006298969965428114\n",
      "epoch: 4 step: 419, loss is 0.0002310256677446887\n",
      "epoch: 4 step: 420, loss is 0.0017920867539942265\n",
      "epoch: 4 step: 421, loss is 0.001127499039284885\n",
      "epoch: 4 step: 422, loss is 0.0001787483924999833\n",
      "epoch: 4 step: 423, loss is 0.0006702127866446972\n",
      "epoch: 4 step: 424, loss is 0.001241253805346787\n",
      "epoch: 4 step: 425, loss is 0.0002331069263163954\n",
      "epoch: 4 step: 426, loss is 0.0007118884241208434\n",
      "epoch: 4 step: 427, loss is 0.0003252013120800257\n",
      "epoch: 4 step: 428, loss is 0.0011317938333377242\n",
      "epoch: 4 step: 429, loss is 0.000864801462739706\n",
      "epoch: 4 step: 430, loss is 0.0006088436930440366\n",
      "epoch: 4 step: 431, loss is 0.000868231407366693\n",
      "epoch: 4 step: 432, loss is 0.0014405855908989906\n",
      "epoch: 4 step: 433, loss is 0.0006260631489567459\n",
      "epoch: 4 step: 434, loss is 0.0005135378451086581\n",
      "epoch: 4 step: 435, loss is 0.0005987340700812638\n",
      "epoch: 4 step: 436, loss is 0.0004627119633369148\n",
      "epoch: 4 step: 437, loss is 0.0007998301298357546\n",
      "epoch: 4 step: 438, loss is 0.0013922607759013772\n",
      "epoch: 4 step: 439, loss is 0.0005190001102164388\n",
      "epoch: 4 step: 440, loss is 0.0007696596439927816\n",
      "epoch: 4 step: 441, loss is 0.00041249635978601873\n",
      "epoch: 4 step: 442, loss is 0.0007830343092791736\n",
      "epoch: 4 step: 443, loss is 0.0018701591761782765\n",
      "epoch: 4 step: 444, loss is 0.0003433747624512762\n",
      "epoch: 4 step: 445, loss is 0.0015634482260793447\n",
      "epoch: 4 step: 446, loss is 0.0004938333877362311\n",
      "epoch: 4 step: 447, loss is 0.0002809532452374697\n",
      "epoch: 4 step: 448, loss is 0.0010704282904043794\n",
      "epoch: 4 step: 449, loss is 0.00033115685801021755\n",
      "epoch: 4 step: 450, loss is 0.0003205460379831493\n",
      "epoch: 4 step: 451, loss is 0.0005651769461110234\n",
      "epoch: 4 step: 452, loss is 0.0007815493736416101\n",
      "epoch: 4 step: 453, loss is 0.0005286983796395361\n",
      "epoch: 4 step: 454, loss is 0.0005315844900906086\n",
      "epoch: 4 step: 455, loss is 0.0009863936575129628\n",
      "epoch: 4 step: 456, loss is 0.0002177902206312865\n",
      "epoch: 4 step: 457, loss is 0.003556233597919345\n",
      "epoch: 4 step: 458, loss is 0.0008715327130630612\n",
      "epoch: 4 step: 459, loss is 0.0024671743158251047\n",
      "epoch: 4 step: 460, loss is 0.00046499614836648107\n",
      "epoch: 4 step: 461, loss is 0.0004288450290914625\n",
      "epoch: 4 step: 462, loss is 0.0019611527677625418\n",
      "epoch: 4 step: 463, loss is 0.0008400121005252004\n",
      "epoch: 4 step: 464, loss is 0.0010786601342260838\n",
      "epoch: 4 step: 465, loss is 0.00050161819672212\n",
      "epoch: 4 step: 466, loss is 0.0009502721950411797\n",
      "epoch: 4 step: 467, loss is 0.00015875260578468442\n",
      "epoch: 4 step: 468, loss is 0.00034756335662677884\n",
      "epoch: 4 step: 469, loss is 0.00568149471655488\n",
      "epoch: 4 step: 470, loss is 0.0005027094157412648\n",
      "epoch: 4 step: 471, loss is 0.00039996608393266797\n",
      "epoch: 4 step: 472, loss is 0.0009677289053797722\n",
      "epoch: 4 step: 473, loss is 0.0008407849818468094\n",
      "epoch: 4 step: 474, loss is 0.00045207637595012784\n",
      "epoch: 4 step: 475, loss is 9.650858555687591e-05\n",
      "epoch: 4 step: 476, loss is 0.0006052319658920169\n",
      "epoch: 4 step: 477, loss is 0.00041280704317614436\n",
      "epoch: 4 step: 478, loss is 0.0013054209994152188\n",
      "epoch: 4 step: 479, loss is 0.0010713431984186172\n",
      "epoch: 4 step: 480, loss is 0.0005604168400168419\n",
      "epoch: 4 step: 481, loss is 0.0007680213893763721\n",
      "epoch: 4 step: 482, loss is 0.00016873383719939739\n",
      "epoch: 4 step: 483, loss is 0.0009614619193598628\n",
      "epoch: 4 step: 484, loss is 0.0009751905454322696\n",
      "epoch: 4 step: 485, loss is 0.0007548952708020806\n",
      "epoch: 4 step: 486, loss is 0.00020712938567157835\n",
      "epoch: 4 step: 487, loss is 0.000843707995954901\n",
      "epoch: 4 step: 488, loss is 0.0006746016442775726\n",
      "epoch: 4 step: 489, loss is 0.0004312586097512394\n",
      "epoch: 4 step: 490, loss is 0.0002465405850671232\n",
      "epoch: 4 step: 491, loss is 0.0024394909851253033\n",
      "epoch: 4 step: 492, loss is 0.001982821151614189\n",
      "epoch: 4 step: 493, loss is 0.0002692712005227804\n",
      "epoch: 4 step: 494, loss is 0.0006464541656896472\n",
      "epoch: 4 step: 495, loss is 0.000717739574611187\n",
      "epoch: 4 step: 496, loss is 0.0015131414402276278\n",
      "epoch: 4 step: 497, loss is 0.0018300751689821482\n",
      "epoch: 4 step: 498, loss is 0.00028470539837144315\n",
      "epoch: 4 step: 499, loss is 0.0007249644258990884\n",
      "epoch: 4 step: 500, loss is 0.0011478543747216463\n",
      "epoch: 4 step: 501, loss is 0.0007623923011124134\n",
      "epoch: 4 step: 502, loss is 0.0018539921147748828\n",
      "epoch: 4 step: 503, loss is 0.0005041453987360001\n",
      "epoch: 4 step: 504, loss is 0.0005694463616237044\n",
      "epoch: 4 step: 505, loss is 0.00018875558453146368\n",
      "epoch: 4 step: 506, loss is 0.00038485327968373895\n",
      "epoch: 4 step: 507, loss is 0.00024802470579743385\n",
      "epoch: 4 step: 508, loss is 0.0010808701626956463\n",
      "epoch: 4 step: 509, loss is 0.00029051804449409246\n",
      "epoch: 4 step: 510, loss is 0.0015365834115073085\n",
      "epoch: 4 step: 511, loss is 0.0002864490379579365\n",
      "epoch: 4 step: 512, loss is 0.0008534870576113462\n",
      "epoch: 4 step: 513, loss is 0.0014259337913244963\n",
      "epoch: 4 step: 514, loss is 0.0007945891702547669\n",
      "epoch: 4 step: 515, loss is 0.00026880245422944427\n",
      "epoch: 4 step: 516, loss is 0.001003618584945798\n",
      "epoch: 4 step: 517, loss is 0.0003404589369893074\n",
      "epoch: 4 step: 518, loss is 0.00040918082231655717\n",
      "epoch: 4 step: 519, loss is 0.001437295926734805\n",
      "epoch: 4 step: 520, loss is 0.0006725899875164032\n",
      "epoch: 4 step: 521, loss is 0.0015651307767257094\n",
      "epoch: 4 step: 522, loss is 0.0010264832526445389\n",
      "epoch: 4 step: 523, loss is 0.0017348357941955328\n",
      "epoch: 4 step: 524, loss is 0.0001255643874173984\n",
      "epoch: 4 step: 525, loss is 0.0010693096555769444\n",
      "epoch: 4 step: 526, loss is 0.004222954623401165\n",
      "epoch: 4 step: 527, loss is 0.0006701972451992333\n",
      "epoch: 4 step: 528, loss is 0.0007839120808057487\n",
      "epoch: 4 step: 529, loss is 0.0005259037134237587\n",
      "epoch: 4 step: 530, loss is 0.0006848964258097112\n",
      "epoch: 4 step: 531, loss is 0.0015498874709010124\n",
      "epoch: 4 step: 532, loss is 0.0019475470762699842\n",
      "epoch: 4 step: 533, loss is 0.0005071526393294334\n",
      "epoch: 4 step: 534, loss is 0.0006131071131676435\n",
      "epoch: 4 step: 535, loss is 0.0013541507069021463\n",
      "epoch: 4 step: 536, loss is 0.0003574518486857414\n",
      "epoch: 4 step: 537, loss is 0.000903463049326092\n",
      "epoch: 4 step: 538, loss is 0.0002848711155820638\n",
      "epoch: 4 step: 539, loss is 0.0006602643406949937\n",
      "epoch: 4 step: 540, loss is 0.00039644737262278795\n",
      "epoch: 4 step: 541, loss is 0.0006724196719005704\n",
      "epoch: 4 step: 542, loss is 0.0013735441025346518\n",
      "epoch: 4 step: 543, loss is 0.0013329207431524992\n",
      "epoch: 4 step: 544, loss is 0.00060175487305969\n",
      "epoch: 4 step: 545, loss is 0.00026756752049550414\n",
      "epoch: 4 step: 546, loss is 0.0002902531996369362\n",
      "epoch: 4 step: 547, loss is 0.00041406118543818593\n",
      "epoch: 4 step: 548, loss is 0.000222866001422517\n",
      "epoch: 4 step: 549, loss is 0.0005235398421064019\n",
      "epoch: 4 step: 550, loss is 0.007771637756377459\n",
      "epoch: 4 step: 551, loss is 0.0017395531758666039\n",
      "epoch: 4 step: 552, loss is 0.012126648798584938\n",
      "epoch: 4 step: 553, loss is 0.00016257079550996423\n",
      "epoch: 4 step: 554, loss is 0.0009540640166960657\n",
      "epoch: 4 step: 555, loss is 0.000751414627302438\n",
      "epoch: 4 step: 556, loss is 0.0011839233338832855\n",
      "epoch: 4 step: 557, loss is 0.0014587317127734423\n",
      "epoch: 4 step: 558, loss is 0.00016083985974546522\n",
      "epoch: 4 step: 559, loss is 0.0006005926989018917\n",
      "epoch: 4 step: 560, loss is 0.0006440572906285524\n",
      "epoch: 4 step: 561, loss is 0.0028550433926284313\n",
      "epoch: 4 step: 562, loss is 0.0002281232737004757\n",
      "epoch: 4 step: 563, loss is 0.0018385149305686355\n",
      "epoch: 4 step: 564, loss is 0.0008333236328326166\n",
      "epoch: 4 step: 565, loss is 0.0006133802235126495\n",
      "epoch: 4 step: 566, loss is 0.0012742638355121017\n",
      "epoch: 4 step: 567, loss is 0.0005547074833884835\n",
      "epoch: 4 step: 568, loss is 0.00039063679287210107\n",
      "epoch: 4 step: 569, loss is 0.0016919008921831846\n",
      "epoch: 4 step: 570, loss is 0.000720067007932812\n",
      "epoch: 4 step: 571, loss is 0.0004471769498195499\n",
      "epoch: 4 step: 572, loss is 0.00040438753785565495\n",
      "epoch: 4 step: 573, loss is 0.0007716920808888972\n",
      "epoch: 4 step: 574, loss is 0.00043408674537204206\n",
      "epoch: 4 step: 575, loss is 0.0006241153459995985\n",
      "epoch: 4 step: 576, loss is 0.0011854720069095492\n",
      "epoch: 4 step: 577, loss is 0.0006166658131405711\n",
      "epoch: 4 step: 578, loss is 0.0008521719137206674\n",
      "epoch: 4 step: 579, loss is 0.0002737805189099163\n",
      "epoch: 4 step: 580, loss is 0.00091262545902282\n",
      "epoch: 4 step: 581, loss is 0.000776740605942905\n",
      "epoch: 4 step: 582, loss is 0.00036602161708287895\n",
      "epoch: 4 step: 583, loss is 0.0037908884696662426\n",
      "epoch: 4 step: 584, loss is 0.0005002231919206679\n",
      "epoch: 4 step: 585, loss is 0.00030932389199733734\n",
      "epoch: 4 step: 586, loss is 0.0002963374136015773\n",
      "epoch: 4 step: 587, loss is 0.0005046389997005463\n",
      "epoch: 4 step: 588, loss is 0.0019162008538842201\n",
      "epoch: 4 step: 589, loss is 0.0003783033462241292\n",
      "epoch: 4 step: 590, loss is 0.0011600085999816656\n",
      "epoch: 4 step: 591, loss is 0.0002956110693048686\n",
      "epoch: 4 step: 592, loss is 0.0008660575258545578\n",
      "epoch: 4 step: 593, loss is 0.0004892393480986357\n",
      "epoch: 4 step: 594, loss is 0.0003699911176227033\n",
      "epoch: 4 step: 595, loss is 0.0011351272696629167\n",
      "epoch: 4 step: 596, loss is 0.00015645689563825727\n",
      "Train epoch time: 16954.942 ms, per step time: 28.448 ms\n",
      "train success\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 6. 测试评估"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "checkpoint_path = './ckpt/train_textcnn-4_596.ckpt'"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:43:04.342495300Z",
     "start_time": "2024-03-18T06:43:04.274497800Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = instance.create_test_dataset(batch_size=cfg.batch_size)\n",
    "opt = nn.Adam(filter(lambda x: x.requires_grad, net.get_parameters()), \n",
    "              learning_rate=0.001, weight_decay=cfg.weight_decay)\n",
    "loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True)\n",
    "net = TextCNN(vocab_len=instance.get_dict_len(),word_len=cfg.word_len,\n",
    "                  num_classes=cfg.num_classes,vec_length=cfg.vec_length)\n",
    "\n",
    "if checkpoint_path is not None:\n",
    "    param_dict = load_checkpoint(checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(checkpoint_path))\n",
    "else:\n",
    "    param_dict = load_checkpoint(cfg.checkpoint_path)\n",
    "    print(\"load checkpoint from [{}].\".format(cfg.checkpoint_path))\n",
    "\n",
    "load_param_into_net(net, param_dict)\n",
    "net.set_train(False)\n",
    "model = Model(net, loss_fn=loss, metrics={'acc': Accuracy()})\n",
    "\n",
    "acc = model.eval(dataset)\n",
    "print(\"accuracy: \", acc)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:43:05.364953700Z",
     "start_time": "2024-03-18T06:43:05.056065300Z"
    }
   },
   "execution_count": 21,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(358762:140626981450816,MainProcess):2024-03-18-14:43:04.826.311 [mindspore/nn/layer/basic.py:173] For Dropout, this parameter `keep_prob` will be deprecated, please use `p` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from [./ckpt/train_textcnn-4_596.ckpt].\n",
      "accuracy:  {'acc': 0.78125}\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 7. 在线测试"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = sentence.replace('\\n','')\\\n",
    "                                    .replace('\"','')\\\n",
    "                                    .replace('\\'','')\\\n",
    "                                    .replace('.','')\\\n",
    "                                    .replace(',','')\\\n",
    "                                    .replace('[','')\\\n",
    "                                    .replace(']','')\\\n",
    "                                    .replace('(','')\\\n",
    "                                    .replace(')','')\\\n",
    "                                    .replace(':','')\\\n",
    "                                    .replace('--','')\\\n",
    "                                    .replace('-',' ')\\\n",
    "                                    .replace('\\\\','')\\\n",
    "                                    .replace('0','')\\\n",
    "                                    .replace('1','')\\\n",
    "                                    .replace('2','')\\\n",
    "                                    .replace('3','')\\\n",
    "                                    .replace('4','')\\\n",
    "                                    .replace('5','')\\\n",
    "                                    .replace('6','')\\\n",
    "                                    .replace('7','')\\\n",
    "                                    .replace('8','')\\\n",
    "                                    .replace('9','')\\\n",
    "                                    .replace('`','')\\\n",
    "                                    .replace('=','')\\\n",
    "                                    .replace('$','')\\\n",
    "                                    .replace('/','')\\\n",
    "                                    .replace('*','')\\\n",
    "                                    .replace(';','')\\\n",
    "                                    .replace('<b>','')\\\n",
    "                                    .replace('%','')\\\n",
    "                                    .replace(\"  \",\" \")\n",
    "    sentence = sentence.split(' ')\n",
    "    maxlen = cfg.word_len\n",
    "    vector = [0]*maxlen\n",
    "    for index, word in enumerate(sentence):\n",
    "        if index >= maxlen:\n",
    "            break\n",
    "        if word not in instance.Vocab.keys():\n",
    "            print(word,\"单词未出现在字典中\")\n",
    "        else:\n",
    "            vector[index] = instance.Vocab[word]\n",
    "    sentence = vector\n",
    "\n",
    "    return sentence\n",
    "\n",
    "def inference(review_en):\n",
    "    review_en = preprocess(review_en)\n",
    "    input_en = Tensor(np.array([review_en]).astype(np.int32))\n",
    "    output = net(input_en)\n",
    "    if np.argmax(np.array(output[0])) == 1:\n",
    "        print(\"Positive comments\")\n",
    "    else:\n",
    "        print(\"Negative comments\")"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T06:43:06.902932800Z",
     "start_time": "2024-03-18T06:43:06.895004600Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "review_en = \"the movie is so boring\"\n",
    "inference(review_en)\n",
    "review_en = \"the movie let me happy\"\n",
    "inference(review_en)\n",
    "review_en = \"what a wonderful movie !\"\n",
    "inference(review_en)\n",
    "review_en = \"I hope the movie could be better\"\n",
    "inference(review_en)"
   ],
   "metadata": {
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-03-18T07:07:32.320170700Z",
     "start_time": "2024-03-18T07:07:32.272914Z"
    }
   },
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative comments\n",
      "Negative comments\n",
      "Positive comments\n",
      "Negative comments\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}
